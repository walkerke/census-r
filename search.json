[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"online home Analyzing US Census Data: Methods, Maps, Models R, forthcoming book published CRC Press. online version book published Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC -NC-ND 4.0) license.support development book help maintain free online version, please consider one following:Purchasing hard copy book CRC Press (expected publication 2022);Purchasing hard copy book CRC Press (expected publication 2022);Contributing book filing issue making pull request book’s GitHub repository;Contributing book filing issue making pull request book’s GitHub repository;Sponsoring book development maintenance GitHub Sponsor;Sponsoring book development maintenance GitHub Sponsor;Chipping funds support book maintenance via PayPal.Chipping funds support book maintenance via PayPal.","code":""},{"path":"index.html","id":"about-the-book","chapter":"Preface","heading":"About the book","text":"Census data widely used United States across numerous research applied fields, including education, business, journalism, many others. recently, process working US Census data required use wide array web interfaces software platforms prepare, map, present data products. goal book illustrate utility R programming language handling tasks, allowing Census data users manage projects single computing environment.book focuses two types Census data products commonly used analysts:Aggregate data, involve counts estimates released Census Bureau aggregated geographic unit interest, state;Aggregate data, involve counts estimates released Census Bureau aggregated geographic unit interest, state;Microdata, individual, anonymized Census records represent sample given Census dataset.Microdata, individual, anonymized Census records represent sample given Census dataset.Readers new R Census data read book order, chapter includes concepts build upon examples introduced previous chapters. experienced analysts might use chapters standalone manuals tailored specific tasks topics interest . brief overview chapter follows .Chapter 1 general overview US Census data terms definitions, gives brief background R language R excellent environment working US Census data. ends motivating examples excellent applied projects use R Census data.Chapter 2 introduces tidycensus, R package working US Census Bureau data tidy format. Readers learn make basic data requests package understand various options package.Chapter 3 covers analysis US Census data using tidyverse, integrated framework packages data preparation wrangling. includes simple complex examples common tidyverse data wrangling tasks, discusses workflows handling margins error American Community Survey.Chapter 4 introduces workflows Census data visualization focus ggplot2 package. Examples focus best practices preparing data visualization building charts well-suited presenting Census data analyses.Chapter 5 introduces tigris package working US Census Bureau geographic data R. includes overview spatial data structures R sf package covers key geospatial data topics like coordinate reference systems.Chapter 6 mapping R. Readers learn make simple shaded maps US Census data using packages like ggplot2 tmap; maps complex data requirements like dot-density maps; interactive geographic visualizations apps.Chapter 7 covers spatial data analysis. Topics include geographic data overlay; distance proximity analysis; exploratory spatial data analysis spdep package.Chapter 8’s topic modeling geographic data. Readers learn compute indices segregation diversity Census data; fit linear, spatial, geographically weighted regression models; develop workflows geodemographic segmentation regionalization.Chapter 9 focuses individual-level microdata US Census Bureau’s Public Use Microdata Sample. Readers learn acquire datasets tidycensus use generate unique insights.Chapter 10 covers analysis microdata focus methods analyzing modeling complex survey samples. Topics include estimation standard errors replicate weights, mapping microdata, modeling microdata appropriately survey srvyr packages.Chapter 11’s focus Census datasets beyond decennial US Census American Community Survey. first part chapter focuses historical mapping National Historical Geographic Information System (NHGIS) historical microdata analysis IPUMS-USA; readers learn use ipumsr R package R’s database tools assist tasks. second part chapter covers wide range datasets available US Census Bureau, R packages like censusapi lehdr help analysts access datasets.Chapter 12 covers Census data resources regions outside United States. covers global demographic analysis US Census Bureau’s International Data Base well country-specific examples Canada, Mexico, Brazil, Kenya.","code":""},{"path":"index.html","id":"who-this-book-is-for","chapter":"Preface","heading":"Who this book is for","text":"book designed practitioners interested working efficiently data United States Census Bureau. defines “practitioners” quite broadly, analysts levels expertise find topics within book useful. book focuses examples United States, topics covered designed general applicable use-cases outside United States also outside domain working Census data.Students analysts newer R likely want start Chapters 2 4 cover basics tidycensus give examples several tidyverse-centric workflows.Students analysts newer R likely want start Chapters 2 4 cover basics tidycensus give examples several tidyverse-centric workflows.Chapters 5 7 likely appeal analysts Geographic Information Systems (GIS) background, chapters introduce methods handling mapping spatial data might alternatively done desktop GIS software like ArcGIS QGIS.Chapters 5 7 likely appeal analysts Geographic Information Systems (GIS) background, chapters introduce methods handling mapping spatial data might alternatively done desktop GIS software like ArcGIS QGIS.experienced social science researchers find Chapters 9 11 useful cover ACS microdata, essential resource many social science disciplines. Analysts coming R Stata SAS also learn use R’s survey data modeling tools chapters.experienced social science researchers find Chapters 9 11 useful cover ACS microdata, essential resource many social science disciplines. Analysts coming R Stata SAS also learn use R’s survey data modeling tools chapters.Data scientists interested integrating spatial analysis work may want focus Chapters 7 8, cover range methods can incorporated business intelligence workflows.Data scientists interested integrating spatial analysis work may want focus Chapters 7 8, cover range methods can incorporated business intelligence workflows.Journalists find value throughout book, though Chapters 2, 5-6, 9, 12 may prove especially useful focus rapid retrieval US international Census data can incorporated articles reports.Journalists find value throughout book, though Chapters 2, 5-6, 9, 12 may prove especially useful focus rapid retrieval US international Census data can incorporated articles reports.course, many use cases Census data covered overview. using examples book unique applications, please reach !","code":""},{"path":"index.html","id":"about-the-author","chapter":"Preface","heading":"About the author","text":"(Kyle Walker) work associate professor Geography Texas Christian University spatial data science consultant. ’m proud graduate University Oregon (Go Ducks!) hold Ph.D. Geography University Minnesota. research field population geography, focusing metropolitan demographic trends, consult broadly areas commercial real estate, health sciences, general R training/software development. consulting inquiries, please reach kyle@walker-data.com.live Fort Worth, Texas wife Molly three children Michaela, Landry, Clara, book dedicated. enjoy developing open source software, true passions exploring country family earnestly (though always successfully) coaching kids’ sports teams.","code":""},{"path":"index.html","id":"technical-details","chapter":"Preface","heading":"Technical details","text":"book written using RStudio’s Visual Editor R Markdown, published bookdown R package (Xie 2016).time recent book build, following R version used:examples book created following R package versions:\nTable 0.1: Packages used book\n","code":"## R version 4.1.3 (2022-03-10) -- \"One Push-Up\""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"the-united-states-census-and-the-r-programming-language","chapter":"1 The United States Census and the R programming language","heading":"1 The United States Census and the R programming language","text":"main focus book applied social data analysis R programming language, focus data United States Census Bureau. chapter introduces topics. first part chapter covers United States Census US Census Bureau, gives overview Census data can accessed used analysts. second part chapter introduction R programming language readers new R. chapter wraps examples applied social data analysis projects used R US Census data, setting stage topics covered remainder book.","code":""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"census-data-an-overview","chapter":"1 The United States Census and the R programming language","heading":"1.1 Census data: an overview","text":"United States Constitution mandates Article , Sections 2 9 complete enumeration US population taken every 10 years. language Constitution follows (see https://www.census.gov/programs-surveys/decennial-census/.html):actual enumeration shall made within three years first meeting Congress United States, within every subsequent term ten years, manner shall law direct.government agency tasked completing enumeration United States population United States Census Bureau, part US Department Commerce. first US Census conducted 1790, enumerations taking place every 10 years since . convention, “Census day” April 1 Census year.decennial US Census intended complete enumeration US population assist apportionment, refers balanced arrangement Congressional districts ensure appropriate representation United States House Representatives. asks limited set questions race, ethnicity, age, sex, housing tenure.2010 decennial Census, 1 6 Americans also received Census long form, asked wider range demographic questions income, education, language, housing, . Census long form since replaced American Community Survey, now premier source detailed demographic information US population. ACS mailed approximately 3.5 million households per year (representing around 3 percent US population), allowing annual data updates. Census Bureau releases two ACS datasets public: 1-year ACS, covers areas population 65,000 greater, 5-year ACS, moving average data 5-year period covers geographies Census block group. ACS data distinct decennial Census data data represent estimates rather precise counts, turn characterized margins error around estimates. topic covered depth Section 3.5. Due data collection problems resulting COVID-19 pandemic, 2020 1-year ACS data released, replaced experimental estimates year.decennial US Census American Community Survey popular widely used datasets produced US Census Bureau, Bureau conducts hundreds surveys disseminates data wide range subjects public. datasets include economic business surveys, housing surveys, international data, population estimates projections, much ; full listing available Census website.","code":""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"census-hierarchies","chapter":"1 The United States Census and the R programming language","heading":"1.2 Census hierarchies","text":"Aggregate data decennial US Census, American Community Survey, Census surveys made available public different enumeration units. Enumeration units geographies Census data tabulated. include legal entities states counties, statistical entities official jurisdictions used standardize data tabulation. smallest unit data made available decennial US Census block, smallest unit available ACS block group, represents collection blocks. surveys generally available higher levels aggregation.Enumeration units represent different levels Census hierarchy. hierarchy summarized graphic (https://www.census.gov/programs-surveys/geography/guidance/hierarchy.html).\nFigure 1.1: Census hierarchy enumeration units\ncentral axis diagram represents central Census hierarchy enumeration units, geography Census blocks way nation nests within parent unit. means block groups fully composed Census blocks, Census tracts fully composed block groups, forth. example nesting shown graphic 2020 Census tracts Benton County, Oregon.\nFigure 1.2: Benton County, Census tracts relationship county boundary\nplot illustrates Census tracts Benton County neatly nest within parent geography, county. means sum Census data Census tracts Benton County also equal Benton County’s published total county level.Reviewing diagram shows Census geographies, like congressional districts, nest within states, geographies nest within parent geography . good example Zip Code Tabulation Area (ZCTA), used Census Bureau represent postal code geographies US. -depth discussion ZCTAs (pitfalls) found Section 6.4.2; brief illustration represented graphic .\nFigure 1.3: Benton County, ZCTAs relationship county boundary\ngraphic illustrates, ZCTAs fit entirely within Benton County, others overlap county boundaries. turn, “ZCTAs Benton County” meaning “Census tracts Benton County”, former extend neighboring counties whereas latter .","code":""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"how-to-find-us-census-data","chapter":"1 The United States Census and the R programming language","heading":"1.3 How to find US Census data","text":"US Census data available variety sources, directly Census Bureau also third-party distributors. section gives overview sources.","code":""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"data-downloads-from-the-census-bureau","chapter":"1 The United States Census and the R programming language","heading":"1.3.1 Data downloads from the Census Bureau","text":"years, researchers visit US Census Bureau’s American FactFinder site download Census data custom geographies. American FactFinder decomissioned 2020, giving way new data download interface, data.census.gov. Users can interactively search several Census Bureau datasets (decennial Census & ACS, along Economic Census, Population Estimates Program, others), generate custom queries geography, download data extracts.\nFigure 1.4: View data.census.gov interface\ngraphic shows ACS table DP02 ACS Data Profile Census tracts Arkansas.Users comfortable dealing data bulk want download raw data instead prefer US Census Bureau’s FTP site, https://www2.census.gov/programs-surveys/. site includes directory Census surveys can navigated downloaded. image shows directory structure 2019 American Community Survey, available state alternatively entire country.\nFigure 1.5: View Census FTP download site\nNational files large (full 5-year file geographies 10GB data zipped) users require dedicated software computing workflows interact data.","code":""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"the-census-api","chapter":"1 The United States Census and the R programming language","heading":"1.3.2 The Census API","text":"Obama Administration’s Digital Government Strategy prioritized widespread dissemination government data resources public, highlighted data.gov data portal. part strategy, Census Bureau released Census Application Programming Interface, API, 2012. interface, available https://www.census.gov/data/developers/data-sets.html, grown provide developers programmatic access hundreds data resources Census Bureau.Census APIs characterized API endpoint, base web address given Census dataset, query, customizes data returned API. example, API endpoint 2010 Decennial US Census https://api.census.gov/data/2010/dec/sf1; example query requests total population data counties California, ?get=P001001,NAME&=county:*&=state:06, appended endpoint request Census API. result query can viewed , returning data JavaScript Object Notation (JSON) format shown .cases, users interact Census API software libraries offer simplified programmatic access API’s data resources. example covered extensively book R package tidycensus (K. Walker Herman 2021), introduced Chapter 2. Many libraries exist accessing Census API; resources covered Chapter 11, readers learn write data access functions Section 11.4.2.Users Census API software libraries require Census API key, free fast acquire. Getting API key covered detail Section 2.1. Users may also want join US Census Bureau’s Slack Community, developers interact answer others’ questions using API associated software libraries.","code":"[[\"P001001\",\"NAME\",\"state\",\"county\"],\n[\"21419\",\"Colusa County, California\",\"06\",\"011\"],\n[\"220000\",\"Butte County, California\",\"06\",\"007\"],\n[\"1510271\",\"Alameda County, California\",\"06\",\"001\"],\n[\"1175\",\"Alpine County, California\",\"06\",\"003\"],\n[\"38091\",\"Amador County, California\",\"06\",\"005\"],\n[\"45578\",\"Calaveras County, California\",\"06\",\"009\"],\n[\"1049025\",\"Contra Costa County, California\",\"06\",\"013\"],\n[\"28610\",\"Del Norte County, California\",\"06\",\"015\"],\n[\"152982\",\"Kings County, California\",\"06\",\"031\"],\n[\"28122\",\"Glenn County, California\",\"06\",\"021\"],\n[\"134623\",\"Humboldt County, California\",\"06\",\"023\"],\n[\"174528\",\"Imperial County, California\",\"06\",\"025\"],\n[\"181058\",\"El Dorado County, California\",\"06\",\"017\"],\n[\"930450\",\"Fresno County, California\",\"06\",\"019\"],\n..."},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"third-party-data-distributors","chapter":"1 The United States Census and the R programming language","heading":"1.3.3 Third-party data distributors","text":"US Census Bureau provides data resources free available re-distribution, several third-party data distributors developed streamlined interfaces use Census data. One comprehensive resources University Minnesota’s National Historical Geographic Information System, NHGIS (Manson et al. 2021), provides access ACS data well decennial Census data back 1790. NHGIS covered depth Section 11.1.1.Two recommended third-party Census data distributors Census Reporter Social Explorer. Census Reporter, project based Northwestern University’s Knight Lab, targeted towards journalists offers web interface can help anyone explore tables available ACS download ACS data. Social Explorer commercial product offers table-based map-based interface exploring visualizing Census data, makes mapping Census data straightfoward users aren’t experienced data analysis mapping software.readers book want learn use Census data resources produce unique insights field interest. requires identifying workflow access download custom data extracts relevant topics study areas, set software environment help wrangle, visualize, model data extracts. book recommends R programming language ideal software environment completing tasks integrated way. next section introduces R new users.","code":""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"what-is-r","chapter":"1 The United States Census and the R programming language","heading":"1.4 What is R?","text":"R (R Core Team 2021) one popular programming languages software environments statistical computing, focus book respect software applications. section introduces basics working R covers terminology help readers work sections book. experienced R user, can safely skip section; however, readers new R find information helpful getting started applied examples book.","code":""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"getting-started-with-r","chapter":"1 The United States Census and the R programming language","heading":"1.4.1 Getting started with R","text":"get started R, visit CRAN (Comprehensive R Archive Network) website https://cloud.r-project.org/ download appropriate version R operating system, install software. time writing, recent version R 4.1.1; good idea make sure recent version R installed computer.R installed, strongly recommend install RStudio (RStudio Team 2021), premier integrated development environment (IDE) R. can run R without RStudio, RStudio offers wide variety utilities make analysts’ work R easier streamlined. fact, entire book written inside RStudio! RStudio can installed http://www.rstudio.com/download.RStudio installed, open find Console pane. interactive console allows type copy-paste R commands get results back.","code":""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"basic-data-structures-in-r","chapter":"1 The United States Census and the R programming language","heading":"1.4.2 Basic data structures in R","text":"basic level, R can function calculator, computing everything simple arithmetic advanced math:Often, want assign analytic results like object (also commonly called variable). Objects R created assignment operator (either <- =) like :, assigned result mathematical operation 2 + 3 object x:Object names can composed unquoted combination letters numbers long first character letter. object, stores value 5, characterized class:x object class numeric, general class indicating can perform mathematical operations object. Numeric objects can contrasted objects class \"character\", represent character strings, textual information. Objects class \"character\" defined either single- double-quotes around block text.many classes objects ’ll encounter book; however distinction objects class \"numeric\" \"character\" come frequently.Data analysts commonly encounter another class object: data frame derivatives (class \"data.frame\"). Data frames rectangular objects characterized rows, generally represent individual observations, columns, represent characteristics attributes common rows.","code":"\n2 + 3## [1] 5\nx <- 2 + 3\nx## [1] 5\nclass(x)## [1] \"numeric\"\ny <- \"census\"\nclass(y)## [1] \"character\"\ndf <- data.frame(\n  v1 = c(2, 5, 1, 7, 4),\n  v2 = c(10, 2, 4, 2, 1),\n  v3 = c(\"a\", \"b\", \"c\", \"d\", \"e\")\n)\n\ndf##   v1 v2 v3\n## 1  2 10  a\n## 2  5  2  b\n## 3  1  4  c\n## 4  7  2  d\n## 5  4  1  e"},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"functions-and-packages","chapter":"1 The United States Census and the R programming language","heading":"1.4.3 Functions and packages","text":"code generates data frame previous section uses two built-functions: data.frame(), creates columns one vectors (defined sequences objects), c(), used create vectors data frame. can think functions “wrappers” condense longer code-based workflows simpler representations. R users can define functions follows:basic example, function named multiply() defined function. x y parameters, locally-varying elements function. function called, user supplies arguments, passed parameters series calculations. example, x takes value 232, y takes value 7; result returned multiply() function.can quite bit R without ever write functions; however, almost certainly use functions written others. R, functions generally available packages, libraries code designed complete related set tasks. example, main focus Chapter 2 tidycensus package, includes functions help users access Census data. Packages can installed CRAN install.packages() function:installed, functions package can loaded user’s R environment library() command, e.g. library(tidycensus). Alternatively, can used package_name::function_name() notation, e.g. tidycensus::get_acs(). notations used times book.“official” versions R packages usually published CRAN installable install.packages(), experimental -development R packages may available GitHub instead. packages installed install_github() function remotes package (Hester et al. 2021), referencing user name package name.packages used book available CRAN, available GitHub installed accordingly.","code":"\nmultiply <- function(x, y) {\n  x * y\n}\n\nmultiply(232, 7)## [1] 1624\ninstall.packages(\"tidycensus\")\nlibrary(remotes)\ninstall_github(\"Shelmith-Kariuki/rKenyaCensus\")"},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"package-ecosystems-in-r","chapter":"1 The United States Census and the R programming language","heading":"1.4.4 Package ecosystems in R","text":"R free open source software (FOSS), means R free download install source code open anyone view. brings substantial benefit encouraging innovation user community, anyone can create new packages either submit publication official CRAN repository host personal GitHub page. turn, new methodological innovations often quickly accessible R user community. However, can make R feel fragmented, especially users coming commercial software designed consistent interface. Package syntax sometimes represent idiosyncratic choices developer can make R confusing beginners.tidyverse ecosystem developed RStudio (Wickham et al. 2019) one popular frameworks data analysis R, attempts respond problems introduced package fragmentation. tidyverse consists series R packages designed address common data analysis tasks (data wrangling, data reshaping, data visualization, among many others) using consistent syntax. Many R packages now developed integration within tidyverse mind. good example sf package (Pebesma 2018) integrates spatial data analysis tidyverse. book largely written tidyverse sf ecosystems mind; tidyverse covered greater depth Chapter 3, sf introduced Chapter 5.ecosystems exist may preferable R users. R’s core functionality, commonly termed “base R,” consists original syntax language R users get know independent preferred analytic framework. R users prefer maintain analysis base R require dependencies, meaning can run without installing external libraries. Another popular framework data.table (Dowle Srinivasan 2021) associated packages, extend base R’s data.frame allow fast high-performance data wrangling analysis.","code":""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"analyses-using-r-and-us-census-data","chapter":"1 The United States Census and the R programming language","heading":"1.5 Analyses using R and US Census data","text":"large ecosystem R packages exists help analysts work US Census data. good summary ecosystem found Ari Lamstein Logan Powell’s Guide Working US Census Data R (Lamstein Powell 2018), ecosystem grown since report published. non-comprehensive summary R packages interest readers; others covered throughout book.","code":""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"census-data-packages-in-r-a-brief-summary","chapter":"1 The United States Census and the R programming language","heading":"1.5.1 Census data packages in R: a brief summary","text":"users prefer work raw Census data files, totalcensus package (Li 2021) helps download Census data bulk Census FTP server loads R. election analysts, PL94171 package (McCartan Kenny 2021) processes loads PL-94171 redistricting files, including recent data 2020 Census.Users want make custom queries interested R packages interact Census APIs. pioneering package area acs package (Glenn 2019), uses custom interface class system return data extracts various Census APIs. package informed variety convenient Census data packages, choroplethr (Lamstein 2020) automates map production data Census API. censusapi package (Recht 2021) also offers comprehensive interface hundreds datasets available Census Bureau via API. examples book largely focus tidycensus tigris packages created author, interact several Census API endpoints return geographic data mapping spatial analysis.R interfaces third-party Census data resources emerged well. good example ipumsr R package (Ellis Burk 2020), helps users interact datasets Minnesota Population Center like NHGIS. aforementioned R packages - along R’s rich ecosystem data analysis - contributed wide range projects using Census data variety fields. examples highlighted .","code":""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"health-resource-access","chapter":"1 The United States Census and the R programming language","heading":"1.5.2 Health resource access","text":"R Census data widespread applications analysis health care resource access. excellent example censusapi developer Hannah Recht’s study stroke care access Mississippi Delta Appalachia, published KHN 2021 (Pattani, Recht, Grey 2021). graphic illustrate Recht’s work integrating travel-time analytics Census data identify populations limited access stroke care across US South.\nFigure 1.6: Accessibility stroke care across US South Pattani et al. (2021). Image reprinted permission publisher.\nRecht published analysis code corresponding GitHub repository, allowing readers developers understand methodology reproduce analysis. R packages used analysis include censusapi Census data, sf spatial analysis, tidyverse framework data preparation wrangling.","code":""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"covid-19-and-pandemic-response","chapter":"1 The United States Census and the R programming language","heading":"1.5.3 COVID-19 and pandemic response","text":"R Census data can also used together generate applications benefit public health initiatives. great example Texas COVID-19 Vaccine Tracker, developed Matt Worthington University Texas LBJ School Public Affairs.\nFigure 1.7: Screenshot Texas COVID-19 Vaccine Tracker website\napplication, illustrated graphic , provides comprehensive set information vaccine uptake access around Texas. example map shown visualizes vaccine doses administered per 1,000 persons Austin-area ZCTAs. Census data used throughout application help visitors understand differential vaccine update regions demographics.Source code Worthington’s application can explored corresponding GitHub repository. site built variety R frameworks, including Shiny framework interactive dashboarding, includes range static graphics, interactive graphics, interactive maps generated R.","code":""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"politics-and-gerrymandering","chapter":"1 The United States Census and the R programming language","heading":"1.5.4 Politics and gerrymandering","text":"discussed , one primary purposes US Census determine congressional apportionment. process requires re-drawing congressional districts every 10 years, termed redistricting. redistricting process politically fraught due gerrymandering, refers drawing districts way gives one political party (US, Republican Democratic) built-advantage , potentially disenfranchising voters process.Harvard University’s ALARM project uses R analyze redistricting gerrymandering contribute equitable solutions. ALARM project developed veritable ecosystem R packages incorporate Census data make Census data accessible redistricting analysts. includes PL94171 package mentioned get 2020 redistricting data, geomander package prepare data redistricting analysis (Kenny et al. 2021), redist package algorithmically derive evaluate redistricting solutions (Kenny et al. 2021). example , generated modified code redist documentation, shows basic example potential redistricting solutions based Census data Iowa.\nFigure 1.8: Example redistricting solutions using redist package\ncourt battles contentious discussions around redistricting ramp following release 2020 Census redistricting files, R users can use ALARM’s family packages analyze produce potential solutions.","code":""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"social-equity-research","chapter":"1 The United States Census and the R programming language","heading":"1.5.5 Social equity research","text":"Census data core resource large body research social sciences speaks directly issues inequality opportunity. Jerry Shannon’s study dollar store geography (Shannon 2020) uses Census data R compelling way purpose. analysis examines growth dollar stores across United States relationship patterns racial segregation United States. Shannon finds dollar stores likely found nearer predominantly Black Latino neighborhoods opposed predominantly white neighborhoods, even controlling structural economic characteristics neighborhoods. Shannon’s analysis completed R, analysis code available corresponding GitHub repository. demographic analysis uses American Community Survey data obtained NHGIS.","code":""},{"path":"the-united-states-census-and-the-r-programming-language.html","id":"census-data-visualization","chapter":"1 The United States Census and the R programming language","heading":"1.5.6 Census data visualization","text":"One favorite projects worked Mapping Immigrant America, interactive map US foreign-born population. map scatters dots within Census tracts proportionally represent residences immigrants based data American Community Survey. map viewable link .\nFigure 1.9: Screenshot Mapping Immigrant America interactive map\nversion 1 map used variety tools process data including ArcGIS QGIS, data processing version 2 completed entirely R, data uploaded Mapbox Studio platform hosting visualization. data preparation code can viewed map’s GitHub repository.map uses dasymetric dot-density methodology (K. E. Walker 2018) implemented R using series techniques covered book. ACS data corresponding Census tract boundaries acquired using tools learned Chapter 2 Section 6.1; areas population removed Census tracts using spatial analysis technique covered Section 7.5.1; dots generated mapping using method introduced Section 6.3.4.3.examples small sampling volumes work completed analysts using R US Census Bureau data. next chapter, get started using R access Census data , focus tidycensus package.","code":""},{"path":"an-introduction-to-tidycensus.html","id":"an-introduction-to-tidycensus","chapter":"2 An introduction to tidycensus","heading":"2 An introduction to tidycensus","text":"tidycensus package (K. Walker Herman 2021), first released 2017, R package designed facilitate process acquiring working US Census Bureau population data R environment. package two distinct goals. First, tidycensus aims make Census data available R users tidyverse-friendly format, helping kick-start process generating insights US Census data. Second, package designed streamline data wrangling process spatial Census data analysts. tidycensus, R users can request geometry along attributes Census data, helping facilitate mapping spatial analysis. functionality tidycensus covered depth Chapters 6, 7, 8.discussed previous chapter, US Census Bureau makes wide range datasets available user community APIs data download resources. tidycensus comprehensive portal data resources; instead, focuses select number datasets implemented series core functions. core functions tidycensus include:get_decennial(), requests data US Decennial Census APIs 2000, 2010, 2020.get_decennial(), requests data US Decennial Census APIs 2000, 2010, 2020.get_acs(), requests data 1-year 5-year American Community Survey samples. Data available 1-year ACS back 2005 5-year ACS back 2005-2009.get_acs(), requests data 1-year 5-year American Community Survey samples. Data available 1-year ACS back 2005 5-year ACS back 2005-2009.get_estimates(), interface Population Estimates APIs. datasets include yearly estimates population characteristics state, county, metropolitan area, along components change demographic estimates like births, deaths, migration rates.get_estimates(), interface Population Estimates APIs. datasets include yearly estimates population characteristics state, county, metropolitan area, along components change demographic estimates like births, deaths, migration rates.get_pums(), accesses data ACS Public Use Microdata Sample APIs. samples include anonymized individual-level records ACS organized household highly useful many different social science analyses. get_pums() covered depth Chapters 9 10.get_pums(), accesses data ACS Public Use Microdata Sample APIs. samples include anonymized individual-level records ACS organized household highly useful many different social science analyses. get_pums() covered depth Chapters 9 10.get_flows(), interface ACS Migration Flows APIs. Includes information - -flows various geographies 5-year ACS samples, enabling origin-destination analyses.get_flows(), interface ACS Migration Flows APIs. Includes information - -flows various geographies 5-year ACS samples, enabling origin-destination analyses.","code":""},{"path":"an-introduction-to-tidycensus.html","id":"getting-started-with-tidycensus","chapter":"2 An introduction to tidycensus","heading":"2.1 Getting started with tidycensus","text":"get started tidycensus, users install package install.packages(\"tidycensus\") yet installed; load package library(\"tidycensus\"); set Census API key census_api_key() function. API keys can obtained https://api.census.gov/data/key_signup.html. ’ve signed API key, sure activate key email receive Census Bureau works correctly. Declaring install = TRUE calling census_api_key() install key use future R sessions, may convenient many users.","code":"\nlibrary(tidycensus)\n# census_api_key(\"YOUR KEY GOES HERE\", install = TRUE)"},{"path":"an-introduction-to-tidycensus.html","id":"decennial-census","chapter":"2 An introduction to tidycensus","heading":"2.1.1 Decennial Census","text":"API key installed, users can obtain decennial Census ACS data single function call. Let’s start get_decennial(), used access decennial Census data 2000, 2010, 2020 decennial US Censuses.get data decennial US Census, users must specify string representing requested geography; vector Census variable IDs, represented variable; optionally Census table ID, passed table. code gets data total population state 2010 decennial Census.\nTable 2.1: Total population state, 2010 Census\nfunction returns tibble data 2010 US Census (function default year) information total population state, assigns object total_population_10. Data 2000 2020 can also obtained supplying appropriate year year parameter.","code":"\ntotal_population_10 <- get_decennial(\n  geography = \"state\", \n  variables = \"P001001\",\n  year = 2010\n)"},{"path":"an-introduction-to-tidycensus.html","id":"summary-files-in-the-decennial-census","chapter":"2 An introduction to tidycensus","heading":"2.1.1.1 Summary files in the decennial Census","text":"default, get_decennial() uses argument sumfile = \"sf1\", fetches data decennial Census Summary File 1. summary file exists 2000 2010 decennial US Censuses, includes core demographic characteristics Census geographies. 2000 2010 decennial Census data also include Summary File 2, contains information range population housing unit characteristics specified \"sf2\". Detailed demographic information 2000 decennial Census income occupation can found Summary Files 3 (\"sf3\") 4 (\"sf4\"). Data 2000 2010 Decennial Censuses island territories Puerto Rico must accessed corresponding summary files: \"\" American Samoa, \"mp\" Northern Mariana Islands, \"gu\" Guam, \"vi\" US Virgin Islands.2020 Decennial Census data available PL 94-171 Redistricting summary file, specified sumfile = \"pl\" also available 2010. Redistricting summary files include limited subset variables decennial US Census used legislative redistricting. variables include total population housing units; race ethnicity; voting-age population; group quarters population. example, code retrieves information American Indian & Alaska Native population state 2020 decennial Census.\nTable 2.2: American Indian Alaska Native alone population state 2020 decennial Census\nargument sumfile = \"pl\" assumed (turn required) users request data 2020 remain main Demographic Housing Characteristics File released mid--late 2022.users request data 2020 decennial Census first time given R session, get_decennial() prints following message:message alerts users 2020 decennial Census data use differential privacy method preserve confidentiality individuals responded Census. can lead inaccuracies small area analyses using 2020 Census data also can make comparisons small counts across years difficult. -depth discussion differential privacy 2020 Census found conclusion book.","code":"\naian_2020 <- get_decennial(\n  geography = \"state\",\n  variables = \"P1_005N\",\n  year = 2020,\n  sumfile = \"pl\"\n)Note: 2020 decennial Census data use differential privacy, a technique that\nintroduces errors into data to preserve respondent confidentiality.\nℹ Small counts should be interpreted with caution.\nℹ See https://www.census.gov/library/fact-sheets/2021/protecting-the-confidentiality-of-the-2020-census-redistricting-data.html for additional guidance."},{"path":"an-introduction-to-tidycensus.html","id":"american-community-survey","chapter":"2 An introduction to tidycensus","heading":"2.1.2 American Community Survey","text":"Similarly, get_acs() retrieves data American Community Survey. discussed previous chapter, ACS includes wide variety variables detailing characteristics US population found decennial Census. example fetches data number residents born Mexico state.\nTable 2.3: Mexican-born population state, 2016-2020 5-year ACS\nyear specified, get_acs() defaults recent five-year ACS sample, time writing 2016-2020. data returned similar structure returned get_decennial(), includes estimate column (ACS estimate) moe column (margin error around estimate) instead value column. Different years different surveys available adjusting year survey parameters. survey defaults 5-year ACS; however can changed 1-year ACS using argument survey = \"acs1\". example, following code fetch data 1-year ACS 2019:\nTable 2.4: Mexican-born population state, 2019 1-year ACS\nNote differences 5-year ACS estimates 1-year ACS estimates shown. states larger Mexican-born populations like Arizona, California, Colorado, 1-year ACS data represent --date estimates, albeit characterized larger margins error relative estimates. states smaller Mexican-born populations like Alabama, Alaska, Arkansas, however, estimate returns NA, R’s notation representing missing data. encounter data’s estimate column, generally mean estimate small given geography deemed reliable Census Bureau. case, states largest Mexican-born populations data available variable 1-year ACS, meaning 5-year ACS used make full state-wise comparisons desired.users try accessing data 2020 1-year ACS tidycensus, encounter following error:means 1-year ACS data, tidycensus users need use older datasets (2019 earlier) access 2021 data released late 2022.Variables ACS detailed tables, data profiles, summary tables, comparison profile, supplemental estimates available tidycensus’s get_acs() function; function auto-detect dataset look variables based names. Alternatively, users can supply table name table parameter get_acs(); return data every variable table. example, get variables associated table B01001, covers sex broken age, 2016-2020 5-year ACS:\nTable 2.5: Table B01001 state 2016-2020 5-year ACS\nfind variables associated given ACS table, tidycensus downloads dataset variables Census Bureau website looks variable codes download. cache_table parameter set TRUE, function instructs tidycensus cache dataset user’s computer faster future access. needs done per ACS Census dataset user like specify option.","code":"\nborn_in_mexico <- get_acs(\n  geography = \"state\", \n  variables = \"B05006_150\",\n  year = 2020\n)\nborn_in_mexico_1yr <- get_acs(\n  geography = \"state\", \n  variables = \"B05006_150\", \n  survey = \"acs1\",\n  year = 2019\n)Error: The regular 1-year ACS was not released in 2020 due to low response rates.\nThe Census Bureau released a set of experimental estimates for the 2020 1-year ACS\nthat are not available in tidycensus.\nThese estimates can be downloaded at https://www.census.gov/programs-surveys/acs/data/experimental-data/1-year.html.\nage_table <- get_acs(\n  geography = \"state\", \n  table = \"B01001\",\n  year = 2020\n)"},{"path":"an-introduction-to-tidycensus.html","id":"geography-and-variables-in-tidycensus","chapter":"2 An introduction to tidycensus","heading":"2.2 Geography and variables in tidycensus","text":"geography parameter get_acs() get_decennial() allows users request data aggregated common Census enumeration units. time writing, tidycensus accepts enumeration units nested within states /counties, applicable. Census blocks available get_decennial() get_acs() block-level data available American Community Survey. request data within states /counties, state county names can supplied state county parameters, respectively. Arguments formatted way accepted US Census Bureau API, specified table . “Available ” geography bold, argument required geography.geographies available 2000 \"state\", \"county\", \"county subdivision\", \"tract\", \"block group\", \"place\". geographies available Census API available tidycensus moment require complex hierarchy specification package supports, variables available every geography.geography parameter must typed exactly specified table request data correctly Census API; use guide reference copy-paste longer strings. core-based statistical areas zip code tabulation areas, two heavily-requested geographies, aliases \"cbsa\" \"zcta\" can used, respectively, fetch data geographies.\nTable 2.6: Population CBSA\n","code":"\ncbsa_population <- get_acs(\n  geography = \"cbsa\",\n  variables = \"B01003_001\",\n  year = 2020\n)"},{"path":"an-introduction-to-tidycensus.html","id":"geographic-subsets","chapter":"2 An introduction to tidycensus","heading":"2.2.1 Geographic subsets","text":"many geographies, tidycensus supports granular requests subsetted state even county, supported API. information found “Available ” column guide . geographic subset bold, required; , optional.example, analyst might interested studying variations household income state Wisconsin. Although analyst can request counties United States, necessary specific task. turn, can use state parameter subset request specific state.\nTable 2.7: Median household income county Wisconsin\ntidycensus accepts state names (e.g. \"Wisconsin\"), state postal codes (e.g. \"WI\"), state FIPS codes (e.g. \"55\"), analyst can use comfortable .Smaller geographies like Census tracts can also subsetted county. Given Census tracts nest neatly within counties (cross county boundaries), can request Census tracts given county using optional county parameter. Dane County, home Wisconsin’s capital city Madison, shown . Note name county can supplied well FIPS code. state two counties similar names (e.g. “Collin” “Collingsworth” Texas) ’ll need spell full county string type \"Collin County\".\nTable 2.8: Median household income Dane County Census tract\nrespect geography American Community Survey, users aware whereas 5-year ACS covers geographies block group, 1-year ACS returns data geographies population 65,000 greater. means geographies (e.g. Census tracts) never available 1-year ACS, geographies counties partially available. illustrate , can check number rows object wi_income:72 rows dataset, one county Wisconsin. However, data requested 2019 1-year ACS:23 rows dataset, representing 23 counties meet “total population 65,000 greater” threshold required included 1-year ACS data.","code":"\nwi_income <- get_acs(\n  geography = \"county\", \n  variables = \"B19013_001\", \n  state = \"WI\",\n  year = 2020\n)\ndane_income <- get_acs(\n  geography = \"tract\", \n  variables = \"B19013_001\", \n  state = \"WI\", \n  county = \"Dane\",\n  year = 2020\n)\nnrow(wi_income)## [1] 72\nwi_income_1yr <- get_acs(\n  geography = \"county\", \n  variables = \"B19013_001\", \n  state = \"WI\",\n  year = 2019,\n  survey = \"acs1\"\n)\n\nnrow(wi_income_1yr)## [1] 23"},{"path":"an-introduction-to-tidycensus.html","id":"searching-for-variables-in-tidycensus","chapter":"2 An introduction to tidycensus","heading":"2.3 Searching for variables in tidycensus","text":"One additional challenge searching Census variables understanding variable IDs, required fetch data Census ACS APIs. thousands variables available across different datasets summary files. make searching easier R users, tidycensus offers load_variables() function. function obtains dataset variables Census Bureau website formats fast searching, ideally RStudio.function takes two required arguments: year, takes year endyear Census dataset ACS sample, dataset, references dataset name. 2000 2010 Decennial Census, use \"sf1\" \"sf2\" dataset name access variables Summary Files 1 2, respectively. 2000 Decennial Census also accepts \"sf3\" \"sf4\" Summary Files 3 4. 2020, dataset supported time writing \"pl\" PL-94171 Redistricting dataset; datasets supported 2020 Census data released. example request look like load_variables(year = 2020, dataset = \"pl\") variables 2020 Decennial Census Redistricting data.variables American Community Survey, users specify dataset \"acs1\" 1-year ACS \"acs5\" 5-year ACS. suffix dataset names specified, users retrieve data ACS Detailed Tables. Variables ACS Data Profile, Summary Tables, Comparison Profile also available appending suffixes /profile, /summary, /cprofile, respectively. example, user requesting variables 2020 5-year ACS Detailed Tables specify load_variables(year = 2020, dataset = \"acs5\"); request variables Data Profile load_variables(year = 2020, dataset = \"acs5/profile\"). addition datasets, ACS Supplemental Estimates variables can accessed dataset name \"acsse\".function requires processing thousands variables Census Bureau may take moments depending user’s internet connection, user can specify cache = TRUE function call store data user’s cache directory future access. subsequent calls load_variables() function, cache = TRUE direct function look cache directory variables rather Census website.example load_variables() works follows:\nTable 2.9: Variables 2012-2016 5-year ACS\nreturned data frame always three columns: name, refers Census variable ID; label, descriptive data label variable; concept, refers topic data often corresponds table Census data. 5-year ACS detailed tables, returned data frame also includes fourth column, geography, specifies smallest geography given variable available Census API. illustrated , data frame can filtered using tidyverse tools variable exploration. However, RStudio integrated development environment includes interactive data viewer ideal browsing dataset, allows interactive sorting filtering. data viewer can accessed View() function:\nFigure 2.1: Variable viewer RStudio\nbrowsing table way, users can identify appropriate variable IDs (found name column) can passed variables parameter get_acs() get_decennial(). Users may note raw variable IDs ACS, consumed API, require suffix E M. tidycensus require suffix, automatically return estimate margin error given requested variable. Additionally, users desire entire table related variables ACS, user supply characters prior underscore variable ID table parameter.","code":"\nv16 <- load_variables(2016, \"acs5\", cache = TRUE)\nView(v16)"},{"path":"an-introduction-to-tidycensus.html","id":"data-structure-in-tidycensus","chapter":"2 An introduction to tidycensus","heading":"2.4 Data structure in tidycensus","text":"Key design philosophy tidycensus interpretation tidy data. Following Wickham (2014), “tidy” data defined follows:observation forms row;variable forms column;observational unit forms table.default, tidycensus returns tibble ACS decennial Census data “tidy” format. decennial Census data, include four columns:GEOID, representing Census ID code uniquely identifies geographic unit;GEOID, representing Census ID code uniquely identifies geographic unit;NAME, represents descriptive name unit;NAME, represents descriptive name unit;variable, contains information Census variable name corresponding row;variable, contains information Census variable name corresponding row;value, contains data values unit-variable combination. ACS data, two columns replace value column: estimate, represents ACS estimate, moe, representing margin error around estimate.value, contains data values unit-variable combination. ACS data, two columns replace value column: estimate, represents ACS estimate, moe, representing margin error around estimate.Given terminology used Census Bureau distinguish data, important provide clarifications nomenclature . Census ACS variables, specific series data available enumeration unit, interpreted tidycensus characteristics enumeration units. turn, rows datasets returned output = \"tidy\", default setting get_acs() get_decennial() functions, represent data unique unit-variable combinations. example illustrated income groups state 2016 1-year American Community Survey.\nTable 2.10: Household income groups state, 2016 1-year ACS\nexample, row represents state-characteristic combinations, consistent tidy data model. Alternatively, user desires variables spread across columns dataset, setting output = \"wide\" enable . ACS data, estimates margins error ACS variable found columns. example:\nTable 2.11: Income table wide form\nwide-form dataset includes GEOID NAME columns, tidy dataset, also characterized estimate/margin error pairs across columns Census variable table.","code":"\nhhinc <- get_acs(\n  geography = \"state\", \n  table = \"B19001\", \n  survey = \"acs1\",\n  year = 2016\n)\nhhinc_wide <- get_acs(\n  geography = \"state\", \n  table = \"B19001\", \n  survey = \"acs1\", \n  year = 2016,\n  output = \"wide\"\n)"},{"path":"an-introduction-to-tidycensus.html","id":"understanding-geoids","chapter":"2 An introduction to tidycensus","heading":"2.4.1 Understanding GEOIDs","text":"GEOID column returned default tidycensus can used uniquely identify geographic units given dataset. geographies within core Census hierarchy (Census block state, discussed Section 1.2), GEOIDs can used uniquely identify specific units well units’ parent geographies. Let’s take example households Census block 2020 Census Cimarron County, Oklahoma.\nTable 2.12: Households block Cimarron County, Oklahoma\nmapping GEOID NAME columns returned 2020 Census block data offers insight GEOIDs work geographies within core Census hierarchy. Take first block table, Block 1110, GEOID 400259503001110. GEOID value breaks follows:first two digits, 40, correspond Federal Information Processing Series (FIPS) code state Oklahoma. states US territories, along geographies Census Bureau tabulates data, FIPS code can uniquely identify geography.first two digits, 40, correspond Federal Information Processing Series (FIPS) code state Oklahoma. states US territories, along geographies Census Bureau tabulates data, FIPS code can uniquely identify geography.Digits 3 5, 025, representative Cimarron County. three digits uniquely identify Cimarron County within Oklahoma. County codes generally combined corresponding state codes uniquely identify county within United States, three-digit codes repeated across states. Cimarron County’s code example 40025.Digits 3 5, 025, representative Cimarron County. three digits uniquely identify Cimarron County within Oklahoma. County codes generally combined corresponding state codes uniquely identify county within United States, three-digit codes repeated across states. Cimarron County’s code example 40025.next six digits, 950300, represent block’s Census tract. tract name NAME column Census Tract 9503; six-digit tract ID right-padded zeroes.next six digits, 950300, represent block’s Census tract. tract name NAME column Census Tract 9503; six-digit tract ID right-padded zeroes.twelfth digit, 1, represents parent block group Census block. nine block groups Census tract, block group name exceed 9.twelfth digit, 1, represents parent block group Census block. nine block groups Census tract, block group name exceed 9.last three digits, 110, represent individual Census block, though digits combined parent block group digit form block’s name.last three digits, 110, represent individual Census block, though digits combined parent block group digit form block’s name.geographies outside core Census hierarchy, GEOIDs uniquely identify geographic units include IDs parent geographies degree nest within . example, geography nests within states may cross county boundaries like school districts include state GEOID GEOID unique digits . Geographies like core-based statistical areas nest within states fully unique GEOIDs, independent geographic level aggregation states.","code":"\ncimarron_blocks <- get_decennial(\n  geography = \"block\",\n  variables = \"H1_001N\",\n  state = \"OK\",\n  county = \"Cimarron\",\n  year = 2020,\n  sumfile = \"pl\"\n)"},{"path":"an-introduction-to-tidycensus.html","id":"renaming-variable-ids","chapter":"2 An introduction to tidycensus","heading":"2.4.2 Renaming variable IDs","text":"Census variables IDs can cumbersome type remember course R session. , tidycensus built-tools automatically rename variable IDs requested user. example, let’s say user requesting data median household income (variable ID B19013_001) median age (variable ID B01002_001). passing named vector variables parameter get_acs() get_decennial(), functions return desired names rather Census variable IDs. Let’s examine counties Georgia 2016-2020 five-year ACS.\nTable 2.13: Multi-variable dataset Georgia counties\nACS variable IDs, found variable column, replaced medage medinc, requested. wide-form dataset requested, tidycensus still append E M specified column names, illustrated .\nTable 2.14: Georgia dataset wide form\nMedian household income county represented medincE, estimate, medincM, margin error. time writing, custom variable names available variables table, users always know number variables found table beforehand.","code":"\nga <- get_acs(\n  geography = \"county\",\n  state = \"Georgia\",\n  variables = c(medinc = \"B19013_001\",\n                medage = \"B01002_001\"),\n  year = 2020\n)\nga_wide <- get_acs(\n  geography = \"county\",\n  state = \"Georgia\",\n  variables = c(medinc = \"B19013_001\",\n                medage = \"B01002_001\"),\n  output = \"wide\",\n  year = 2020\n)"},{"path":"an-introduction-to-tidycensus.html","id":"other-census-bureau-datasets-in-tidycensus","chapter":"2 An introduction to tidycensus","heading":"2.5 Other Census Bureau datasets in tidycensus","text":"mentioned earlier chapter, tidycensus grant access datasets available Census API; users look censusapi package (Recht 2021) functionality. However, Population Estimates ACS Migration Flows APIs accessible get_estimates() get_flows() functions, respectively. section includes brief examples .","code":""},{"path":"an-introduction-to-tidycensus.html","id":"using-get_estimates","chapter":"2 An introduction to tidycensus","heading":"2.5.1 Using get_estimates()","text":"Population Estimates Program, PEP, provides yearly estimates US population components decennial Censuses. differs ACS directly based dedicated survey, rather projects forward data recent decennial Census based birth, death, migration rates. turn, estimates PEP differ slightly may see data returned get_acs(), estimates produced using different methodology.One advantage using PEP retrieve data allows access indicators used produce intercensal population estimates. indicators can specified variables direction get_estimates() function tidycensus, requested bulk using product argument. products available include \"population\", \"components\", \"housing\", \"characteristics\". example, can request components change population estimates 2019 specific county:\nTable 2.15: Components change estimates Queens County, NY\nreturned variables include raw values births deaths (BIRTHS DEATHS) previous 12 months, defined mid-year 2018 (July 1) mid-year 2019. Crude rates per 1000 people Queens County also available RBIRTH RDEATH. NATURALINC, natural increase, measures number births minus number deaths. Net domestic international migration also available counts rates, NETMIG variable accounts overall migration, domestic international included. Alternatively, single variable vector variables can requested variable argument, output = \"wide\" argument can also used spread variable names across columns.product = \"characteristics\" argument also unique options. argument breakdown lets users get breakdowns population estimates US, states, counties \"AGEGROUP\", \"RACE\", \"SEX\", \"HISP\" (Hispanic origin). set TRUE, breakdown_labels argument return informative labels population estimates. example, get population estimates sex Hispanic origin metropolitan areas, can use following code:\nTable 2.16: Population characteristics Louisiana\nvalue column gives estimate characterized population labels SEX HISP columns. example, estimated population value 2019 Hispanic males Louisiana 131,071.","code":"\nlibrary(tidycensus)\nlibrary(tidyverse)\n\nqueens_components <- get_estimates(\n  geography = \"county\",\n  product = \"components\",\n  state = \"NY\",\n  county = \"Queens\",\n  year = 2019\n)\nlouisiana_sex_hisp <- get_estimates(\n  geography = \"state\",\n  product = \"characteristics\",\n  breakdown = c(\"SEX\", \"HISP\"),\n  breakdown_labels = TRUE,\n  state = \"LA\",\n  year = 2019\n)"},{"path":"an-introduction-to-tidycensus.html","id":"using-get_flows","chapter":"2 An introduction to tidycensus","heading":"2.5.2 Using get_flows()","text":"version 1.0, tidycensus also includes support ACS Migration Flows API. flows API returns information - -migration states, counties, metropolitan areas. default, function allows analysis -migrants, emigrants, net migration given geography using data given 5-year ACS sample. example , request migration data Honolulu County, Hawaii. -migration world regions available along -migration net migration US locations.\nTable 2.17: Migration flows data Honolulu, HI\nget_flows() also includes functionality migration flow mapping; advanced feature covered Section 6.6.1.","code":"\nhonolulu_migration <- get_flows(\n  geography = \"county\",\n  state = \"HI\",\n  county = \"Honolulu\",\n  year = 2019\n)"},{"path":"an-introduction-to-tidycensus.html","id":"debugging-tidycensus-errors","chapter":"2 An introduction to tidycensus","heading":"2.6 Debugging tidycensus errors","text":"times, may think ’ve formatted use tidycensus function correctly Census API doesn’t return data expected. Whenever possible, tidycensus carries error message Census API translates common errors user. example , user mis-typed variable ID:“unknown variable” error message Census API carried user. instances, users might request geographies available given dataset:user attempted get bachelor’s degree attainment CBSA Ohio ACS Data Profile. However, CBSA geographies available state given many CBSAs cross state boundaries. response, API returns “unsupported geography hierarchy” error.assist debugging errors, generally help users understand tidycensus functions translated Census API calls, tidycensus offers parameter show_call set TRUE prints actual API call tidycensus making Census API.printed URL https://api.census.gov/data/2019/acs/acs5/profile?get=DP02_0068PE%2CDP02_0068PM%2CNAME&=metropolitan%20statistical%20area%2Fmicropolitan%20statistical%20area%3A%2A can copy-pasted web browser users can see raw JSON returned Census API inspect results.common use-case show_call = TRUE understand data available API, especially functions tidycensus returning NA certain rows. raw API call contains missing values given variables, confirm requested data available API given geography.","code":"\nstate_pop <- get_decennial(\n  geography = \"state\",\n  variables = \"P01001\",\n  year = 2010\n)## Error : Your API call has errors.  The API message returned is error: error: unknown variable 'P01001'.## Error in UseMethod(\"gather\"): no applicable method for 'gather' applied to an object of class \"character\"\ncbsa_ohio <- get_acs(\n  geography = \"cbsa\",\n  variables = \"DP02_0068P\",\n  state = \"OH\",\n  year = 2019\n)## Error: Your API call has errors.  The API message returned is error: unknown/unsupported geography heirarchy.\ncbsa_bachelors <- get_acs(\n  geography = \"cbsa\",\n  variables = \"DP02_0068P\",\n  year = 2019,\n  show_call = TRUE\n)## Getting data from the 2015-2019 5-year ACS## Using the ACS Data Profile## Census API call: https://api.census.gov/data/2019/acs/acs5/profile?get=DP02_0068PE%2CDP02_0068PM%2CNAME&for=metropolitan%20statistical%20area%2Fmicropolitan%20statistical%20area%3A%2A[[\"DP02_0068PE\",\"DP02_0068PM\",\"NAME\",\"metropolitan statistical area/micropolitan statistical area\"],\n[\"15.7\",\"1.5\",\"Big Stone Gap, VA Micro Area\",\"13720\"],\n[\"31.6\",\"1.0\",\"Billings, MT Metro Area\",\"13740\"],\n[\"27.9\",\"0.7\",\"Binghamton, NY Metro Area\",\"13780\"],\n[\"31.4\",\"0.4\",\"Birmingham-Hoover, AL Metro Area\",\"13820\"],\n[\"33.3\",\"1.0\",\"Bismarck, ND Metro Area\",\"13900\"],\n[\"21.2\",\"2.0\",\"Blackfoot, ID Micro Area\",\"13940\"],\n[\"35.2\",\"1.1\",\"Blacksburg-Christiansburg, VA Metro Area\",\"13980\"],\n[\"44.8\",\"1.1\",\"Bloomington, IL Metro Area\",\"14010\"],\n[\"40.8\",\"1.2\",\"Bloomington, IN Metro Area\",\"14020\"],\n[\"24.9\",\"1.0\",\"Bloomsburg-Berwick, PA Metro Area\",\"14100\"],\n..."},{"path":"an-introduction-to-tidycensus.html","id":"exercises","chapter":"2 An introduction to tidycensus","heading":"2.7 Exercises","text":"Review available geographies tidycensus geography table chapter. Acquire data median age (variable B01002_001) geography yet used.Review available geographies tidycensus geography table chapter. Acquire data median age (variable B01002_001) geography yet used.Use load_variables() function find variable interests haven’t used yet. Use get_acs() fetch data 2016-2020 ACS counties state live, visited, like visit.Use load_variables() function find variable interests haven’t used yet. Use get_acs() fetch data 2016-2020 ACS counties state live, visited, like visit.","code":""},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"wrangling-census-data-with-tidyverse-tools","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3 Wrangling Census data with tidyverse tools","text":"One popular frameworks data analysis R tidyverse, suite packages designed integrated data wrangling, visualization, modeling. “tidy” long-form data returned default tidycensus designed work well tidyverse analytic workflows. chapter provides overview use tidyverse tools gain additional insights US Census data retrieved tidycensus. concludes discussion margins error (MOEs) American Community Survey wrangle interpret MOEs appropriately.","code":""},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"the-tidyverse","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3.1 The tidyverse","text":"tidyverse collection R packages designed work together common data wrangling, analysis, visualization projects. Many R packages, maintained RStudio, among popular R packages worldwide. key packages ’ll use tidyverse include:readr (Wickham Hester 2021), contains tools importing exporting datasets;readr (Wickham Hester 2021), contains tools importing exporting datasets;dplyr (Wickham et al. 2021), powerful framework data wrangling tasks;dplyr (Wickham et al. 2021), powerful framework data wrangling tasks;tidyr (Wickham 2021b), package reshaping data;tidyr (Wickham 2021b), package reshaping data;purrr (Henry Wickham 2020), comprehensive framework functional programming iteration;purrr (Henry Wickham 2020), comprehensive framework functional programming iteration;ggplot2 (Wickham 2016), data visualization package based Grammar Graphicsggplot2 (Wickham 2016), data visualization package based Grammar GraphicsThe core data structure used tidyverse tibble, R data frame small enhancements improve user experience. tidycensus returns tibbles default.full treatment tidyverse functionality beyond scope book; however, examples chapter introduce several key tidyverse features using US Census Bureau data. general broader treatment tidyverse, recommend R Data Science book (Wickham Grolemund 2017).","code":""},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"exploring-census-data-with-tidyverse-tools","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3.2 Exploring Census data with tidyverse tools","text":"Census data queries using tidycensus, combined core tidyverse functions, excellent ways explore downloaded Census data. Chapter 2 covered download data various Census datasets using tidycensus return data desired format. common next step analytic process involve data exploration, handled wide range tools tidyverse.get started, tidycensus tidyverse packages loaded. “tidyverse” specifically package , rather loads several core packages within tidyverse. package load message gives information:Eight tidyverse packages loaded: ggplot2, tibble (Müller Wickham 2021), purrr, dplyr, readr, tidyr included along stringr (Wickham 2019a) string manipulation forcats (Wickham 2021a) working factors. tools collectively can used many core Census data analysis tasks.","code":"\nlibrary(tidycensus)\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n## ✓ tibble  3.1.6     ✓ dplyr   1.0.8\n## ✓ tidyr   1.2.0     ✓ stringr 1.4.0\n## ✓ readr   2.1.2     ✓ forcats 0.5.1## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()"},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"sorting-and-filtering-data","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3.2.1 Sorting and filtering data","text":"first example, let’s request data median age 2016-2020 ACS get_acs() counties United States. requires specifying geography = \"county\" leaving state set NULL, default.\nTable 3.1: Median age US counties\ndefault method printing data used tibble package shows first 10 rows dataset, case prints counties Alabama. first exploratory data analysis question might involve understanding counties youngest oldest United States measured median age. task can accomplished arrange() function found dplyr package. arrange() sorts dataset values one columns returns sorted result. view dataset ascending order given column, supply data object column name arrange() function.\nTable 3.2: youngest counties US median age\nPer 2016-2020 ACS, youngest county De Baca County, New Mexico. Two five youngest “counties” United States independent cities Virginia, treated county-equivalents. Lexington Radford college towns; Lexington home Washington & Lee University Virginia Military Institute, Radford houses Radford University.retrieve oldest counties United States median age, analyst can use desc() function available dplyr sort estimate column descending order.\nTable 3.3: oldest counties US median age\noldest county United States Sumter County, Florida. Sumter County home Villages, Census-designated place includes large age-restricted community also called Villages.tidyverse includes several tools parsing datasets allow exploration beyond sorting browsing data. filter() function dplyr queries dataset rows given condition evaluates TRUE, retains rows . analysts familiar databases SQL, equivalent clause. helps analysts subset data specific areas characteristics, answer questions like “many counties US median age 50 older?”\nTable 3.4: Counties median age 50 \nFunctions like arrange() filter() operate row values organize data row. tidyverse functions, like tidyr’s separate(), operate columns. NAME column, returned default tidycensus functions, contains basic description location can intuitive GEOID. 2016-2020 ACS, NAME formatted “X County, Y”, X county name Y state name. separate() can split column two columns one retains county name retains state; can useful analysts need complete comparative analysis state.\nTable 3.5: Separate columns county state\nmay noticed existing variable names unquoted referenced tidyverse functions. Many tidyverse functions use non-standard evaluation refer column names, means column names can used arguments directly without quotation marks. Non-standard evaluation makes interactive programming faster, especially beginners; however, can introduce complications writing functions R packages. full treatment non-standard evaluation beyond scope book; Hadley Wickham’s Advanced R (Wickham 2019b) best resource topic ’d like learn .","code":"\nmedian_age <- get_acs(\n  geography = \"county\",\n  variables = \"B01002_001\",\n  year = 2020\n)\narrange(median_age, estimate)\narrange(median_age, desc(estimate))\nfilter(median_age, estimate >= 50)\nseparate(\n  median_age,\n  NAME,\n  into = c(\"county\", \"state\"),\n  sep = \", \"\n)"},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"using-summary-variables-and-calculating-new-columns","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3.2.2 Using summary variables and calculating new columns","text":"Data Census ACS tables, example , frequently comprised variables individually constitute sub-categories numbers households different household income bands. One limitation approach , however, data resulting analysis return estimated counts, difficult compare across geographies. example, Maricopa County Arizona state’s populous county 4.3 million residents; second-largest county, Pima, just 1 million residents six state’s 15 counties fewer 100,000 residents. turn, comparing Maricopa’s estimates smaller counties state often inappropriate.solution issue might involve normalizing estimated count data dividing overall population sub-group derived. Appropriate denominators ACS tables frequently found tables variables. ACS table B19001, covers number households income bands, variable B19001_001 represents total number households given enumeration unit, removed analysis earlier. Given variable appropriate denominator variables table, merits column facilitate calculation proportions percentages.tidycensus, can accomplished supplying variable ID summary_var parameter get_acs() get_decennial() functions. using get_decennial(), create two new columns decennial Census datasets, summary_var summary_value, representing summary variable ID summary variable’s value. using get_acs(), using summary_var creates three new columns ACS datasets, summary_var, summary_est, summary_moe, include ACS estimate margin error summary variable.information hand, normalizing data straightforward. following example uses summary_var parameter compare population counties Arizona race & Hispanic origin baseline populations, using data 2016-2020 ACS.\nTable 3.6: Race ethnicity Arizona\nusing dplyr’s mutate() function, calculate new column, percent, representing percentage Census tract’s population corresponds racial/ethnic group 2016-2020. select() function, also dplyr, retains columns need view.\nTable 3.7: Race ethnicity Arizona percentages\nexample introduces additional syntax common tidyverse data analyses. %>% operator magrittr R package (Bache Wickham 2020) pipe operator allows analysts develop analytic pipelines, deeply embedded tidyverse-centric data analytic workflows. pipe operator passes result given line code first argument code next line. turn, analysts can develop data analysis pipelines related operations fit together coherent way.tidyverse developers recommend pipe operator read “”. code can turn interpreted “Create new data object az_race_percent using existing data object az_race creating new percent column selecting NAME, variable, percent columns.”Since R version 4.1, base installation R also includes pipe operator, |>. works much way magrittr pipe %>%, though %>% small additional features make work well within tidyverse analysis pipelines. turn, %>% used examples throughout book.","code":"\nrace_vars <- c(\n  White = \"B03002_003\",\n  Black = \"B03002_004\",\n  Native = \"B03002_005\",\n  Asian = \"B03002_006\",\n  HIPI = \"B03002_007\",\n  Hispanic = \"B03002_012\"\n)\n\naz_race <- get_acs(\n  geography = \"county\",\n  state = \"AZ\",\n  variables = race_vars,\n  summary_var = \"B03002_001\",\n  year = 2020\n) \naz_race_percent <- az_race %>%\n  mutate(percent = 100 * (estimate / summary_est)) %>%\n  select(NAME, variable, percent)"},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"group-wise-census-data-analysis","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3.3 Group-wise Census data analysis","text":"split-apply-combine model data analysis, discussed Wickham (2011), powerful framework analyzing demographic data. general terms, analyst apply framework follows:analyst identifies salient groups dataset want make comparisons. dataset split multiple pieces, one group.analyst identifies salient groups dataset want make comparisons. dataset split multiple pieces, one group.function applied group turn. might simple summary function, taking maximum calculating mean, custom function defined analyst.function applied group turn. might simple summary function, taking maximum calculating mean, custom function defined analyst.Finally, results function applied group combined back single dataset, allowing analyst compare results group.Finally, results function applied group combined back single dataset, allowing analyst compare results group.Given hierarchical nature US Census Bureau data, “groups” across analysts can make comparisons found just every analytic tasks. many cases, split-apply-combine model data analysis useful analysts make sense patterns trends found Census data.tidyverse, split-apply-combine implemented group_by() function dplyr package. group_by() work analyst splitting dataset groups, allowing subsequent functions used analyst analytic pipeline applied group combined back single dataset. examples follow illustrate common group-wise analyses.","code":""},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"making-group-wise-comparisons","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3.3.1 Making group-wise comparisons","text":"az_race_percent dataset created example dataset suitable group-wise data analysis. includes two columns used group definitions: NAME, representing county, variable, representing racial ethnic group. Split-apply-combine used either group definition make comparisons data Arizona across categories.first example, can deploy group-wise data analysis identify largest racial ethnic group county Arizona. involves setting data analysis pipeline magrittr pipe calculating grouped filter filter() operation applied specific group. example, filter condition specified percent == max(percent). can read analytic pipeline “Create new dataset, largest_group, using az_race_dataset grouping dataset NAME column filtering rows equal maximum value percent group.”\nTable 3.8: Largest group county Arizona\nresult grouped filter allows us review common racial ethnic group Arizona County along percentages vary. example, two Arizona counties (Greenlee Navajo), none racial ethnic groups form majority population.group_by() commonly paired summarize() function data analysis pipelines. summarize() generates new, condensed dataset default returns column grouping variable(s) columns representing results one functions applied groups. example , median() function used identify median percentage racial & ethnic groups dataset across counties Arizona. turn, variable passed group_by() grouping variable.\nTable 3.9: Median percentage group\nresult operation tells us median county percentage racial ethnic group state Arizona. broader analysis might involve calculation percentages hierarchically, finding median county percentage given attributes across states, example.","code":"\nlargest_group <- az_race_percent %>%\n  group_by(NAME) %>%\n  filter(percent == max(percent))\naz_race_percent %>%\n  group_by(variable) %>%\n  summarize(median_pct = median(percent))"},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"tabulating-new-groups","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3.3.2 Tabulating new groups","text":"examples , suitable groups NAME variable columns already found data retrieved get_acs(). Commonly, analysts also need calculate new custom groups address specific analytic questions. example, variables ACS table B19001 represent groups households whose household incomes fall variety categories: less $10,000/year, $10,000/year $19,999/year, forth. categories may granular needed analyst. , analyst might take following steps: 1) recode ACS variables wider income bands; 2) group data wider income bands; 3) calculate grouped sums generate new estimates.Consider following example, using household income data Minnesota counties 2012-2016 ACS:\nTable 3.10: Table B19001 counties Minnesota, 2012-2016 ACS\ndata include household income categories county rows. However, let’s say need three income categories purposes analysis: $35,000/year, $35,000/year $75,000/year, $75,000/year .first need transformation data recode variables appropriately. First, remove variable B19001_001, represents total number households county. Second, use case_when() function dplyr package identify groups variables correspond desired groupings. Given variables ordered ACS table relationship household income values, less operator can used identify groups.syntax case_when() can appear complex beginners, worth stepping function works. Inside mutate() function, used create new variable named incgroup, case_when() steps series logical conditions evaluated order similar series /else statements. first condition evaluated, telling function assign value below35k rows variable value comes \"B19001_008\" - case B19001_002 (income less $10,000) B19001_007 (income $30,000 $34,999). second condition evaluated rows accounted first condition. means case_when() knows assign \"bw35kand75k\" income group $10,000 even though variable comes B19001_013. final condition case_when() can set TRUE scenario translates “values.”\nTable 3.11: Recoded household income categories\nresult illustrates different variable IDs mapped new, recoded categories specified case_when(). group_by() %>% summarize() workflow can now applied recoded categories county tabulate data smaller number groups.\nTable 3.12: Grouped sums income bands\ndata now reflect new estimates group county.","code":"\nmn_hh_income <- get_acs(\n  geography = \"county\",\n  table = \"B19001\",\n  state = \"MN\",\n  year = 2016\n)\nmn_hh_income_recode <- mn_hh_income %>%\n  filter(variable != \"B19001_001\") %>%\n  mutate(incgroup = case_when(\n    variable < \"B19001_008\" ~ \"below35k\", \n    variable < \"B19001_013\" ~ \"bw35kand75k\", \n    TRUE ~ \"above75k\"\n  )) \nmn_group_sums <- mn_hh_income_recode %>%\n  group_by(GEOID, incgroup) %>%\n  summarize(estimate = sum(estimate))"},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"comparing-acs-estimates-over-time","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3.4 Comparing ACS estimates over time","text":"common task working Census data examine demographic change time. Data Census API - consequently tidycensus - go back 2000 Decennial Census. historical analysts want go even back, decennial Census data available since 1790 National Historical Geographic Information System, NHGIS, covered detail Chapter 11.","code":""},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"time-series-analysis-some-cautions","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3.4.1 Time-series analysis: some cautions","text":"engaging sort time series analysis Census data, analysts need account potential problems can emerge using Census data longitudinally. One major issue can emerge geography changes time. example, let’s say interested analyzing data Oglala Lakota County, South Dakota. can get recent data ACS using tools learned Chapter 2:\nTable 3.13: 2016-2020 age table Oglala Lakota County, SD\nunderstand age composition county changed past 10 years, may want look 2006-2010 ACS county. Normally, just change year argument 2010:request errors, don’t get informative error message back API discussed Section 2.6. problem Oglala Lakota County different name 2010, Shannon County, meaning county = \"Oglala Lakota\" argument return data. turn, equivalent code 2006-2010 ACS use county = \"Shannon\".\nTable 3.14: 200-2010 age table Oglala Lakota County, SD (named Shannon County)\nNote differences GEOID column two tables data. county geographic entity changes name, Census Bureau assigns new GEOID, meaning analysts need take care dealing changes. full listing geography changes available Census website year.addition changes geographic identifiers, variable IDs can change time well. example, ACS Data Profile commonly used pre-computed normalized ACS estimates. Let’s say interested analyzing percentage residents age 25 4-year college degree counties Colorado 2019 1-year ACS. ’d first look appropriate variable ID load_variables(2019, \"acs1/profile\") use get_acs():\nTable 3.15: ACS Data Profile data 2019\nget back data counties population 65,000 greater geographies available 1-year ACS. data make sense: Boulder County, home University Colorado, high percentage population 4-year degree higher. However, run exact query 2018 1-year ACS:\nTable 3.16: ACS Data Profile data 2018\nvalues completely different, clearly percentages! variable IDs Data Profile unique year turn used time-series analysis. returned results represent civilian population age 18 , nothing educational attainment.","code":"\noglala_lakota_age <- get_acs(\n  geography = \"county\",\n  state = \"SD\",\n  county = \"Oglala Lakota\",\n  table = \"B01001\",\n  year = 2020\n)\noglala_lakota_age_10 <- get_acs(\n  geography = \"county\",\n  state = \"SD\",\n  county = \"Oglala Lakota\",\n  table = \"B01001\",\n  year = 2010\n)## Error: Your API call has errors.  The API message returned is .\noglala_lakota_age_10 <- get_acs(\n  geography = \"county\",\n  state = \"SD\",\n  county = \"Shannon\",\n  table = \"B01001\",\n  year = 2010\n)\nco_college19 <- get_acs(\n  geography = \"county\",\n  variables = \"DP02_0068P\",\n  state = \"CO\",\n  survey = \"acs1\",\n  year = 2019\n)\nco_college18 <- get_acs(\n  geography = \"county\",\n  variables = \"DP02_0068P\",\n  state = \"CO\",\n  survey = \"acs1\",\n  year = 2018\n)"},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"preparing-time-series-acs-estimates","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3.4.2 Preparing time-series ACS estimates","text":"safest option time-series analysis ACS use Comparison Profile Tables. tables available 1-year 5-year ACS, allow comparison demographic indicators past five years given year. Using Comparison Profile tables also brings benefit additional variable harmonization, inflation-adjusted income estimates.Data Comparison Profile accessed just like ACS variables using get_acs(). example illustrates get data ACS Comparison Profile inflation-adjusted median household income counties county-equivalents Alaska.2016-2020 ACS, “comparison year” 2015, representing closest non-overlapping 5-year dataset, case 2011-2015. can examine results, inflation-adjusted appropriate comparison:\nTable 3.17: Comparative income data ACS CP tables\n","code":"\nak_income_compare <- get_acs(\n  geography = \"county\",\n  variables = c(\n    income15 = \"CP03_2015_062\",\n    income20 = \"CP03_2020_062\"\n  ),\n  state = \"AK\",\n  year = 2020\n)"},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"iterating-over-acs-years-with-tidyverse-tools","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3.4.2.1 Iterating over ACS years with tidyverse tools","text":"Using Detailed Tables also represents safer option Data Profile, ensures variable IDs remain consistent across years allowing consistent correct analysis. said, still potential pitfalls account using Detailed Tables. Census Bureau add remove variables survey survey depending data needs data availability. example, questions sometimes added removed ACS survey meaning won’t always able get every data point every year geography combination. turn, still important check data availability using load_variables() years plan analyze carrying time-series analysis.Let’s re-engineer analysis educational attainment Colorado counties, computed time series 2010 2019. Information “bachelor’s degree higher” split sex across different tiers educational attainment detailed tables, found ACS table 15002. Given need variables (representing estimates populations age 25+ finished 4-year degree graduate degrees, sex), ’ll request variables directly rather entire B15002 table.’ll now use variables request data college degree holders ACS counties Colorado 1-year ACS surveys 2010 2019. cases, process streamlined iteration. Thus far, familiar using year argument get_acs() request data specific year. Writing ten different calls get_acs(), however - one year - tedious require fair amount repetitive code! Iteration helps us avoid repetitive coding allows us carry process sequence values. Programmers familiar iteration likely know “loop” operators like , available base R programming languages variety. Base R also includes *apply() family functions (e.g. lapply(), mapply(), sapply()), iterates sequence values applies given function value.tidyverse approach iteration found purrr package. purrr includes variety functions designed integrate well workflows require iteration use tidyverse tools. map_*() family functions iterate values try return desired result; map() returns list, map_int() returns integer vector, map_chr() returns character vector, example. tidycensus, map_dfr() function particularly useful. map_dfr() iterates input applies function process defined user, row-binds result single data frame. example illustrates works years 2010 2019.users newer R, iteration purrr syntax can feel complex, worth stepping code sample works.First, numeric vector years defined syntax 2010:2019. create vector years 1-year intervals. values set names vector well, map_dfr() additional functionality working named objects.First, numeric vector years defined syntax 2010:2019. create vector years 1-year intervals. values set names vector well, map_dfr() additional functionality working named objects.map_dfr() takes three arguments .\nfirst argument object map_dfr() iterate , case years vector. means process set run element years.\nsecond argument formula specify tilde (~) operator curly braces ({...}). code inside curly braces run element years. local variable .x, used inside formula, takes value years sequentially. turn, running equivalent get_acs() year = 2010, year = 2011, forth. get_acs() run year, result combined single output data frame.\n.id argument, optional used , creates new column output data frame contains values equivalent names input object, case years. setting .id = \"year\", tell map_dfr() name new column contain values year.\nmap_dfr() takes three arguments .first argument object map_dfr() iterate , case years vector. means process set run element years.second argument formula specify tilde (~) operator curly braces ({...}). code inside curly braces run element years. local variable .x, used inside formula, takes value years sequentially. turn, running equivalent get_acs() year = 2010, year = 2011, forth. get_acs() run year, result combined single output data frame..id argument, optional used , creates new column output data frame contains values equivalent names input object, case years. setting .id = \"year\", tell map_dfr() name new column contain values year.Let’s review result:\nTable 3.18: Educational attainment time\nresult long-form dataset contains time series requested ACS variable county Colorado available 1-year ACS. code outlines group_by() %>% summarize() workflow calculating percentage population age 25 4-year college degree, uses pivot_wider() function tidyr package spread years across columns tabular data display.\nTable 3.19: Percent college year\nparticular format suitable data display writing Excel spreadsheet colleagues R-based. Methods visualization time-series estimates ACS covered Section 4.4.","code":"\ncollege_vars <- c(\"B15002_015\",\n                  \"B15002_016\",\n                  \"B15002_017\",\n                  \"B15002_018\",\n                  \"B15002_032\",\n                  \"B15002_033\",\n                  \"B15002_034\",\n                  \"B15002_035\")\nyears <- 2010:2019\nnames(years) <- years\n\ncollege_by_year <- map_dfr(years, ~{\n  get_acs(\n    geography = \"county\",\n    variables = college_vars,\n    state = \"CO\",\n    summary_var = \"B15002_001\",\n    survey = \"acs1\",\n    year = .x\n  )\n}, .id = \"year\")\ncollege_by_year %>% \n  arrange(NAME, variable, year)\npercent_college_by_year <- college_by_year %>%\n  group_by(NAME, year) %>%\n  summarize(numerator = sum(estimate),\n            denominator = first(summary_est)) %>%\n  mutate(pct_college = 100 * (numerator / denominator)) %>%\n  pivot_wider(id_cols = NAME,\n              names_from = year,\n              values_from = pct_college)"},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"handling-margins-of-error-in-the-american-community-survey-with-tidycensus","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3.5 Handling margins of error in the American Community Survey with tidycensus","text":"topic critical importance working data American Community Survey margin error. opposed decennial US Census, based complete enumeration US population, ACS based sample estimates characterized margins error. default, MOEs returned 90 percent confidence level. can translated roughtly “90 percent sure true value falls within range defined estimate plus minus margin error.”discussed Chapter 2, tidycensus takes opinionated approach margins error. applicable, tidycensus always return margin error associated estimate, option available return estimates . “tidy” long-form data, margins error found moe column; wide-form data, margins error found columns M suffix.confidence level MOE can controlled moe_level argument get_acs(). default moe_level 90, Census Bureau returns default. tidycensus can also return MOEs confidence level 95 99 uses Census Bureau-recommended formulas adjust MOE. example, might look data median household income county Rhode Island using default moe_level 90:\nTable 3.20: Default MOE 90 percent confidence level\nstricter margin error increase size MOE relative estimate.\nTable 3.21: MOE 99 percent confidence level\n","code":"\nget_acs(\n  geography = \"county\",\n  state = \"Rhode Island\",\n  variables = \"B19013_001\",\n  year = 2020\n)\nget_acs(\n  geography = \"county\",\n  state = \"Rhode Island\",\n  variables = \"B19013_001\",\n  year = 2020,\n  moe_level = 99\n)"},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"calculating-derived-margins-of-error-in-tidycensus","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3.5.1 Calculating derived margins of error in tidycensus","text":"small geographies small populations, margins error can get quite large, cases exceeding corresponding estimates. example , can examine data age groups sex population age 65 older Census tracts Salt Lake County, Utah. first generate vector variable IDs want request data ACS using base R functionality.workflow, analyst used load_variables() look variables represent estimates populations age 65 ; includes B01001_020 B01001_025 males, B01001_044 B01001_049 females. Typing variable individually tedious, analyst can use string concatenation generate required vector variable IDs follows:R evaluates nested expressions like , starts inner-expressions evaluates inside . steps taken assemble correct vector variable IDs follows:First, expressions 20:25 44:49 evaluated. colon operator : base R, used two numbers, generate vector numbers first second intervals 1. creates vectors integers 20 25 44 49, serve suffixes variable IDs.First, expressions 20:25 44:49 evaluated. colon operator : base R, used two numbers, generate vector numbers first second intervals 1. creates vectors integers 20 25 44 49, serve suffixes variable IDs.Second, c() function used combine two vectors integers single vector.Second, c() function used combine two vectors integers single vector.Third, paste0() function concatenates string prefix \"B01001_0\" integers vector created c() returns vector variable IDs. paste0() convenient extension flexible paste() function concatenates strings spaces default.Third, paste0() function concatenates string prefix \"B01001_0\" integers vector created c() returns vector variable IDs. paste0() convenient extension flexible paste() function concatenates strings spaces default.resulting variables object, named vars, can now used request variables call get_acs().now want examine margins error around estimates returned data. Let’s focus specific Census tract Salt Lake County using filter():\nTable 3.22: Example Census tract Salt Lake City\nmany cases, margins error exceed corresponding estimates. example, ACS data suggest Census tract 49035100100, male population age 85 (variable ID B01001_0025), anywhere 0 45 people Census tract. can make ACS data small geographies problematic planning analysis purposes.potential solution large margins error small estimates ACS aggregate data upwards satisfactory margin error estimate ratio reached. US Census Bureau publishes formulas appropriately calculating margins error around derived estimates, included tidycensus following functions:moe_sum(): calculates margin error derived sum;moe_product(): calculates margin error derived product;moe_ratio(): calculates margin error derived ratio;moe_prop(): calculates margin error derived proportion.basic form, functions can used constants. example, let’s say ACS estimate 25 margin error 5 around estimate. appropriate denominator estimate 100 margin error 3. determine margin error around derived proportion 0.25, can use moe_prop():margin error around derived estimate 0.25 approximately 0.049.","code":"\nvars <- paste0(\"B01001_0\", c(20:25, 44:49))\n\nvars##  [1] \"B01001_020\" \"B01001_021\" \"B01001_022\" \"B01001_023\" \"B01001_024\"\n##  [6] \"B01001_025\" \"B01001_044\" \"B01001_045\" \"B01001_046\" \"B01001_047\"\n## [11] \"B01001_048\" \"B01001_049\"\nsalt_lake <- get_acs(\n  geography = \"tract\",\n  variables = vars,\n  state = \"Utah\",\n  county = \"Salt Lake\",\n  year = 2020\n)\nexample_tract <- salt_lake %>%\n  filter(GEOID == \"49035100100\")\n\nexample_tract %>% \n  select(-NAME)\nmoe_prop(25, 100, 5, 3)## [1] 0.0494343"},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"calculating-group-wise-margins-of-error","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3.5.2 Calculating group-wise margins of error","text":"margin error functions tidycensus can turn integrated tidyverse-centric analytic pipelines handle large margins error around estimates. Given smaller age bands Salt Lake City dataset characterized much uncertainty analysis, decide scenario aggregate data upwards represent populations aged 65 older sex.code , use case_when() function create new column, sex, represents mapping variables pulled ACS sex categories. employ familiar group_by() %>% summarize() method aggregate data Census tract sex. Notably, call summarize() includes call tidycensus’s moe_sum() function, generate new column represents margin error around derived sum.\nTable 3.23: Grouped margins error\nmargins error relative estimates now much reasonable disaggregated data.said, Census Bureau issues note caution (American Community Survey Office 2020):derived MOE methods approximations users cautious using . methods consider correlation covariance basic estimates. may overestimates underestimates derived estimate’s standard error depending whether two basic estimates highly correlated either positive negative direction. result, approximated standard error may match direct calculations standard errors calculations obtained methods.means “best bet” first search ACS tables see data found aggregated form elsewhere aggregation MOE estimation . many cases, ’ll find aggregated information ACS combined tables, Data Profile, Subject Tables include pre-computed margins error .","code":"\nsalt_lake_grouped <- salt_lake %>%\n  mutate(sex = case_when(\n    str_sub(variable, start = -2) < \"26\" ~ \"Male\",\n    TRUE ~ \"Female\"\n  )) %>%\n  group_by(GEOID, sex) %>%\n  summarize(sum_est = sum(estimate), \n            sum_moe = moe_sum(moe, estimate))"},{"path":"wrangling-census-data-with-tidyverse-tools.html","id":"exercises-1","chapter":"3 Wrangling Census data with tidyverse tools","heading":"3.6 Exercises","text":"ACS Data Profile includes number pre-computed percentages can reduce data wrangling time. variable 2015-2019 ACS “percent population age 25 bachelor’s degree” DP02_0068P. state choosing, use variable determine:\ncounty highest percentage state;\ncounty lowest percentage state;\nmedian value counties chosen state.\nACS Data Profile includes number pre-computed percentages can reduce data wrangling time. variable 2015-2019 ACS “percent population age 25 bachelor’s degree” DP02_0068P. state choosing, use variable determine:county highest percentage state;county highest percentage state;county lowest percentage state;county lowest percentage state;median value counties chosen state.median value counties chosen state.","code":""},{"path":"exploring-us-census-data-with-visualization.html","id":"exploring-us-census-data-with-visualization","chapter":"4 Exploring US Census data with visualization","heading":"4 Exploring US Census data with visualization","text":"core visualization package within tidyverse suite packages ggplot2 (Wickham 2016). Originally developed RStudio chief scientist Hadley Wickham, ggplot2 widely-used visualization framework R developers, accounting 70,000 downloads per day June 2021 RStudio CRAN mirror. ggplot2 allows R users visualize data using layered grammar graphics approach, plot objects initialized upon R user layers plot elements.ggplot2 ideal package visualization US Census data, especially obtained tidy format tidycensus package. powerful capacity basic charts, group-wise comparisons, advanced chart types maps.chapter includes several examples R users can visualize data US Census Bureau using ggplot2. Chart types explored chapter include basic chart types; faceted, “small multiples” plots; population pyramids; margin error plots ACS data; advanced visualizations using extensions ggplot2. Finally, chapter introduces plotly package interactive visualization, can used convert ggplot2 objects interactive web graphics.","code":""},{"path":"exploring-us-census-data-with-visualization.html","id":"basic-census-visualization-with-ggplot2","chapter":"4 Exploring US Census data with visualization","heading":"4.1 Basic Census visualization with ggplot2","text":"critical part Census data analysis process data visualization, analyst examines patterns trends found data graphically. many cases, exploratory analyses outlined previous two chapters augmented significantly accompanying graphics. first section illustrates examples getting started exploratory Census data visualization ggplot2.get started, ’ll return dataset used Section 2.4.2, includes data median household income median age county state Georgia 2016-2020 ACS. requesting data wide format, spread estimate margin error information across columns.","code":"\nlibrary(tidycensus)\n\nga_wide <- get_acs(\n  geography = \"county\",\n  state = \"Georgia\",\n  variables = c(medinc = \"B19013_001\",\n                medage = \"B01002_001\"),\n  output = \"wide\",\n  year = 2020\n)"},{"path":"exploring-us-census-data-with-visualization.html","id":"getting-started-with-ggplot2","chapter":"4 Exploring US Census data with visualization","heading":"4.1.1 Getting started with ggplot2","text":"ggplot2 visualizations initialized ggplot() function, user commonly supplies dataset aesthetic, defined aes() function. Within aes() function, user can specify series mappings onto either data axes characteristics plot, element fill color.initializing ggplot object, users can layer plot elements onto plot object. Essential plot geom, specifies one many chart types available ggplot2. example, geom_bar() create bar chart, geom_line() line chart, geom_point() point plot, forth. Layers linked ggplot object using + operator.One first exploratory graphics analyst want produce examining new dataset histogram, characterizes distribution values column varying lengths bars. first example uses ggplot2 geom_histogram() function generate histogram median household income county Georgia. optional call options(scipen = 999) instructs R avoid using scientific notation output, including ggplot2 tick labels.\nFigure 4.1: Histogram median household income, Georgia counties\nhistogram shows modal median household income Georgia counties around $40,000 per year, longer tail wealthier counties right-hand side plot. histogram, counties organized “bins”, groups equal width along X-axis. Y-axis represents number counties fall within bin. default, ggplot2 organizes data 30 bins; option can changed bins parameter. example, can re-make visualization half previous number bins including argument bins = 15 call geom_histogram().\nFigure 4.2: Histogram number bins reduced 15\nHistograms options visualizing univariate data distributions. popular alternative box--whisker plot, implemented ggplot2 geom_boxplot(). example, column medincE passed y parameter instead x; creates vertical rather horizontal box plot.\nFigure 4.3: Box plot median household income, Georgia counties\ngraphic visualizes distribution median household incomes county Georgia number different components. central box covers interquartile range (IQR, representing 25th 75th percentile values distribution) central line representing value distribution’s median. whiskers extend either minimum maximum values distribution 1.5 times IQR. example, lower whisker extends minimum value, upper whisker extends 1.5 times IQR. Values beyond whiskers represented outliers plot points.","code":"\nlibrary(tidyverse)\noptions(scipen = 999)\n\nggplot(ga_wide, aes(x = medincE)) + \n  geom_histogram()\nggplot(ga_wide, aes(x = medincE)) + \n  geom_histogram(bins = 15)\nggplot(ga_wide, aes(y = medincE)) + \n  geom_boxplot()"},{"path":"exploring-us-census-data-with-visualization.html","id":"visualizing-multivariate-relationships-with-scatter-plots","chapter":"4 Exploring US Census data with visualization","heading":"4.1.2 Visualizing multivariate relationships with scatter plots","text":"part exploratory data analysis process, analysts often want visualize interrelationships Census variables along univariate data distributions discussed . two numeric variables, common exploratory chart scatter plot, maps values one column X-axis values another column Y-axis. resulting plot gives analyst sense nature relationship two variables.Scatter plots implemented ggplot2 geom_point() function, plots points chart relative X Y values observations dataset. requires specification two columns call aes() opposed single column used univariate distribution visualization examples. example follows generates scatter plot visualize relationship county median age county median household income Georgia.\nFigure 4.4: Scatter plot median age median household income, counties Georgia\ngraphic shows cloud points cases can suggest nature correlation two columns. example, however, correlation immediately clear distribution points. Fortunately, ggplot2 includes ability “layer ” additional chart elements help clarify nature relationship two columns. geom_smooth() function draws fitted line representing relationship two columns plot. argument method = \"lm\" draws straight line based linear model fit; smoothed relationships can visualized well method = \"loess\".\nFigure 4.5: Scatter plot linear relationship superimposed graphic\nregression line suggests modest negative relationship two columns, showing county median household income Georgia tends decline slightly median age increases.","code":"\nggplot(ga_wide, aes(x = medageE, y = medincE)) + \n  geom_point()\nggplot(ga_wide, aes(x = medageE, y = medincE)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")"},{"path":"exploring-us-census-data-with-visualization.html","id":"customizing-ggplot2-visualizations","chapter":"4 Exploring US Census data with visualization","heading":"4.2 Customizing ggplot2 visualizations","text":"attractive defaults ggplot2 visualizations allow creation legible graphics little customization. helps greatly exploratory data analysis tasks primary audience analyst exploring dataset. Analysts planning present work external audience, however, want customize appearance plots beyond defaults maximize interpretability. section covers take Census data visualization relatively illegible default polish eventual presentation export R.example, create visualization illustrates percent commuters take public transportation work largest metropolitan areas United States. data come 2019 1-year American Community Survey Data Profile, variable DP03_0021P. determine information, can use tidyverse tools sort data descending order summary variable representing total population retaining 20 largest metropolitan areas population using slice_max() function.\nTable 4.1: Large metro areas public transit commuting share\nreturned data frame 7 columns, standard get_acs() summary variable, 20 rows specified slice_max() command. data can filtered sorted facilitate comparative analysis, also can represented succinctly visualization. tidy format returned get_acs() well-suited visualization ggplot2.basic example , can create bar chart comparing public transportation commute share populous metropolitan areas United States minimum code. first argument ggplot() example name dataset; second argument aesthetic mapping columns plot elements, specified inside aes() function. plot initialization linked + operator geom_col() function create bar chart.\nFigure 4.6: first bar chart ggplot2\nchart visualization metros dataset, tells us little data given lack necessary formatting. x-axis labels lengthy overlap impossible read; axis titles intuitive; data sorted, making difficult compare similar observations.","code":"\nlibrary(tidycensus)\nlibrary(tidyverse)\n\nmetros <-  get_acs(\n  geography = \"cbsa\",\n  variables = \"DP03_0021P\",\n  summary_var = \"B01003_001\",\n  survey = \"acs1\",\n  year = 2019\n) %>%\n  slice_max(summary_est, n = 20)\nggplot(metros, aes(x = NAME, y = estimate)) + \n  geom_col()"},{"path":"exploring-us-census-data-with-visualization.html","id":"improving-plot-legibility","chapter":"4 Exploring US Census data with visualization","heading":"4.2.1 Improving plot legibility","text":"Fortunately, plot can made legible cleaning metropolitan area name, re-ordering data descending order, adding layers plot definition. Additionally, ggplot2 visualization can used combination magrittr piping tidyverse functions, allowing analysts string together data manipulation visualization processes.first step format NAME column intuitive way. NAME column default provides description geography formatted US Census Bureau. However, detailed description like \"Atlanta-Sandy Springs-Roswell, GA Metro Area\" likely unnecessary chart, metropolitan area can represented chart name first principal city, case \"Atlanta\". accomplish , can overwrite NAME column using tidyverse function str_remove(), found stringr package. example uses regular expressions first remove text first dash, remove text first comma dash originally present. two subsequent calls mutate() account various ways metropolitan area names specified.chart, legibility can improved mapping metro name y-axis ACS estimate x-axis, plotting points descending order estimate values. ordering points way accomplished reorder() function, used inside call aes(). result mutate() operations piped ggplot() function example %>% operator, dataset argument ggplot() inferred function.\nFigure 4.7: improved bar chart ggplot2\nplot much legible modifications. Metropolitan areas can directly compared one another, metro area labels convey enough information different places without overwhelming plot long axis labels. However, plot still lacks information inform viewer plot’s content. can accomplished specifying labels inside labs() function. example , ’ll specify title subtitle, modify X Y axis labels defaults.\nFigure 4.8: cleaned-bar chart ggplot2\ninclusion labels provides key information contents plot also gives polished look presentation.","code":"\nmetros %>%\n  mutate(NAME = str_remove(NAME, \"-.*$\")) %>%\n  mutate(NAME = str_remove(NAME, \",.*$\")) %>%\n  ggplot(aes(y = reorder(NAME, estimate), x = estimate)) + \n  geom_col()\nmetros %>%\n  mutate(NAME = str_remove(NAME, \"-.*$\")) %>%\n  mutate(NAME = str_remove(NAME, \",.*$\")) %>%\n  ggplot(aes(y = reorder(NAME, estimate), x = estimate)) + \n  geom_col() +  \n  theme_minimal() + \n  labs(title = \"Public transit commute share\", \n       subtitle = \"2019 1-year ACS estimates\", \n       y = \"\", \n       x = \"ACS estimate\", \n       caption = \"Source: ACS Data Profile variable DP03_0021P via the tidycensus R package\") "},{"path":"exploring-us-census-data-with-visualization.html","id":"custom-styling-of-ggplot2-charts","chapter":"4 Exploring US Census data with visualization","heading":"4.2.2 Custom styling of ggplot2 charts","text":"analyst may comfortable plot -, ggplot2 allows significant customization respect stylistic presentation. example makes modifications. includes styling bars plot different color internal transparency; changing font; customizing axis tick labels.\nFigure 4.9: ggplot2 bar chart custom styling\ncode used produced styled graphic uses following modifications:aesthetic mappings relative column input dataset specified call aes(), ggplot2 geoms can styled directly corresponding functions. Bars ggplot2 characterized color, outline bar, fill. code sets \"navy\" modifies internal transparency bar alpha argument. Finally, width = 0.85 slightly increases spacing bars.aesthetic mappings relative column input dataset specified call aes(), ggplot2 geoms can styled directly corresponding functions. Bars ggplot2 characterized color, outline bar, fill. code sets \"navy\" modifies internal transparency bar alpha argument. Finally, width = 0.85 slightly increases spacing bars.call theme_minimal(), base_size base_family parameters available. base_size specifies base font size plot text elements drawn; defaults 11. many cases, want increase base_size improve plot legibility. base_family allows change font family used plot. example, base_family set \"Verdana\", can use font families accessible R operating system. check information, use system_fonts() function systemfonts package (Pedersen, Ooms, Govett 2021).call theme_minimal(), base_size base_family parameters available. base_size specifies base font size plot text elements drawn; defaults 11. many cases, want increase base_size improve plot legibility. base_family allows change font family used plot. example, base_family set \"Verdana\", can use font families accessible R operating system. check information, use system_fonts() function systemfonts package (Pedersen, Ooms, Govett 2021).scale_x_continuous() function used customize X-axis plot. labels parameter can accept range values including function (used ) formula operates tick labels. scales (Wickham Seidel, n.d.) package contains many useful formatting functions neatly present tick labels, label_percent(), label_dollar(), label_date(). functions also accept arguments modify presentation.scale_x_continuous() function used customize X-axis plot. labels parameter can accept range values including function (used ) formula operates tick labels. scales (Wickham Seidel, n.d.) package contains many useful formatting functions neatly present tick labels, label_percent(), label_dollar(), label_date(). functions also accept arguments modify presentation.","code":"\nlibrary(scales)\n\nmetros %>%\n  mutate(NAME = str_remove(NAME, \"-.*$\")) %>%\n  mutate(NAME = str_remove(NAME, \",.*$\")) %>%\n  ggplot(aes(y = reorder(NAME, estimate), x = estimate)) + \n  geom_col(color = \"navy\", fill = \"navy\", \n           alpha = 0.5, width = 0.85) +  \n  theme_minimal(base_size = 12, base_family = \"Verdana\") + \n  scale_x_continuous(labels = label_percent(scale = 1)) + \n  labs(title = \"Public transit commute share\", \n       subtitle = \"2019 1-year ACS estimates\", \n       y = \"\", \n       x = \"ACS estimate\", \n       caption = \"Source: ACS Data Profile variable DP03_0021P via the tidycensus R package\") "},{"path":"exploring-us-census-data-with-visualization.html","id":"exporting-data-visualizations-from-r","chapter":"4 Exploring US Census data with visualization","heading":"4.2.3 Exporting data visualizations from R","text":"analyst settled visualization design, may want export image R display website, blog post, report. Plots generated RStudio can exported Export > Save Image command; however, analysts want programmatic control image exports can script ggplot2. ggsave() function ggplot2 save last plot generated image file user’s current working directory default. specified file extension control output image format, e.g. .png.ggsave() includes variety options fine-grained control image output. Common options used analysts width height control image size; dpi control image resolution (dots per inch); path specify directory image located. example, code write recent plot generated 8 inch 5 inch image file custom location resolution 300 dpi.","code":"\nggsave(\"metro_transit.png\")\nggsave(\n  filename = \"metro_transit.png\",\n  path = \"~/images\",\n  width = 8,\n  height = 5,\n  units = \"in\",\n  dpi = 300\n)"},{"path":"exploring-us-census-data-with-visualization.html","id":"visualizing-margins-of-error","chapter":"4 Exploring US Census data with visualization","heading":"4.3 Visualizing margins of error","text":"discussed Chapter 3, handling margins error appropriately significant importance analysts working ACS data. tidycensus tools available working margins error data wrangling workflow, also often useful visualize margins error illustrate degree uncertainty around estimates, especially making comparisons estimates.example visualization public transportation mode share metropolitan area largest metros United States, estimates associated margins error; however, margins error relatively small given large population size geographic units represented plot. However, studying demographic trends geographies smaller population size - like counties, Census tracts, block groups - comparisons can subject considerable degree uncertainty.","code":""},{"path":"exploring-us-census-data-with-visualization.html","id":"data-setup","chapter":"4 Exploring US Census data with visualization","heading":"4.3.1 Data setup","text":"example , compare median household incomes counties US state Maine 2016-2020 ACS. , helpful understand basic information counties Maine, number counties total population. can retrieve information tidycensus 2020 decennial Census data.\nTable 4.2: Population sizes counties Maine\nsixteen counties Maine, ranging population maximum 303,069 minimum 16,800. turn, estimates counties smaller population sizes likely subject larger margin error larger baseline populations.Comparing median household incomes sixteen counties illustrates point. Let’s first obtain data tidycensus clean NAME column str_remove() remove redundant information.Using tips covered previous visualization section, can produce plot appropriate styling formatting rank counties.\nFigure 4.10: dot plot median household income county Maine\nvisualization suggests ranking counties wealthiest (Cumberland) poorest (Piscataquis). However, data used generate chart significantly different metropolitan area data used previous example. first example, ACS estimates covered top 20 US metros population - areas populations exceeding 2.8 million. areas, margins error small enough meaningfully change interpretation estimates given large sample sizes used generate . However, discussed Section 3.5, smaller geographies may much larger margins error relative ACS estimates.","code":"\nmaine <- get_decennial(\n  state = \"Maine\",\n  geography = \"county\",\n  variables = c(totalpop = \"P1_001N\"),\n  year = 2020\n) %>%\n  arrange(desc(value))\nmaine_income <- get_acs(\n  state = \"Maine\",\n  geography = \"county\",\n  variables = c(hhincome = \"B19013_001\"),\n  year = 2020\n) %>%\n  mutate(NAME = str_remove(NAME, \" County, Maine\"))\nggplot(maine_income, aes(x = estimate, y = reorder(NAME, estimate))) + \n  geom_point(size = 3, color = \"darkgreen\") + \n  labs(title = \"Median household income\", \n       subtitle = \"Counties in Maine\", \n       x = \"\", \n       y = \"ACS estimate\") + \n  theme_minimal(base_size = 12.5) + \n  scale_x_continuous(labels = label_dollar())"},{"path":"exploring-us-census-data-with-visualization.html","id":"using-error-bars-for-margins-of-error","chapter":"4 Exploring US Census data with visualization","heading":"4.3.2 Using error bars for margins of error","text":"Several county estimates chart quite close one another, may mean ranking counties misleading given margin error around estimates. can explore looking directly data.\nTable 4.3: Margins error Maine\nSpecifically, margins error around estimated median household incomes vary low $1563 (Cumberland County) high $4616 (Sagadahoc County). many cases, margins error around estimated county household income exceed differences counties neighboring ranks, suggesting uncertainty ranks .turn, dot plot like one intended visualize ranking county household incomes Maine may misleading. However, using visualization tools ggplot2, can visualize uncertainty around estimate, giving chart readers sense uncertainty ranking. accomplished geom_errorbar() function, plot horizontal error bars around dot stretch given value around estimate. instance, use moe column determine lengths error bars.\nFigure 4.11: Median household income county Maine error bars shown\nAdding horizontal error bars around point gives us critical information help us understand ranking Maine counties median household income. example, ACS estimate suggests Piscataquis County lowest median household income Maine, large margin error around estimate Piscataquis County suggests either Aroostook Washington Counties conceivably lower median household incomes. Additionally, Hancock County higher estimated median household income Lincoln, Waldo, Knox Counties, margin error plot shows us ranking subject considerable uncertainty.","code":"\nmaine_income %>% \n  arrange(desc(moe))\nggplot(maine_income, aes(x = estimate, y = reorder(NAME, estimate))) + \n  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) + \n  geom_point(size = 3, color = \"darkgreen\") + \n  theme_minimal(base_size = 12.5) + \n  labs(title = \"Median household income\", \n       subtitle = \"Counties in Maine\", \n       x = \"2016-2020 ACS estimate\", \n       y = \"\") + \n  scale_x_continuous(labels = label_dollar())"},{"path":"exploring-us-census-data-with-visualization.html","id":"visualizing-acs-estimates-over-time","chapter":"4 Exploring US Census data with visualization","heading":"4.4 Visualizing ACS estimates over time","text":"Section 3.4.2 covered obtain time series ACS estimates explore temporal demographic shifts. output table usefully represented time series educational attainment Colorado counties, data visualization also commonly used illustrate change time. Arguably common chart type chosen time-series visualization line chart, ggplot2 handles capably geom_line() function.illustrative example, ’ll obtain 1-year ACS data 2005 2019 median home value Deschutes County, Oregon, home city Bend large numbers -migrants recent years Bay Area California. Chapter 3, map_dfr() used iterate named vector years, creating time-series dataset median home value Deschutes County since 2005, use formula specification anonymous functions ~ .x translates function(x) x.\nTable 4.4: Time series median home values Deschutes County, \ninformation can visualized familiar ggplot2 syntax. deschutes_value specified input dataset, year mapped X-axis estimate mapped y-axis. argument group = 1 used help ggplot2 understand connect yearly data points lines given one county visualized. geom_line() draws lines, layer points top lines well highlight actual ACS estimates.\nFigure 4.12: time series chart median home values Deschutes County, \nchart shows rising home values prior 2008 recession; notable drop housing market crash; rising values since 2011, reflecting increased demand wealthy -migrants locations like Bay Area. Given learned previous sections, also several opportunities chart cleanup. can include intuitive tick axis labels; re-designed visual scheme; title caption. can also build margin error information line chart like previous section. ’ll use ggplot2 function geom_ribbon() draw margin error interval around line, helping represent uncertainty ACS estimates.\nFigure 4.13: Deschutes County home value line chart error ranges shown\n","code":"\nyears <- 2005:2019\nnames(years) <- years\n\ndeschutes_value <- map_dfr(years, ~{\n  get_acs(\n    geography = \"county\",\n    variables = \"B25077_001\",\n    state = \"OR\",\n    county = \"Deschutes\",\n    year = .x,\n    survey = \"acs1\"\n  )\n}, .id = \"year\")\nggplot(deschutes_value, aes(x = year, y = estimate, group = 1)) + \n  geom_line() + \n  geom_point()\nggplot(deschutes_value, aes(x = year, y = estimate, group = 1)) + \n  geom_ribbon(aes(ymax = estimate + moe, ymin = estimate - moe), \n              fill = \"navy\",\n              alpha = 0.4) + \n  geom_line(color = \"navy\") + \n  geom_point(color = \"navy\", size = 2) + \n  theme_minimal(base_size = 12) + \n  scale_y_continuous(labels = label_dollar(scale = .001, suffix = \"k\")) + \n  labs(title = \"Median home value in Deschutes County, OR\",\n       x = \"Year\",\n       y = \"ACS estimate\",\n       caption = \"Shaded area represents margin of error around the ACS estimate\")"},{"path":"exploring-us-census-data-with-visualization.html","id":"exploring-age-and-sex-structure-with-population-pyramids","chapter":"4 Exploring US Census data with visualization","heading":"4.5 Exploring age and sex structure with population pyramids","text":"common method visualizing demographic structure particular area population pyramid. Population pyramids typically constructed visualizing population size proportion x-axis; age cohort y-axis; sex represented categorically male female bars mirrored around central axis.","code":""},{"path":"exploring-us-census-data-with-visualization.html","id":"preparing-data-from-the-population-estimates-api","chapter":"4 Exploring US Census data with visualization","heading":"4.5.1 Preparing data from the Population Estimates API","text":"can illustrate type visualization using data Population Estimates API state Utah. first obtain data using get_estimates() function tidycensus 2019 population estimates Census Bureau’s Population Estimates API.\nTable 4.5: Age sex data Utah PEP API\nfunction returns long-form dataset row represents population values broken age sex state Utah. However, key issues dataset must addressed constructing population pyramid. First, several rows represent values don’t need population pyramid visualization. example, first rows dataset represent population values \"sexes\" \"ages\". turn, necessary isolate rows represent five-year age bands sex, remove rows . can resolved data wrangling using tidyverse tools.dataset returned get_estimates(), five-year age bands identified AGEGROUP column beginning word \"Age\". can filter dataset rows match pattern, remove rows represent sexes. leaves us rows represent five-year age bands sex. However, achieve desired visual effect, data one sex must mirror another, split central vertical axis. accomplish , can set values Male values negative.\nTable 4.6: Filtered transformed Utah population data\n","code":"\nutah <- get_estimates(\n  geography = \"state\",\n  state = \"UT\",\n  product = \"characteristics\",\n  breakdown = c(\"SEX\", \"AGEGROUP\"),\n  breakdown_labels = TRUE,\n  year = 2019\n) \nutah_filtered <- filter(utah, str_detect(AGEGROUP, \"^Age\"), \n                  SEX != \"Both sexes\") %>%\n  mutate(value = ifelse(SEX == \"Male\", -value, value))"},{"path":"exploring-us-census-data-with-visualization.html","id":"designing-and-styling-the-population-pyramid","chapter":"4 Exploring US Census data with visualization","heading":"4.5.2 Designing and styling the population pyramid","text":"data now ready visualization. core components pyramid visualization require mapping population value age group chart axes. Sex can mapped fill aesthetic allowing plotting categories color.\nFigure 4.14: first population pyramid\nvisualization represents functional population pyramid nonetheless need cleanup. particular, axis labels informative; y-axis tick labels redundant information (“Age” “years”); x-axis tick labels difficult parse. Cleaning plot allows us use additional visualization options ggplot2. addition specifying appropriate chart labels, can format axis tick labels using appropriate scale_* functions ggplot2 setting X-axis limits show sides 0 equally. particular, involves use custom absolute values represent population sizes, removal redundant age group information. ’ll also make use alternative ggplot2 theme, theme_minimal(), uses white background muted gridlines.\nFigure 4.15: formatted population pyramid Utah\n","code":"\nggplot(utah_filtered, aes(x = value, y = AGEGROUP, fill = SEX)) + \n  geom_col()\nutah_pyramid <- ggplot(utah_filtered, \n                       aes(x = value, \n                           y = AGEGROUP, \n                           fill = SEX)) + \n  geom_col(width = 0.95, alpha = 0.75) + \n  theme_minimal(base_family = \"Verdana\", \n                base_size = 12) + \n  scale_x_continuous(\n    labels = ~ number_format(scale = .001, suffix = \"k\")(abs(.x)),\n    limits = 140000 * c(-1,1)\n  ) + \n  scale_y_discrete(labels = ~ str_remove_all(.x, \"Age\\\\s|\\\\syears\")) + \n  scale_fill_manual(values = c(\"darkred\", \"navy\")) + \n  labs(x = \"\", \n       y = \"2019 Census Bureau population estimate\", \n       title = \"Population structure in Utah\", \n       fill = \"\", \n       caption = \"Data source: US Census Bureau population estimates & tidycensus R package\")\n\nutah_pyramid"},{"path":"exploring-us-census-data-with-visualization.html","id":"visualizing-group-wise-comparisons","chapter":"4 Exploring US Census data with visualization","heading":"4.6 Visualizing group-wise comparisons","text":"One powerful features ggplot2 ability generate faceted plots, also commonly referred small multiples. Faceted plots allow sub-division dataset groups, plotted side--side facilitate comparisons groups. particularly useful examining distributions values vary across different geographies. example shown involves comparison median home values Census tract three counties Portland, Oregon area: Multnomah, contains city Portland, suburban counties Clackamas Washington.\nTable 4.7: Median home values Census tract Portland, area, 2016-2020 ACS\ndatasets obtained tidycensus, NAME column contains descriptive information can parsed make comparisons. case, Census tract ID, county, state separated commas; turn tidyverse separate() function can split column three columns accordingly.\nTable 4.8: Data NAME column split comma\nexplored previous chapters, major strength tidyverse ability perform group-wise data analysis. dimensions median home values Census tract three counties can explored way. example, call group_by() followed summarize() facilitates calculation county minimums, means, medians, maximums.\nTable 4.9: Summary statistics Census tracts Portland-area counties\nbasic summary statistics offer insights comparisons three counties, limited ability help us understand dynamics overall distribution values. task can turn augmented visualization, allows quick visual comparison distributions. Group-wise visualization ggplot2 can accomplished facet_wrap() function added onto existing ggplot2 code salient groups visualize. example, kernel density plot can show overall shape distribution median home values dataset:\nFigure 4.16: density plot using values dataset\nMapping county column onto fill aesthetic draw superimposed density plots county chart:\nFigure 4.17: density plot separate curves county\nAlternatively, adding facet_wrap() function, specifying county column used group data, splits visualization side--side graphics based counties Census tract belongs.\nFigure 4.18: example faceted density plot\nside--side comparative graphics show value distributions vary three counties. Home values three counties common around $250,000, Multnomah County Census tracts represent highest values dataset.","code":"\nhousing_val <- get_acs(\n  geography = \"tract\", \n  variables = \"B25077_001\", \n  state = \"OR\", \n  county = c(\n    \"Multnomah\", \n    \"Clackamas\", \n    \"Washington\",\n    \"Yamhill\", \n    \"Marion\", \n    \"Columbia\"\n  ),\n  year = 2020\n)\nhousing_val2 <- separate(\n  housing_val, \n  NAME, \n  into = c(\"tract\", \"county\", \"state\"), \n  sep = \", \"\n)\nhousing_val2 %>%\n  group_by(county) %>%\n  summarize(min = min(estimate, na.rm = TRUE), \n            mean = mean(estimate, na.rm = TRUE), \n            median = median(estimate, na.rm = TRUE), \n            max = max(estimate, na.rm = TRUE))\nggplot(housing_val2, aes(x = estimate)) + \n  geom_density()\nggplot(housing_val2, aes(x = estimate, fill = county)) + \n  geom_density(alpha = 0.3)\nggplot(housing_val2, aes(x = estimate)) +\n  geom_density(fill = \"darkgreen\", color = \"darkgreen\", alpha = 0.5) + \n  facet_wrap(~county) + \n  scale_x_continuous(labels = dollar_format(scale = 0.000001, \n                                            suffix = \"m\")) + \n  theme_minimal(base_size = 14) + \n  theme(axis.text.y = element_blank(), \n        axis.text.x = element_text(angle = 45)) + \n  labs(x = \"ACS estimate\",\n       y = \"\",\n       title = \"Median home values by Census tract, 2015-2019 ACS\")"},{"path":"exploring-us-census-data-with-visualization.html","id":"advanced-visualization-with-ggplot2-extensions","chapter":"4 Exploring US Census data with visualization","heading":"4.7 Advanced visualization with ggplot2 extensions","text":"core functionality ggplot2 powerful, notable advantage using ggplot2 visualization contributions made user community form extensions. ggplot2 extensions packages developed practitioners outside core ggplot2 development team add functionality package. encourage review gallery ggplot2 extensions extensions website; highlight notable examples .","code":""},{"path":"exploring-us-census-data-with-visualization.html","id":"ggridges","chapter":"4 Exploring US Census data with visualization","heading":"4.7.1 ggridges","text":"ggridges package (Wilke 2021) adapts concept faceted density plot generate ridgeline plots, densities overlap one another. example creates ridgeline plot using Portland-area home value data; geom_density_ridges() generates ridgelines, theme_ridges() styles plot appropriate manner.\nFigure 4.19: Median home values Portland-area counties visualized ggridges\noverlapping density “ridges” offer pleasing aesthetic also practical way compare different data distributions. ggridges extends ggplot2, analysts can style different chart components liking using methods introduced earlier chapter.","code":"\nlibrary(ggridges)\n\nggplot(housing_val2, aes(x = estimate, y = county)) + \n  geom_density_ridges() + \n  theme_ridges() + \n  labs(x = \"Median home value: 2016-2020 ACS estimate\", \n       y = \"\") + \n  scale_x_continuous(labels = label_dollar(scale = .000001, suffix = \"m\"),\n                     breaks = c(0, 500000, 1000000)) + \n  theme(axis.text.x = element_text(angle = 45))"},{"path":"exploring-us-census-data-with-visualization.html","id":"ggbeeswarm","chapter":"4 Exploring US Census data with visualization","heading":"4.7.2 ggbeeswarm","text":"ggbeeswarm package (Clarke Sherrill-Mix 2017) extends ggplot2 allowing users generate beeswarm plots, clouds points jittered show overall density distribution data values. Beeswarm plots can compelling ways visualize multiple data variables chart, distributions median household income racial ethnic composition neighborhoods. motivating example chart , looks household income race/ethnicity New York City. data wrangling first part code chunk takes advantage skills covered Chapters 2 3, allowing visualization household income distributions largest group Census tract.\nFigure 4.20: beeswarm plot median household income common racial ethnic group, NYC Census tracts\nplot shows wealthiest neighborhoods New York City - median household incomes exceeding $150,000 - plurality majority non-Hispanic white. However, chart also illustrates range values among neighborhoods pluralities different racial ethnic groups, suggesting nuanced portrait intersections race income inequality city.","code":"\nlibrary(ggbeeswarm)\n\nny_race_income <- get_acs(\n  geography = \"tract\", \n  state = \"NY\",  \n  county = c(\"New York\", \"Bronx\", \"Queens\", \"Richmond\", \"Kings\"),\n  variables = c(White = \"B03002_003\", \n                Black = \"B03002_004\", \n                Asian = \"B03002_006\",\n                Hispanic = \"B03002_012\"), \n  summary_var = \"B19013_001\",\n  year = 2020\n) %>%\n  group_by(GEOID) %>%\n  filter(estimate == max(estimate, na.rm = TRUE)) %>%\n  ungroup() %>%\n  filter(estimate != 0)\n\nggplot(ny_race_income, aes(x = variable, y = summary_est, color = summary_est)) +\n  geom_quasirandom(alpha = 0.5) + \n  coord_flip() + \n  theme_minimal(base_size = 13) + \n  scale_color_viridis_c(guide = \"none\") + \n  scale_y_continuous(labels = label_dollar()) + \n  labs(x = \"Largest group in Census tract\", \n       y = \"Median household income\", \n       title = \"Household income distribution by largest racial/ethnic group\", \n       subtitle = \"Census tracts, New York City\", \n       caption = \"Data source: 2016-2020 ACS\")"},{"path":"exploring-us-census-data-with-visualization.html","id":"geofaceted-plots","chapter":"4 Exploring US Census data with visualization","heading":"4.7.3 Geofaceted plots","text":"next four chapters book, Chapters 5 8, spatial data, mapping, spatial analysis. Geographic location can incorporated Census data visualizations without using geographic information explicitly way geofacet package (Hafen 2020). Geofaceted plots enhanced versions faceted visualizations arrange subplots relationship relative geographic location. geofacet package 100 available grids choose allowing faceted plots US states, counties, regions around world. key use column can map correctly information geofaceted grid using.example , replicate population pyramid code generate population pyramids state US. However, also modify output data population information reflects proportion overall population state states consistent scales. population pyramid code similar, though axis information unnecessary stripped final plot.\nFigure 4.21: Geofaceted population pyramids US states\nresult compelling visualization expresses general geographic relationships patterns data showing comparative population pyramids states. unique nature Washington, DC stands large population 20s 30s; can also compare Utah, youngest state country, older states Northeast.specific grid arrangement liking (e.g. Minnesotans may take issue Wisconsin north!), try built-grid options package. Also, can use grid_design() function geofacet package pull interactive app can design grid use geofaceted plots!","code":"\nlibrary(geofacet)\n\nus_pyramid_data <- get_estimates(\n  geography = \"state\",\n  product = \"characteristics\",\n  breakdown = c(\"SEX\", \"AGEGROUP\"),\n  breakdown_labels = TRUE,\n  year = 2019\n) %>%\n  filter(str_detect(AGEGROUP, \"^Age\"),\n         SEX != \"Both sexes\") %>%\n  group_by(NAME) %>%\n  mutate(prop = value / sum(value, na.rm = TRUE)) %>%\n  ungroup() %>%\n  mutate(prop = ifelse(SEX == \"Male\", -prop, prop))\n\nggplot(us_pyramid_data, aes(x = prop, y = AGEGROUP, fill = SEX)) + \n  geom_col(width = 1) + \n  theme_minimal() + \n  scale_fill_manual(values = c(\"darkred\", \"navy\")) + \n  facet_geo(~NAME, grid = \"us_state_with_DC_PR_grid2\",\n            label = \"code\") + \n  theme(axis.text = element_blank(),\n        strip.text.x = element_text(size = 8)) + \n  labs(x = \"\", \n       y = \"\", \n       title = \"Population structure by age and sex\", \n       fill = \"\", \n       caption = \"Data source: US Census Bureau population estimates & tidycensus R package\")"},{"path":"exploring-us-census-data-with-visualization.html","id":"interactive-visualization-with-plotly","chapter":"4 Exploring US Census data with visualization","heading":"4.7.4 Interactive visualization with plotly","text":"htmlwidgets package (Vaidyanathan et al. 2020) provides bridge JavaScript libraries interactive web-based data visualization R language. Since 2015, R community released hundreds packages depend htmlwidgets bring interactive data visualization R. One popular packages interactive visualization plotly package (Sievert 2020), interface Plotly visualization library.plotly well-developed library allows many different types custom visualizations. However, one useful functions plotly package arguably simplest. ggplotly() function can convert existing ggplot2 graphic interactive web visualization single line code! Let’s try utah_pyramid population pyramid:\nFigure 4.22: interactive population pyramid rendered ggplotly\nTry hovering cursor different bars; reveals tooltip shares information data. legend interactive, data series can clicked ; viewers can also pan zoom plot using toolbar appears upper right corner visualization. Interactive graphics like excellent way facilitate additional data exploration, can polished presentation web.","code":"\nlibrary(plotly)\n\nggplotly(utah_pyramid)"},{"path":"exploring-us-census-data-with-visualization.html","id":"learning-more-about-visualization","chapter":"4 Exploring US Census data with visualization","heading":"4.8 Learning more about visualization","text":"chapter introduced series visualization techniques implemented ggplot2 appropriate US Census Bureau data. Readers may want learn effective principles visualization design communication apply techniques covered . Chapter 6 covers principles brief, literature focuses specifically topics comprehensive. Munzner (2014) -depth overview visualization techniques design principles, offers corresponding website lecture slides. Evergreen (2020) Knaflic (2015) provide guidelines effective communication visualization offer excellent design tips business general audiences. R users may interested Wilke (2019) Healy (2019), offer comprehensive overview data visualization best practices along corresponding R code reproduce figures books.","code":""},{"path":"exploring-us-census-data-with-visualization.html","id":"exercises-2","chapter":"4 Exploring US Census data with visualization","heading":"4.9 Exercises","text":"Choose different variable ACS /different location create margin error visualization .Choose different variable ACS /different location create margin error visualization .Modify population pyramid code create different, customized population pyramid. can choose different location (state county), different colors/plot design, combination!Modify population pyramid code create different, customized population pyramid. can choose different location (state county), different colors/plot design, combination!","code":""},{"path":"census-geographic-data-and-applications-in-r.html","id":"census-geographic-data-and-applications-in-r","chapter":"5 Census geographic data and applications in R","heading":"5 Census geographic data and applications in R","text":"discussed previous chapters book, Census ACS data associated geographies, units data aggregated. defined geographies represented US Census Bureau’s TIGER/Line database, acronym TIGER stands Topologically Integrated Geographic Encoding Referencing. database includes high-quality series geographic datasets suitable spatial analysis cartographic visualization . Spatial datasets made available shapefiles, common format encoding geographic data.TIGER/Line shapefiles include three general types data:Legal entities, geographies official legal standing United States. include states counties.Statistical entities, geographies defined Census Bureau purposes data collection dissemination. Examples statistical entities include Census tracts block groups.Geographic features, geographic datasets linked aggregate demographic data Census Bureau. datasets include roads water features.Traditionally, TIGER/Line shapefiles downloaded web interface zipped folders, unzipped use Geographic Information System (GIS) software can work geographic data. However, R package tigris (K. Walker 2016a) allows R users access datasets directly R sessions without go steps.chapter cover core functionality tigris package working Census geographic data R. , highlight sf package (Pebesma 2018) representing spatial data R objects.","code":""},{"path":"census-geographic-data-and-applications-in-r.html","id":"basic-usage-of-tigris","chapter":"5 Census geographic data and applications in R","heading":"5.1 Basic usage of tigris","text":"tigris R package simplifies process R users obtaining using Census geographic datasets. Functions tigris download requested Census geographic dataset US Census Bureau website, load dataset R spatial object. Generally speaking, type geographic dataset available Census Bureau’s TIGER/Line database available corresponding function tigris. example, states() function can run without arguments download boundary file US states state equivalents.get message letting us know data year 2020 returned; tigris typically defaults recent year complete set Census shapefiles available, time writing 2020.Let’s take look got back:returned object class \"sf\" \"data.frame\". can print first 10 rows inspect object :object st, representing US states territories, includes data frame series columns representing characteristics states, like name, postal code, Census ID (GEOID column). also contains special list-column, geometry, made sequence coordinate longitude/latitude coordinate pairs collectively represent boundary state.geometry column can visualized R plot() function:\nFigure 5.1: Default US states data obtained tigris\nCensus datasets may available state county within state. cases, subsetting optional; cases, state /county arguments required. example, counties() function can used obtain county boundaries entirety United States, also can used state argument return counties specific state, like New Mexico.\nFigure 5.2: County boundaries New Mexico\ncase state postal code \"NM\" used instruct tigris subset counties dataset counties New Mexico. full name state, \"New Mexico\", work well. Obtaining Census shapefiles programmatically requires inputting Federal Information Processing Standard (FIPS) code; however, tigris translates postal codes names states counties FIPS codes R users look .States counties examples legal entities can accessed tigris. Statistical entities geographic features similarly accessible exist TIGER/Line database. example, user might request Census tract boundaries given county New Mexico corresponding tracts() function.\nFigure 5.3: Census tract boundaries Los Alamos County, NM\nSeveral geographic features available tigris well, including roads water features can useful thematic mapping. example, user request area water data Los Alamos County area_water() function.\nFigure 5.4: Water area Los Alamos County, NM\n","code":"\nlibrary(tigris)\n\nst <- states()## Retrieving data for the year 2020\nclass(st)## [1] \"sf\"         \"data.frame\"\nst## Simple feature collection with 56 features and 14 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -179.2311 ymin: -14.60181 xmax: 179.8597 ymax: 71.43979\n## Geodetic CRS:  NAD83\n## First 10 features:\n##    REGION DIVISION STATEFP  STATENS GEOID STUSPS           NAME LSAD MTFCC\n## 1       3        5      54 01779805    54     WV  West Virginia   00 G4000\n## 2       3        5      12 00294478    12     FL        Florida   00 G4000\n## 3       2        3      17 01779784    17     IL       Illinois   00 G4000\n## 4       2        4      27 00662849    27     MN      Minnesota   00 G4000\n## 5       3        5      24 01714934    24     MD       Maryland   00 G4000\n## 6       1        1      44 01219835    44     RI   Rhode Island   00 G4000\n## 7       4        8      16 01779783    16     ID          Idaho   00 G4000\n## 8       1        1      33 01779794    33     NH  New Hampshire   00 G4000\n## 9       3        5      37 01027616    37     NC North Carolina   00 G4000\n## 10      1        1      50 01779802    50     VT        Vermont   00 G4000\n##    FUNCSTAT        ALAND      AWATER    INTPTLAT     INTPTLON\n## 1         A  62266296765   489206049 +38.6472854 -080.6183274\n## 2         A 138958484319 45975808217 +28.3989775 -082.5143005\n## 3         A 143778461053  6216594318 +40.1028754 -089.1526108\n## 4         A 206232157570 18949864226 +46.3159573 -094.1996043\n## 5         A  25151895765  6979171386 +38.9466584 -076.6744939\n## 6         A   2677759219  1323691129 +41.5964850 -071.5264901\n## 7         A 214049923496  2391577745 +44.3484222 -114.5588538\n## 8         A  23190113978  1025973001 +43.6726907 -071.5843145\n## 9         A 125933025759 13456395178 +35.5397100 -079.1308636\n## 10        A  23873081385  1030243281 +44.0589536 -072.6710173\n##                          geometry\n## 1  MULTIPOLYGON (((-81.74725 3...\n## 2  MULTIPOLYGON (((-86.39964 3...\n## 3  MULTIPOLYGON (((-91.18529 4...\n## 4  MULTIPOLYGON (((-96.78438 4...\n## 5  MULTIPOLYGON (((-77.45881 3...\n## 6  MULTIPOLYGON (((-71.7897 41...\n## 7  MULTIPOLYGON (((-116.8997 4...\n## 8  MULTIPOLYGON (((-72.3299 43...\n## 9  MULTIPOLYGON (((-82.41674 3...\n## 10 MULTIPOLYGON (((-73.31328 4...\nplot(st$geometry)\nnm_counties <- counties(\"NM\")\n\nplot(nm_counties$geometry)\nla_tracts <- tracts(\"NM\", \"Los Alamos\")\n\nplot(la_tracts$geometry)\nla_water <- area_water(\"NM\", \"Los Alamos\")\n\nplot(la_water$geometry)"},{"path":"census-geographic-data-and-applications-in-r.html","id":"understanding-tigris-and-simple-features","chapter":"5 Census geographic data and applications in R","heading":"5.1.1 Understanding tigris and simple features","text":"Data returned tigris package examples vector spatial data, spatial data model represents geographic features points, lines, polygons. vector spatial data model represented R sf package, implementation simple features R language. sf package R interface C libraries power much broader geographic data ecosystem: GDAL reading & writing spatial data, GEOS modeling spatial relationships, PROJ representing coordinate reference systems. topics outlined detail chapter remainder section.mentioned earlier, sf represents vector spatial data much like regular R data frame, special column, geometry, represents shape feature. simple features object printed, information data frame gives additional geographic context coordinates geometry column. includes geometry type, bounding box, coordinate reference system (CRS) definition. spatial concepts help define R represents data geographically, explored later chapter.Vector data typically represented either points, lines, polygons, tigris gives access three types.","code":""},{"path":"census-geographic-data-and-applications-in-r.html","id":"points","chapter":"5 Census geographic data and applications in R","heading":"5.1.1.1 Points","text":"example point dataset available tigris package Census landmarks, point--interest dataset comprehensive used Census Bureau guide field enumerators. Let’s acquire landmark point data District Columbia take look.\nFigure 5.5: Census landmarks Washington, DC\nPoints vector data represented single coordinate pair; location, length area turn zero-dimensional. Points useful representing geographic phenomena physical properties features importance visualization analysis. example, interested geographic distribution Census landmarks Washington DC, actual shape physical area specific landmarks, representing landmarks points makes sense. sf represents points geometry type POINT.","code":"\ndc_landmarks <- landmarks(\"DC\", type = \"point\")\n\nplot(dc_landmarks$geometry)"},{"path":"census-geographic-data-and-applications-in-r.html","id":"lines","chapter":"5 Census geographic data and applications in R","heading":"5.1.1.2 Lines","text":"\nFigure 5.6: Primary secondary roads Washington, DC\nLines one-dimensional representations geographic features used length, area, features primary importance. respect TIGER/Line shapefiles, transportation network features roads railroads represented lines. Line features least two linked coordinate pairs, complex linear representations many . Lines represented geometry type LINESTRING.","code":"\ndc_roads <- primary_secondary_roads(\"DC\")\n\nplot(dc_roads$geometry)"},{"path":"census-geographic-data-and-applications-in-r.html","id":"polygons","chapter":"5 Census geographic data and applications in R","heading":"5.1.1.3 Polygons","text":"\nFigure 5.7: Block groups Washington, DC\nPolygons enclosed shapes least three connected coordinate pairs. respect Census geometries, enumeration units like block groups represented polygons TIGER/Line files. Polygon geometry useful analyst needs represent shape area geographic features project.three core geometries (point, line, polygon) can encoded complex ways simple features representation used sf. example, geometry type POLYGON use one row simple features data frame discrete shape; geometry type MULTIPOLYGON, contrast, can link multiple discrete shapes part geographic feature. important encoding features may detached parts, series islands belong county. vein, points can represented MULTIPOINT lines can represented MULTILINESTRING, respectively, accommodate similar scenarios.","code":"\ndc_block_groups <- block_groups(\"DC\")\n\nplot(dc_block_groups$geometry)"},{"path":"census-geographic-data-and-applications-in-r.html","id":"data-availability-in-tigris","chapter":"5 Census geographic data and applications in R","heading":"5.1.2 Data availability in tigris","text":"examples provided sampling datasets available tigris; full enumeration available datasets functions access found guide .Note guide many datasets available TIGER/Line cartographic boundary versions, can downloaded multiple years; distinctions covered Section 5.3 .","code":""},{"path":"census-geographic-data-and-applications-in-r.html","id":"plotting-geographic-data","chapter":"5 Census geographic data and applications in R","heading":"5.2 Plotting geographic data","text":"Geographic information science inherently visual discipline. analysts coming R desktop GIS background (e.g. ArcGIS, QGIS), used visual display geographic data central interactions . may make transition R unfamiliar geospatial analysts geographic data first foremost represented tabular data frame.previous section, used plot() function visualize geometry column simple features object obtained tigris. R includes variety options quick visualization geographic data useful geospatial anlaysts.","code":""},{"path":"census-geographic-data-and-applications-in-r.html","id":"ggplot2-and-geom_sf","chapter":"5 Census geographic data and applications in R","heading":"5.2.1 ggplot2 and geom_sf()","text":"ggplot2 version 3.0, package released support plotting simple features objects directly function geom_sf(). Prior release functionality, plotting geographic data (consequence making maps) reasonably cumbersome; geom_sf() streamlines geographic visualization process makes ggplot2 go-package visualization simple features objects.basic level, couple lines ggplot2 code needed plot Census shapes obtained tigris. example, taking look Los Alamos County Census tracts:default, ggplot2 includes standard grey grid latitude longitude values displayed along axes. many cartographic applications, analyst want remove background information. theme_void() function strips background grid axis labels plot accordingly:\nFigure 5.8: ggplot2 map blank background\nSection 4.6 introduced concept faceted plots compare different views, also useful concept geographic visualization. Faceted mapping addressed directly next chapter. comparative spatial plots, patchwork R package (Pedersen 2020) works well arranging multi-plot layout. , ’ll use patchwork put two ggplot2 spatial plots - one Census tracts one block groups Los Alamos County - side--side using + operator.\nFigure 5.9: Comparing Census tracts block groups\nAlternatively, patchwork allows R users arrange plots vertically using / operator.","code":"\nlibrary(ggplot2)\n\nggplot(la_tracts) + \n  geom_sf()\nggplot(la_tracts) + \n  geom_sf() + \n  theme_void()\nlibrary(patchwork)\n\nla_block_groups <- block_groups(\"NM\", \"Los Alamos\")\n\ngg1 <- ggplot(la_tracts) + \n  geom_sf() + \n  theme_void() + \n  labs(title = \"Census tracts\")\n\ngg2 <- ggplot(la_block_groups) + \n  geom_sf() + \n  theme_void() + \n  labs(title = \"Block groups\")\n\ngg1 + gg2"},{"path":"census-geographic-data-and-applications-in-r.html","id":"interactive-viewing-with-mapview","chapter":"5 Census geographic data and applications in R","heading":"5.2.2 Interactive viewing with mapview","text":"major hesitation geospatial analysts considering switch desktop GIS software R strengths desktop GIS interactive visual exploration. ArcGIS QGIS allow analysts quickly load geographic dataset interactively pan zoom explore geographic trends. Prior development htmlwidgets framework (discussed previous chapter), R equivalent capabilities. Frankly, major reason hesitated fully transition work desktop GIS software R spatial analysis.mapview R package (Appelhans et al. 2020) fills crucial gap. single call function mapview(), mapview visualizes geographic data interactive, zoomable map. Let’s try Census tracts Los Alamos County.\nFigure 5.10: Interactive view Los Alamos, NM\nClicking Census tract shape reveals pop-attribute information found dataset. Additionally, users can change underlying basemap understand geographic context surrounding data. mapview also includes significant functionality interactive mapping data display beyond basic example; features covered next chapter.","code":"\nlibrary(mapview)\n\nmapview(la_tracts)"},{"path":"census-geographic-data-and-applications-in-r.html","id":"tigris-workflows","chapter":"5 Census geographic data and applications in R","heading":"5.3 tigris workflows","text":"covered previous sections, tigris useful package getting TIGER/Line shapefiles R geospatial projects without navigate Census website. Functions tigris include additional options allow customization output better integration geospatial projects. sections provide overview options.","code":""},{"path":"census-geographic-data-and-applications-in-r.html","id":"tigerline-and-cartographic-boundary-shapefiles","chapter":"5 Census geographic data and applications in R","heading":"5.3.1 TIGER/Line and cartographic boundary shapefiles","text":"addition core TIGER/Line shapefiles, tigris package named, Census Bureau also makes available cartographic boundary shapefiles. files derived TIGER/Line shapefiles generalized interior clipped shoreline United States, making better choice many cases TIGER/Line shapefiles thematic mapping. polygon datasets tigris available cartographic boundary files, accessible argument cb = TRUE.years developing tigris, gotten multiple inquires akin “area look strange download ?” Michigan, extensive shoreline along Great Lakes, commonly comes . TIGER/Line shapefiles include water area geographic features, connecting Upper Peninsula Michigan southern part state giving unfamiliar representation Michigan’s land area. Using cartographic boundary alternative resolves . Let’s use patchwork compare TIGER/Line cartographic boundary shapefiles counties Michigan illustration.\nFigure 5.11: Comparison TIGER/Line cartographic boundary files Michigan counties\nTIGER/Line shapefiles may represent “official” areas counties - include water area - look unfamiliar viewers expecting usual representation land area Michigan. cartographic boundary file shows islands distinct coastline, better option thematic mapping projects. using cb = TRUE argument counties larger geographies, users can also specify one three resolutions resolution argument: \"500k\" (default), \"5m\", \"20m\", higher values representing generalized boundaries smaller file sizes.","code":"\nmi_counties <- counties(\"MI\")\nmi_counties_cb <- counties(\"MI\", cb = TRUE)\n\nmi_tiger_gg <- ggplot(mi_counties) + \n  geom_sf() + \n  theme_void() + \n  labs(title = \"TIGER/Line\")\n\nmi_cb_gg <- ggplot(mi_counties_cb) + \n  geom_sf() + \n  theme_void() + \n  labs(title = \"Cartographic boundary\")\n\nmi_tiger_gg + mi_cb_gg"},{"path":"census-geographic-data-and-applications-in-r.html","id":"caching-tigris-data","chapter":"5 Census geographic data and applications in R","heading":"5.3.2 Caching tigris data","text":"common issue can arise working tigris waiting around large file downloads, especially required geographic data large internet connection spotty. example, Census block shapefile Texas 2019 441MB size zipped, can take obtain without high-speed internet.tigris offers solution way shapefile caching. specifying options(tigris_use_cache = TRUE), users can instruct tigris download shapefiles cache directory computers future use rather temporary directory tigris default. shapefile caching turned , tigris look first cache directory see requested shapefile already . , read without re-downloading. , download file Census website store cache directory.cache directory can checked user_cache_dir() function available rappdirs package (Ratnakumar, Mick, Davis 2021). specific location cache directory depend operating system.desired, users can modify tigris cache directory function tigris_cache_dir().","code":"\noptions(tigris_use_cache = TRUE)\n\nrappdirs::user_cache_dir(\"tigris\")## [1] \"~/.cache/tigris\""},{"path":"census-geographic-data-and-applications-in-r.html","id":"understanding-yearly-differences-in-tigerline-files","chapter":"5 Census geographic data and applications in R","heading":"5.3.3 Understanding yearly differences in TIGER/Line files","text":"US Census Bureau offers time series TIGER/Line cartographic boundary shapefiles 1990 2020. (even older geographies, see NHGIS, topic covered Chapter 11). geographies reasonably static, state boundaries, others change regularly decennial US Census, Census tracts, block groups, blocks. example changes shown Census tracts Tarrant County, Texas displayed, county added nearly 1 million people 1990 2020. Given US Census Bureau aims make population sizes Census tracts relatively consistent (around 4,000 people), subdivide re-draw tracts fast-growing areas Census provide better geographic granularity.can use tidyverse tools covered earlier book generate list tract plots year. purrr::map() iterates year, grabbing cartographic boundary file Census tracts four decennial Census years plotting ggplot2. glue() function glue package used create custom title shows number Census tracts year.plots generated, can use patchwork facet plots earlier chapter. division operator / places plots top one another allowing organization plots grid.\nFigure 5.12: Tarrant County, TX Census tracts, 1990-2020\nTarrant County added 180 new Census tracts 1990 2020. plot shows, many tracts found fast-growing parts county northeast southeast. Notably, changes Census tract geography downstream implications well time-series analysis, covered Section 3.4.1. Data Census tract level 2010, example, tabulated differently 2020 tract geographies different. One common method adjusting demographic data disparate zonal configurations areal interpolation, topic covered Section 7.4.4.default year shapefiles tigris typically updated cartographic boundary shapefiles year become fully available. time writing, default year 2020. changed 2021 pending full data availability cartographic boundary shapefiles year later 2022. users need 2021 boundaries projects want type year = 2021 dataset, command options(tigris_year = 2021) can used. direct tigris download 2021 shapefiles available without specify year explicitly.","code":"\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(glue)\n\nyearly_plots <- map(seq(1990, 2020, 10), ~{\n  year_tracts <- tracts(\"TX\", \"Tarrant\", year = .x,\n                        cb = TRUE)\n\n  ggplot(year_tracts) + \n    geom_sf() + \n    theme_void() + \n    labs(title = glue(\"{.x}: {nrow(year_tracts)} tracts\"))\n})\n(yearly_plots[[1]] + yearly_plots[[2]]) / \n  (yearly_plots[[3]] + yearly_plots[[4]])"},{"path":"census-geographic-data-and-applications-in-r.html","id":"combining-tigris-datasets","chapter":"5 Census geographic data and applications in R","heading":"5.3.4 Combining tigris datasets","text":"years 2019 later, US Census Bureau started releasing national small-area cartographic boundary files, including commonly-requested geographies like block groups, Census tracts, places. tigris, user needs specify cb = TRUE leave state blank get national dataset. One line code tigris need get 242,303 US block groups 2020:However, option years 2018 earlier, means tigris users must turn alternative methods generate national datasets. datasets straightforward create tidyverse tools. covered several examples thus far book, purrr::map() family functions iterate sequence values combine function results directed user. tigris users, map_dfr() function prove especially useful row-binds datasets create output. built-state.abb vector R gives us postal codes 50 US states; Washington, DC Puerto Rico required analysis, add vector c() shown .vector state codes prepared, user can iterate codes map_dfr() produce national block group dataset, shown 2018 .using shapefile caching, process slowed time takes download block group shapefile Census Bureau website. However, local cache block group shapefiles illustrated section, loading combining 220,016 block groups use analysis take seconds.","code":"\nus_bgs_2020 <- block_groups(cb = TRUE, year = 2020)\n\nnrow(us_bgs_2020)## [1] 242303\nstate_codes <- c(state.abb, \"DC\", \"PR\")\n\nus_bgs_2018 <- map_dfr(\n  state_codes, \n  ~block_groups(\n    state = .x, \n    cb = TRUE, \n    year = 2018\n  )\n)\n\nnrow(us_bgs_2018)## [1] 220016"},{"path":"census-geographic-data-and-applications-in-r.html","id":"coordinate-reference-systems","chapter":"5 Census geographic data and applications in R","heading":"5.4 Coordinate reference systems","text":"geographic data appropriately represent locations mapping spatial analysis, must referenced model Earth’s surface correctly. simpler terms - data model state Florida represent Florida actually located relative locations! defined coordinate reference system (CRS), specifies data coordinates mapped model Earth’s surface also measurements computed using given dataset. complete discussion coordinate reference systems found (Lovelace, Nowosad, Muenchow 2019); overview work coordinate systems relationship tigris covered .default, datasets returned tigris stored geographic coordinate system, coordinates represented longitude latitude relative three-dimensional model earth. st_crs() function sf package helps us check CRS data; let’s counties Florida.function returns well-known text representation information coordinate reference system. Census Bureau datasets stored “NAD83” geographic coordinate system, refers North American Datum 1983. relevant information includes ellipsoid used (GRS 1980, generalized three-dimensional model Earth’s shape), prime meridian CRS (Greenwich used ), EPSG (European Petroleum Survey Group) ID 4269, special code can used represent CRS concise terms.sf version 1.0, package uses spherical geometry library s2 appropriately perform calculations spatial data stored geographic coordinate systems. working visualizing geographic data smaller areas, however, projected coordinate reference system represents data two-dimensions planar surface may preferable. Thousands projected CRSs exist - appropriate minimizing data distortion specific part world. can challenge decide right projected CRS data, crsuggest package (K. Walker 2021a) can help narrow choices.","code":"\nlibrary(sf)\n\nfl_counties <- counties(\"FL\", cb = TRUE)\n\nst_crs(fl_counties)## Coordinate Reference System:\n##   User input: NAD83 \n##   wkt:\n## GEOGCRS[\"NAD83\",\n##     DATUM[\"North American Datum 1983\",\n##         ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n##             LENGTHUNIT[\"metre\",1]]],\n##     PRIMEM[\"Greenwich\",0,\n##         ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     CS[ellipsoidal,2],\n##         AXIS[\"latitude\",north,\n##             ORDER[1],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         AXIS[\"longitude\",east,\n##             ORDER[2],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     ID[\"EPSG\",4269]]"},{"path":"census-geographic-data-and-applications-in-r.html","id":"using-the-crsuggest-package","chapter":"5 Census geographic data and applications in R","heading":"5.4.1 Using the crsuggest package","text":"core function implemented crsuggest suggest_crs(), returns tibble possible choices suitable projected CRS data. function works analyzing geometry input dataset comparing built-dataset CRS extents choosing CRSs minimize Hausdorff distance dataset extents.Let’s try Florida counties dataset.\nTable 5.1: Suggested coordinate reference systems Florida\n“best choice” CRS “Florida GDL Albers” coordinate reference system, available four different variations NAD1983 datum. “Florida GDL” refers Florida Geographic Data Library distributes data state-wide equal-area coordinate reference system. large states large irregular extents like Florida (Texas one example) maintain statewide coordinate reference systems like suitable statewide mapping analysis. Let’s choose third entry, “NAD83 (HARN) / Florida GDL Albers”, recommended Florida GDL website. Coordinate reference system transformations sf implemented st_transform() function, used .Note coordinates bounding box feature geometry changed much larger numbers; expressed meters rather decimal degrees used NAD83 geographic coordinate system. Let’s take closer look selected CRS:lot information CRS’s well-known text NAD83 geographic CRS. Information base geographic CRS provided along parameters “false origin.” Whereas coordinates geographic coordinate systems generally represented longitude/latitude relative Prime Meridian Equator, projected coordinate reference systems relative “false origin” specified relative area CRS used (noting USAGE section bottom). “false origin” located -84 degrees longitude, 24 degrees latitude (SW Florida Keys north western Cuba) false X value 400,000 false Y value 0. turn, X Y values projected data expressed meters relative origin, set X Y values Florida positive numbers. makes planar geometric calculations like distance, perimeter, area straightforward.","code":"\nlibrary(crsuggest)\n\nfl_crs <- suggest_crs(fl_counties)\nfl_projected <- st_transform(fl_counties, crs = 3087)\n\nhead(fl_projected)## Simple feature collection with 6 features and 12 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 281876.9 ymin: 397330.2 xmax: 669346.5 ymax: 715363.4\n## Projected CRS: NAD83(HARN) / Florida GDL Albers\n##    STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID         NAME\n## 11      12      121 00295729 0500000US12121 12121     Suwannee\n## 15      12      007 00303634 0500000US12007 12007     Bradford\n## 91      12      037 00306911 0500000US12037 12037     Franklin\n## 92      12      123 00295728 0500000US12123 12123       Taylor\n## 93      12      057 00295757 0500000US12057 12057 Hillsborough\n## 94      12      109 00308371 0500000US12109 12109    St. Johns\n##               NAMELSAD STUSPS STATE_NAME LSAD      ALAND     AWATER\n## 11     Suwannee County     FL    Florida   06 1783430092    9505389\n## 15     Bradford County     FL    Florida   06  761362121   16905200\n## 91     Franklin County     FL    Florida   06 1411498965 2270440522\n## 92       Taylor County     FL    Florida   06 2702224058 1213576997\n## 93 Hillsborough County     FL    Florida   06 2646680847  803027226\n## 94    St. Johns County     FL    Florida   06 1555661651  572103225\n##                          geometry\n## 11 MULTIPOLYGON (((471781.3 69...\n## 15 MULTIPOLYGON (((552464 6583...\n## 91 MULTIPOLYGON (((334863.7 64...\n## 92 MULTIPOLYGON (((416379.5 66...\n## 93 MULTIPOLYGON (((555139.5 42...\n## 94 MULTIPOLYGON (((622427 6727...\nst_crs(fl_projected)## Coordinate Reference System:\n##   User input: EPSG:3087 \n##   wkt:\n## PROJCRS[\"NAD83(HARN) / Florida GDL Albers\",\n##     BASEGEOGCRS[\"NAD83(HARN)\",\n##         DATUM[\"NAD83 (High Accuracy Reference Network)\",\n##             ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n##                 LENGTHUNIT[\"metre\",1]]],\n##         PRIMEM[\"Greenwich\",0,\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         ID[\"EPSG\",4152]],\n##     CONVERSION[\"Florida GDL Albers (meters)\",\n##         METHOD[\"Albers Equal Area\",\n##             ID[\"EPSG\",9822]],\n##         PARAMETER[\"Latitude of false origin\",24,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8821]],\n##         PARAMETER[\"Longitude of false origin\",-84,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8822]],\n##         PARAMETER[\"Latitude of 1st standard parallel\",24,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8823]],\n##         PARAMETER[\"Latitude of 2nd standard parallel\",31.5,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8824]],\n##         PARAMETER[\"Easting at false origin\",400000,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8826]],\n##         PARAMETER[\"Northing at false origin\",0,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8827]]],\n##     CS[Cartesian,2],\n##         AXIS[\"easting (X)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"metre\",1]],\n##         AXIS[\"northing (Y)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"metre\",1]],\n##     USAGE[\n##         SCOPE[\"unknown\"],\n##         AREA[\"USA - Florida\"],\n##         BBOX[24.41,-87.63,31.01,-79.97]],\n##     ID[\"EPSG\",3087]]"},{"path":"census-geographic-data-and-applications-in-r.html","id":"plotting-with-coord_sf","chapter":"5 Census geographic data and applications in R","heading":"5.4.2 Plotting with coord_sf()","text":"visualizing simple feature geometries ggplot2’s geom_sf(), coord_sf() method allows specify coordinate reference system transformation used visualization. coord_sf() inherit CRS spatial object plotted geom_sf() default, can also modify displayed CRS spatial object without performing CRS transformation st_transform(). example:\nFigure 5.13: ggplot2 map CRS specified\ndata displayed plot requested coordinate reference system, underlying graticule (grid lines axis tick labels) default longitude/latitude. show coordinates projected coordinate reference system, argument datum can used controls gridlines.\nFigure 5.14: ggplot2 plot modified graticule\n","code":"\noptions(scipen = 999)\n\nggplot(fl_counties) + \n  geom_sf() + \n  coord_sf(crs = 3087)\nggplot(fl_counties) + \n  geom_sf() + \n  coord_sf(crs = 3087, datum = 3087)"},{"path":"census-geographic-data-and-applications-in-r.html","id":"working-with-geometries","chapter":"5 Census geographic data and applications in R","heading":"5.5 Working with geometries","text":"understanding coordinate reference system transformations important spatial data management geographic visualization, types geometric manipulations may required spatial analyst. can include shifting geometries “correct” positions better cartographic display; changing geometry type spatial object; “exploding” multipart geometries individual geometric units. section gives examples three scenarios.","code":""},{"path":"census-geographic-data-and-applications-in-r.html","id":"shifting-and-rescaling-geometry-for-national-us-mapping","chapter":"5 Census geographic data and applications in R","heading":"5.5.1 Shifting and rescaling geometry for national US mapping","text":"common problem national display United States fragmented nature US states territories geographically. continental United States can displayed map relatively straightforward way, number projected coordinate reference systems designed correct display continental US. Often, analysts cartographers make decisions handle Alaska, Hawaii, Puerto Rico, reasonably plotted using default US projections.example, let’s take US states shapefile obtained tigris low resolution use ggplot2 visualize default geographic CRS, NAD 1983:\nFigure 5.15: Default CRS US states\nplot work well, part Aleutian Islands far west Alaska cross 180 degree line longitude plotted opposite side map. response, projected coordinate reference system appropriate United States used, continental US Albers Equal Area projection:\nFigure 5.16: Equal-area CRS US states\nrepresentation puts territories appropriate locations, clearly appropriate Alaska, Hawaii, Puerto Rico appear distorted. coordinate reference system also ideal comparative mapping states given large amount blank space states map.tigris offers solution problem shift_geometry() function. shift_geometry() takes opinionated approach shifting rescaling Alaska, Hawaii, Puerto Rico geometries offer four options alternative view US. function works projecting geometries Alaska, Hawaii, Puerto Rico appropriate coordinate reference systems areas, re-sizing geometries (requested) moving alternative layout relationship rest US using Albers Equal Area CRS.\nFigure 5.17: US states shifted rescaled geometry\nview uses two default arguments: preserve_area = FALSE, shrinks Alaska inflates Hawaii Puerto Rico, position = \"\", places areas continental United States. Alternatively, can set preserve_area = TRUE position = \"outside\" (used together , can mixed matched) different view:\nFigure 5.18: US states shifted geometry consistent area\nareas Alaska, Hawaii, Puerto Rico preserved relative continental United States, three areas directionally correct positions still proximity continental US national display. addition spatial objects obtained tigris, shift_geometry() can shift rescale geographic datasets display way. Just make sure use arguments shift_geometry() layers end misaligned!","code":"\nus_states <- states(cb = TRUE, resolution = \"20m\")\n\nggplot(us_states) + \n  geom_sf() + \n  theme_void()\nggplot(us_states) + \n  geom_sf() + \n  coord_sf(crs = 'ESRI:102003') + \n  theme_void()\nus_states_shifted <- shift_geometry(us_states)\n\nggplot(us_states_shifted) + \n  geom_sf() + \n  theme_void()\nus_states_outside <- shift_geometry(us_states, \n                                    preserve_area = TRUE,\n                                    position = \"outside\")\n\nggplot(us_states_outside) + \n  geom_sf() + \n  theme_void()"},{"path":"census-geographic-data-and-applications-in-r.html","id":"converting-polygons-to-points","chapter":"5 Census geographic data and applications in R","heading":"5.5.2 Converting polygons to points","text":"discussed earlier chapter, datasets obtained tigris returned geometry type POLYGON MULTIPOLYGON, reflecting fact Census geometries generally areal units Census data aggregated. makes sense many applications, instances default geometry type Census shapes necessary.example, let’s say making simple plot largest cities state Texas. places() function can obtain city geometries, states() function gives us outline state Texas. Two successive calls geom_sf() create graphic displays cities top state outline.\nFigure 5.19: Large cities Texas\nissue graphic city geographies actually quite irregular disjoint practice. six cities spread across large areas, holes, even cases include portions detached main part city. information important local planning purposes, unnecessary state-wide map.alternative representation possible converting city polygons points point represents centroid polygon, placed geometric centers. sf, conversion implemented function st_centroid(). , use st_centroid() convert polygons central points, plot points outline Texas.\nFigure 5.20: Large cities Texas represented points\ncities displayed circles rather irregular polygons, makes sense visualization cities’ locations scale.","code":"\ntx_places <- places(\"TX\", cb = TRUE) %>%\n  filter(NAME %in% c(\"Dallas\", \"Fort Worth\", \"Houston\",\n                     \"Austin\", \"San Antonio\", \"El Paso\")) %>%\n  st_transform(6580)\n\ntx_outline <- states(cb = TRUE) %>%\n  filter(NAME == \"Texas\") %>%\n  st_transform(6580)\n\nggplot() + \n  geom_sf(data = tx_outline) + \n  geom_sf(data = tx_places, fill = \"red\", color = NA) + \n  theme_void()\ntx_centroids <- st_centroid(tx_places)\n\nggplot() + \n  geom_sf(data = tx_outline) + \n  geom_sf(data = tx_centroids, color = \"red\", size = 3) + \n  theme_void()"},{"path":"census-geographic-data-and-applications-in-r.html","id":"exploding-multipolygon-geometries-to-single-parts","chapter":"5 Census geographic data and applications in R","heading":"5.5.3 Exploding multipolygon geometries to single parts","text":"Generally speaking, areal Census features returned geometry type MULTIPOLYGON. makes sense many Census shapes - including several states - include disconnected areas islands belong Census area. particularly significant Florida, let’s return Florida counties dataset used earlier chapter, take example Lee County Florida’s western coast.\nFigure 5.21: Lee County, Florida\nLee County polygon four distinct parts displayed map: mainland area contains cities Cape Coral Fort Myers, three disjoint island areas. islands can extracted using techniques covered Chapter 7. Understandably, four areas interpreted R single feature:Specific parts multipolygon Lee County object can extracted exploding multipart geometry single parts. accomplished sf’s function st_cast(), can convert spatial objects one geometry type another. example, “cast” Lee County POLYGON object create separate row non-contiguous area. analysts coming desktop GIS background, perform similar operation “Multipart Singlepart” geoprocessing tools.resulting spatial object now four rows. Using row indexing, can extract rows individual object, area representing Sanibel Island.\nFigure 5.22: Sanibel Island, Florida\n","code":"\nlee <- fl_projected %>%\n  filter(NAME == \"Lee\")\nmapview(lee)\nlee## Simple feature collection with 1 feature and 12 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 571477.3 ymin: 258768.2 xmax: 642726 ymax: 310584\n## Projected CRS: NAD83(HARN) / Florida GDL Albers\n##   STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID NAME   NAMELSAD STUSPS\n## 1      12      071 00295758 0500000US12071 12071  Lee Lee County     FL\n##   STATE_NAME LSAD      ALAND     AWATER                       geometry\n## 1    Florida   06 2022803068 1900583561 MULTIPOLYGON (((580415.7 30...\nlee_singlepart <- st_cast(lee, \"POLYGON\")\n\nlee_singlepart## Simple feature collection with 4 features and 12 fields\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 571477.3 ymin: 258768.2 xmax: 642726 ymax: 310584\n## Projected CRS: NAD83(HARN) / Florida GDL Albers\n##     STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID NAME   NAMELSAD STUSPS\n## 1        12      071 00295758 0500000US12071 12071  Lee Lee County     FL\n## 1.1      12      071 00295758 0500000US12071 12071  Lee Lee County     FL\n## 1.2      12      071 00295758 0500000US12071 12071  Lee Lee County     FL\n## 1.3      12      071 00295758 0500000US12071 12071  Lee Lee County     FL\n##     STATE_NAME LSAD      ALAND     AWATER                       geometry\n## 1      Florida   06 2022803068 1900583561 POLYGON ((580415.7 300219.6...\n## 1.1    Florida   06 2022803068 1900583561 POLYGON ((576540.8 289935.7...\n## 1.2    Florida   06 2022803068 1900583561 POLYGON ((572595.8 298881, ...\n## 1.3    Florida   06 2022803068 1900583561 POLYGON ((571477.3 310583.5...\nsanibel <- lee_singlepart[2,]\n\nmapview(sanibel)"},{"path":"census-geographic-data-and-applications-in-r.html","id":"exercises-3","chapter":"5 Census geographic data and applications in R","heading":"5.6 Exercises","text":"Give tigris try ! Go available geographies tigris fetch data state /county choosing.Give tigris try ! Go available geographies tigris fetch data state /county choosing.Using data, try plotting options covered chapter. Plot data plot(), geom_sf(), mapview().Using data, try plotting options covered chapter. Plot data plot(), geom_sf(), mapview().Use suggest_crs() crsuggest package identify appropriate projected coordinate reference system data. , use st_transform() sf package apply CRS transformation data.Use suggest_crs() crsuggest package identify appropriate projected coordinate reference system data. , use st_transform() sf package apply CRS transformation data.","code":""},{"path":"mapping-census-data-with-r.html","id":"mapping-census-data-with-r","chapter":"6 Mapping Census data with R","heading":"6 Mapping Census data with R","text":"Data United States Census Bureau commonly visualized using maps, given Census ACS data aggregated enumeration units. chapter cover process map-making using tidycensus R package. Notably, tidycensus enables R users download simple feature geometry common geographies, linking demographic information geographic locations dataset. turn, data model facilitates creation static interactive demographic maps.chapter, readers learn use geometry parameter tidycensus functions download geographic data along demographic data US Census Bureau. chapter cover make static maps Census demographic data using popular ggplot2 tmap visualization packages. closing parts chapter turn interactive mapping, focus mapview Leaflet R packages interactive cartographic visualization.","code":""},{"path":"mapping-census-data-with-r.html","id":"using-geometry-in-tidycensus","chapter":"6 Mapping Census data with R","heading":"6.1 Using geometry in tidycensus","text":"covered previous chapter, Census geographies available tigris R package simple features objects, using data model sf R package. tidycensus wraps several common geographic data functions tigris package allow R users return simple feature geometry pre-linked downloaded demographic data single function call. key argument accomplish geometry = TRUE, available core data download functions tidycensus, get_acs(), get_decennial(), get_estimates().Traditionally, getting “spatial” Census data requires tedious multi-step process can involve several software platforms. steps include:Fetching shapefiles Census website;Fetching shapefiles Census website;Downloading CSV data, cleaning formatting ;Downloading CSV data, cleaning formatting ;Loading geometries data desktop GIS choice;Loading geometries data desktop GIS choice;Aligning key fields desktop GIS joining data.Aligning key fields desktop GIS joining data.major motivation developing tidycensus frustration go process making simple map Census data. geometry = TRUE combines automated data download functionality tidycensus tigris allow R users bypass process entirely. following example illustrates use geometry = TRUE argument, fetching information median household income Census tracts District Columbia. discussed previous chapter, option tigris_use_cache = TRUE used cache downloaded geographic data user’s computer.shown example call, structure object returned tidycensus resembles object ’ve become familiar point book. example, median household income data found estimate column associated margins error moe column, along variable ID, GEOID, Census tract name. However, notable differences. geometry column contains polygon feature geometry Census tract, allowing linking estimates margins error corresponding locations Washington, DC. Beyond , object associated coordinate system information - using NAD 1983 geographic coordinate system Census geographic datasets stored default.","code":"\nlibrary(tidycensus)\noptions(tigris_use_cache = TRUE)\n\ndc_income <- get_acs(\n  geography = \"tract\", \n  variables = \"B19013_001\",\n  state = \"DC\", \n  year = 2020,\n  geometry = TRUE\n)\n\ndc_income## Simple feature collection with 206 features and 5 fields\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -77.11976 ymin: 38.79165 xmax: -76.9094 ymax: 38.99511\n## Geodetic CRS:  NAD83\n## First 10 features:\n##          GEOID                                                           NAME\n## 1  11001005003 Census Tract 50.03, District of Columbia, District of Columbia\n## 2  11001002503 Census Tract 25.03, District of Columbia, District of Columbia\n## 3  11001006801 Census Tract 68.01, District of Columbia, District of Columbia\n## 4  11001009802 Census Tract 98.02, District of Columbia, District of Columbia\n## 5  11001008904 Census Tract 89.04, District of Columbia, District of Columbia\n## 6  11001003301 Census Tract 33.01, District of Columbia, District of Columbia\n## 7  11001000704  Census Tract 7.04, District of Columbia, District of Columbia\n## 8  11001007601 Census Tract 76.01, District of Columbia, District of Columbia\n## 9  11001009604 Census Tract 96.04, District of Columbia, District of Columbia\n## 10 11001009803 Census Tract 98.03, District of Columbia, District of Columbia\n##      variable estimate   moe                       geometry\n## 1  B19013_001   116250 15922 POLYGON ((-77.03195 38.9096...\n## 2  B19013_001    80682 37845 POLYGON ((-77.02971 38.9376...\n## 3  B19013_001   109863 24472 POLYGON ((-76.98364 38.8898...\n## 4  B19013_001    54432 50966 POLYGON ((-77.00057 38.8309...\n## 5  B19013_001    47694  8992 POLYGON ((-76.98341 38.9001...\n## 6  B19013_001   160536 52157 POLYGON ((-77.01493 38.9206...\n## 7  B19013_001    86733 23440 POLYGON ((-77.07957 38.9334...\n## 8  B19013_001    40330  5556 POLYGON ((-76.9901 38.87135...\n## 9  B19013_001    72872 13143 POLYGON ((-76.9618 38.89612...\n## 10 B19013_001    29000 24869 POLYGON ((-77.00774 38.8356..."},{"path":"mapping-census-data-with-r.html","id":"basic-mapping-of-sf-objects-with-plot","chapter":"6 Mapping Census data with R","heading":"6.1.1 Basic mapping of sf objects with plot()","text":"geographic information can difficult understand without visualization. returned object simple features object, geometry attributes can visualized plot(). Key specifying name column plotted inside brackets, case \"estimate\".\nFigure 6.1: Base R plot median household income tract DC\nplot() function returns simple map showing income variation Washington, DC. Wealthier areas, represented warmer colors, tend located northwestern part District. NA values represented map white. desired, map can modified base plotting functions.remainder chapter, however, focus map-making additional data visualization packages R. includes popular ggplot2 package visualization, supports direct visualization simple features objects; tmap package thematic mapping, leaflet package interactive map-making calls Leaflet JavaScript framework directly R.","code":"\nplot(dc_income[\"estimate\"])"},{"path":"mapping-census-data-with-r.html","id":"map-making-with-ggplot2-and-geom_sf","chapter":"6 Mapping Census data with R","heading":"6.2 Map-making with ggplot2 and geom_sf","text":"illustrated Section 5.2, geom_sf() ggplot2 can used quick plotting sf objects using familiar ggplot2 syntax. geom_sf() goes far beyond simple cartographic display. full power ggplot2 available create highly customized maps geographic data visualizations.","code":""},{"path":"mapping-census-data-with-r.html","id":"choropleth-mapping","chapter":"6 Mapping Census data with R","heading":"6.2.1 Choropleth mapping","text":"One common ways visualize statistical information map choropleth mapping. Choropleth maps use shading represent underlying data values vary feature spatial dataset. income plot Washington, DC shown earlier chapter example choropleth map.example , tidycensus used obtain linked ACS spatial data median age state 50 US states plus District Columbia Puerto Rico. national maps, often preferable generate insets Alaska, Hawaii, Puerto Rico can viewed comparatively continental United States. ’ll use shift_geometry() function tigris shift rescale areas national mapping. argument resolution = \"20m\" necessary appropriate results, omit long archipelago islands northwest Hawaii.\nFigure 6.2: Plot shifted rescaled US geometry\nstate polygons can styled using ggplot2 conventions geom_sf() function. two lines ggplot2 code, basic map median age state can created ggplot2 defaults.\nFigure 6.3: US choropleth map ggplot2 defaults\ngeom_sf() function example interprets geometry sf object (case, polygon) visualizes result filled choropleth map. case, ACS estimate median age mapped default blue dark--light color ramp ggplot2, highlighting youngest states (Utah) darker blues oldest states (Maine) lighter blues.","code":"\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(tigris)\n\nus_median_age <- get_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2019,\n  survey = \"acs1\",\n  geometry = TRUE,\n  resolution = \"20m\"\n) %>%\n  shift_geometry()\n\nplot(us_median_age$geometry)\nggplot(data = us_median_age, aes(fill = estimate)) + \n  geom_sf()"},{"path":"mapping-census-data-with-r.html","id":"customizing-ggplot2-maps","chapter":"6 Mapping Census data with R","heading":"6.2.2 Customizing ggplot2 maps","text":"many cases, map-makers using ggplot2 want customize graphic . example, designer may want modify color palette reverse darker colors represent older areas. map also benefit additional information describing content data sources. modifications can specified way user update regular ggplot2 graphic. scale_fill_distiller() function allows users specify ColorBrewer palette use map, includes wide range sequential, diverging, qualitative color palettes (Brewer, Hatchard, Harrower 2003). labs() function can used add title, caption, better legend label plot. Finally, ggplot2 cartographers often want use theme_void() function remove background gridlines map.\nFigure 6.4: Styled choropleth US median age ggplot2\n","code":"\nggplot(data = us_median_age, aes(fill = estimate)) + \n  geom_sf() + \n  scale_fill_distiller(palette = \"RdPu\", \n                       direction = 1) + \n  labs(title = \"  Median Age by State, 2019\",\n       caption = \"Data source: 2019 1-year ACS, US Census Bureau\",\n       fill = \"ACS estimate\") + \n  theme_void()"},{"path":"mapping-census-data-with-r.html","id":"map-making-with-tmap","chapter":"6 Mapping Census data with R","heading":"6.3 Map-making with tmap","text":"ggplot2 users, geom_sf() offers familiar interface mapping data obtained US Census Bureau. However, ggplot2 far option cartographic visualization R. tmap package (Tennekes 2018) excellent alternative mapping R includes wide range functionality custom cartography. section follows overview several cartographic techniques implemented tmap visualizing US Census data. full treatment best practices cartographic design beyond scope section; recommended resources learning include Peterson (2020) Brewer (2016).begin, obtain race ethnicity data 2020 decennial US Census using get_decennial() function. ’ll looking data non-Hispanic white, non-Hispanic Black, Asian, Hispanic populations Census tracts Hennepin County, Minnesota.\nTable 6.1: Race ethnicity Hennepin County, MN\n’ve returned ACS data tidycensus’s regular “tidy” long format, useful moment comparative map-making, completed basic data wrangling tasks learned Chapter 3 calculate group percentages. get started mapping data, ’ll extract single group dataset illustrate tmap works.","code":"\nhennepin_race <- get_decennial(\n  geography = \"tract\",\n  state = \"MN\",\n  county = \"Hennepin\",\n  variables = c(\n    Hispanic = \"P2_002N\",\n    White = \"P2_005N\",\n    Black = \"P2_006N\",\n    Native = \"P2_007N\",\n    Asian = \"P2_008N\"\n  ),\n  summary_var = \"P2_001N\",\n  year = 2020,\n  geometry = TRUE\n) %>%\n  mutate(percent = 100 * (value / summary_value))"},{"path":"mapping-census-data-with-r.html","id":"choropleth-maps-with-tmap","chapter":"6 Mapping Census data with R","heading":"6.3.1 Choropleth maps with tmap","text":"tmap’s map-making syntax somewhat familiar users ggplot2, uses concept layers specify modifications map. map object initialized tm_shape(), allows us view Census tracts tm_polygons(). ’ll first filter long-form spatial dataset get unique set tract polygons, visualize .\nFigure 6.5: Basic polygon plot tmap\nget default view Census tracts Hennepin County, Minnesota. Alternatively, tm_fill() function can used produce choropleth maps, illustrated ggplot2 examples .\nFigure 6.6: Basic choropleth tmap\n’ll notice tmap uses classed color scheme rather continuous palette used ggplot2, default. involves identification “classes” distribution data values mapping color color palette data values belong class. default classification scheme used tm_fill() \"pretty\", identifies clean-looking intervals data based data range. example, data classes change every 20 percent. However, approach always sensitive distribution data values. Let’s take look data distribution understand :\nFigure 6.7: Base R histogram percent Black Census tract\nhistogram illustrates, Census tracts Hennepin County Black populations 20 percent. turn, variation within bucket visible map given tracts fall one class. style argument tm_fill() supports number methods classification, including quantile breaks (\"quantile\"), equal intervals (\"equal\"), Jenks natural breaks (\"jenks\"). Let’s switch quantiles , class contain number Census tracts. can also change color palette add contextual text ggplot2.\nFigure 6.8: tmap choropleth options\nSwitching default classification scheme quantiles reveals additional neighborhood-level heterogeneity Hennepin County’s Black population suburban areas. However, mask heterogeneity Minneapolis top class now includes values ranging 21 percent 88 percent. “compromise” solution commonly used GIS cartography applications Jenks natural-breaks method, uses algorithm identify meaningful breaks data bin boundaries (Jenks 1967). assist understanding different classification methods work, legend.hist argument tm_polygons() can set TRUE, adding histogram map bars colored values used map.\nFigure 6.9: Styled tmap choropleth\ntm_layout() function used customize styling map histogram, many options beyond shown can viewed function’s documentation.","code":"\nlibrary(tmap)\nhennepin_black <- filter(hennepin_race, \n                         variable == \"Black\")\n\ntm_shape(hennepin_black) + \n  tm_polygons()\ntm_shape(hennepin_black) + \n  tm_polygons(col = \"percent\")\nhist(hennepin_black$percent)\ntm_shape(hennepin_black) + \n  tm_polygons(col = \"percent\",\n          style = \"quantile\",\n          n = 5,\n          palette = \"Purples\",\n          title = \"2020 US Census\") + \n  tm_layout(title = \"Percent Black\\nby Census tract\",\n            frame = FALSE,\n            legend.outside = TRUE)\ntm_shape(hennepin_black) + \n  tm_polygons(col = \"percent\",\n          style = \"jenks\",\n          n = 5,\n          palette = \"Purples\",\n          title = \"2020 US Census\",\n          legend.hist = TRUE) + \n  tm_layout(title = \"Percent Black\\nby Census tract\",\n            frame = FALSE,\n            legend.outside = TRUE,\n            bg.color = \"grey70\",\n            legend.hist.width = 5,\n            fontfamily = \"Verdana\")"},{"path":"mapping-census-data-with-r.html","id":"adding-reference-elements-to-a-map","chapter":"6 Mapping Census data with R","heading":"6.3.2 Adding reference elements to a map","text":"choropleth map illustrated previous example represents data statistical graphic, map histogram showing underlying data distribution. Cartographers coming R desktop GIS background accustomed adding variety reference elements map layouts provide additional geographical context map. elements may include basemap, north arrow, scale bar, can accommodated tmap.quickest way get basemaps use tmap rosm package Dunnington (2019), helps users access freely-available basemaps variety different providers. cases, users want design basemaps use R projects. example shown uses mapboxapi R package K. Walker (2021c), gives users Mapbox account access pre-designed Mapbox basemaps well custom-designed basemaps Mapbox Studio.use Mapbox basemaps mapboxapi tmap, ’ll first need Mapbox account access token. Mapbox accounts free; register Mapbox website find access token. R, token can set mb_access_token() function mapboxapi.set, basemap tiles input spatial dataset can fetched get_static_tiles() function mapboxapi, interacts Mapbox Static Tiles API. Mapbox Studio users can design custom basemap style use custom style ID along username fetch tiles style mapping; Mapbox user can also use default Mapbox styles supplying username = \"mapbox\" appropriate style ID. level detail underlying basemap can adjusted zoom argument.cases, users choose muted, monochrome basemap designing map choropleth overlay avoid confusing blending colors.basemap tiles layered familiar tmap workflow tm_rgb() function. show underlying basemap, users modify transparency Census tract polygons alpha argument. Additional tmap functions add ancillary map elements. tm_scale_bar() adds scale bar; tm_compass() adds north arrow; tm_credits() helps cartographers give credit basemap, required using Mapbox OpenStreetMap tiles.\nFigure 6.10: Map percent Black Hennepin County reference elements\nDepending shape Census data, position arguments tm_scale_bar(), tm_compass(), tm_credits() can modified organize ancillary map elements appropriately. capitalized (used tm_credits()), element positioned tighter map frame.","code":"\nlibrary(mapboxapi)\n\n# Replace with your token below\nmb_access_token(\"pk.ey92lksd...\")\n# If you don't have a Mapbox style to use, replace style_id with \"light-v9\"\n# and username with \"mapbox\".  If you do, replace those arguments with your \n# style ID and user name.\nhennepin_tiles <- get_static_tiles(\n  location = hennepin_black,\n  zoom = 10,\n  style_id = \"ckedp72zt059t19nssixpgapb\",\n  username = \"kwalkertcu\"\n)\ntm_shape(hennepin_tiles) + \n  tm_rgb() + \n  tm_shape(hennepin_black) + \n  tm_polygons(col = \"percent\",\n          style = \"jenks\",\n          n = 5,\n          palette = \"Purples\",\n          title = \"2020 US Census\",\n          alpha = 0.7) +\n  tm_layout(title = \"Percent Black\\nby Census tract\",\n            legend.outside = TRUE,\n            fontfamily = \"Verdana\") + \n  tm_scale_bar(position = c(\"left\", \"bottom\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_credits(\"(c) Mapbox, OSM    \", \n             bg.color = \"white\",\n             position = c(\"RIGHT\", \"BOTTOM\"))"},{"path":"mapping-census-data-with-r.html","id":"choosing-a-color-palette","chapter":"6 Mapping Census data with R","heading":"6.3.3 Choosing a color palette","text":"examples shown chapter thus far used variety color palettes display statistical variation choropleth maps. Software packages like sf, ggplot2, tmap color palettes built “defaults”; ’ve shown default palettes three, changed palettes used ggplot2 tmap. go choosing appropriate color palette? variety considerations take account.First, important understand type data working . data quantitative - , expressed numbers, ’ll commonly working using Census data - ’ll want color palette can show statistical variation data correctly. demographic examples shown , decennial Census data range low value high value. type information effectively represented sequential color palette. Sequential color palettes use either single hue related hues modify color lightness intensity generate sequence colors. example single-hue palette “Purples” ColorBrewer palette used map .\nFigure 6.11: Sequential ‘Purples’ color palette\npalette, lighter colors generally used represent lower data values, darker values represent higher values, suggesting greater density/concentration attribute. color palettes, however, intense colors may lighter colors used accordingly represent data values. case popular viridis color palettes, implemented R viridis package (Garnier et al. 2021) shown .\nFigure 6.12: Sequential ‘viridis’ color palette\nDiverging color palettes best used cartographer wants highlight extreme values either end data distribution represent neutral values middle. example shown ColorBrewer “RdBu” palette.\nFigure 6.13: Diverging ‘RdBu’ color palette\nCensus data mapping, diverging palettes well-suited maps visualize change time. map population change using diverging palette highlight extreme population loss extreme population gain intense colors either end palette, represent minimal population change muted, neutral color middle.Qualitative palettes appropriate categorical data, represent data values unique, unordered hues. good example “Set1” color palette shown .\nFigure 6.14: Categorical ‘Set1’ color palette\nmaps Census data returned tidycensus generally use sequential diverging color palettes (given quantitative nature Census data), derived data products may require qualitative palettes. Illustrative examples book include categorical dot-density maps (addressed later chapter) visualizations geodemographic clusters, explored Section 8.5.1.Choosing appropriate color palette maps can challenge. Fortunately, ColorBrewer viridis palettes appropriate wide range cartographic use cases built-support ggplot2 tmap. excellent tool helping decide color palette tmap’s Palette Explorer app, accessible command tmaptools::palette_explorer(). Run command R console launch interactive app helps explore different color scenarios using ColorBrewer viridis palettes. Notably, app includes color blindness simulator help choose color palettes color blindness friendly.","code":""},{"path":"mapping-census-data-with-r.html","id":"alternative-map-types-with-tmap","chapter":"6 Mapping Census data with R","heading":"6.3.4 Alternative map types with tmap","text":"Choropleth maps core part Census data analyst’s toolkit, ideal every application. particular, choropleth maps best suited visualizing rates, percentages, statistical values normalized population areal units. ideal analyst wants compare counts (estimated counts) , however. Choropleth maps count data may ultimately reflect underlying size baseline population; additionally, given counts compared visually relative irregular shape polygons, choropleth maps can make comparisons difficult.","code":""},{"path":"mapping-census-data-with-r.html","id":"graduated-symbols","chapter":"6 Mapping Census data with R","heading":"6.3.4.1 Graduated symbols","text":"alternative commonly used visualize count data graduated symbol map. Graduated symbol maps use shapes referenced geographic units sized relative data attribute. example uses tmap’s tm_bubbles() function create graduated symbol map Black population Hennepin County, mapping estimate column.\nFigure 6.15: Graduated symbols tmap\nvisual comparisons map made circles, polygons , reflecting differences population sizes.","code":"\ntm_shape(hennepin_black) + \n  tm_polygons() + \n  tm_bubbles(size = \"value\", alpha = 0.5, \n             col = \"navy\",\n             title.size = \"Non-Hispanic Black - 2020 US Census\") + \n  tm_layout(legend.outside = TRUE,\n            legend.outside.position = \"bottom\")"},{"path":"mapping-census-data-with-r.html","id":"faceted-maps","chapter":"6 Mapping Census data with R","heading":"6.3.4.2 Faceted maps","text":"Given long-form race & ethnicity dataset returned tidycensus includes information five groups, cartographer may want visualize groups comparatively. single choropleth map effectively visualize five demographic attributes simultaneously, creating five separate maps can tedious. solution using faceting, concept introduced Chapter ??.Faceted maps tmap created tm_facets() function. argument specifies column used identify unique groups data. remaining code familiar tmap code; example, tm_fill() preferred tm_polygons() hide Census tract borders given smaller sizes maps. legend also moved legend.position argument tm_layout() fill empty space faceted map.\nFigure 6.16: Faceted map tmap\nfaceted maps good job showing variations group comparative context. However, common legend classification scheme used means within-group variation suppressed relative need show consistent comparisons groups. turn, “White” subplot shows little variation among Census tracts Hennepin County given large size group area. One additional disadvantage separate maps group show neighborhood heterogeneity diversity well . popular alternative visualizing within-unit heterogeneity dot-density map, covered .","code":"\ntm_shape(hennepin_race) + \n  tm_facets(by = \"variable\", scale.factor = 4) + \n  tm_fill(col = \"percent\",\n          style = \"quantile\",\n          n = 6,\n          palette = \"Blues\",\n          title = \"Percent (2020 US Census)\",) + \n  tm_layout(bg.color = \"grey\", \n            legend.position = c(-0.7, 0.15),\n            panel.label.bg.color = \"white\")"},{"path":"mapping-census-data-with-r.html","id":"dot-density-maps","chapter":"6 Mapping Census data with R","heading":"6.3.4.3 Dot-density maps","text":"Dot-density maps scatter dots within areal units relative size data attribute. cartographic method intended show attribute density dot distributions; Census map, areas dots dense, people live , whereas sparsely positioned dots reflect sparsity population. Dot-density maps can also incorporate categories data visualize densities different subgroups simultaneously.as_dot_density() function tidycensus helps users get Census data ready dot-density visualization. input dataset, function requires specifying value column represents data attribute visualized, values_per_dot value determines many data values dot represent. group column partitions dots group shuffles visual ordering map one group occludes groups.example, specify value = \"estimate\" visualize data 2020 US Census; values_per_dot = 100 data dots ratio 100 people per dot; group = \"variables\" partition dots racial / ethnic group map.map created tm_dots() function, example combined background map using tm_polygons() show relative racial ethnic heterogeneity Census tracts Hennepin County.Issues dot-density maps can include overplotting dots can make legibility problem; experiment different dot sizes dots data ratios improve . Additionally, use Census tract polygons generate dots can cause visual issues. dots placed randomly within Census tract polygons, many cases placed locations people live (lakes Hennepin County). Dot distributions also follow tract boundaries, can create artificial impression abrupt changes population distributions along polygon boundaries (seen example map). solution dasymetric dot-density map (K. E. Walker 2018), first removes areas polygons known uninhabited runs dot-generation algorithm modified areas. as_dot_density() includes argument, erase_water, automatically remove water areas input shapes generating dots, avoiding dot placement large bodies water. technique uses erase_water() function tigris package, covered detail Section 7.5.","code":"\nhennepin_dots <- hennepin_race %>%\n  as_dot_density(\n    value = \"value\",\n    values_per_dot = 100,\n    group = \"variable\"\n  )\nbackground_tracts <- filter(hennepin_race, variable == \"White\")\n\ntm_shape(background_tracts) + \n  tm_polygons(col = \"white\", \n              border.col = \"grey\") + \n  tm_shape(hennepin_dots) +\n  tm_dots(col = \"variable\", \n          palette = \"Set1\",\n          size = 0.005, \n          title = \"1 dot = 100 people\") + \n  tm_layout(legend.outside = TRUE,\n            title = \"Race/ethnicity,\\n2020 US Census\")"},{"path":"mapping-census-data-with-r.html","id":"cartographic-workflows-with-non-census-data","chapter":"6 Mapping Census data with R","heading":"6.4 Cartographic workflows with non-Census data","text":"many instances, analyst may possess data available Census geography available ACS decennial Census. means geometry = TRUE functionality tidycensus, automatically enriches data geographic information, possible. cases, Census shapes obtained tigris can joined tabular data visualized.section covers two workflows. first reproduces popular red/blue election map common presidential election cycles. second focuses mapping zip code tabulation areas, ZCTAs, geography represents spatial location zip codes (postal codes) United States.","code":""},{"path":"mapping-census-data-with-r.html","id":"national-election-mapping-with-tigris-shapes","chapter":"6 Mapping Census data with R","heading":"6.4.1 National election mapping with tigris shapes","text":"enumeration units like Census tracts block groups generally used map Census data, Census shapes representing legal entities useful variety cartographic purposes. popular example political map, shows winner poll results election region. ’ll use data Cook Political Report generate basic red state/blue state map 2020 US Presidential election results. dataset downloaded June 5, 2021 available \"data/us_vote_2020.csv\" book GitHub repository.data include wide variety columns can visualized map. discussed previous chapter, comparative map United States can use shift_geometry() function tigris package shift rescale Alaska Hawaii. State geometries available tigris states() function, used arguments cb = TRUE resolution = \"20m\" appropriately generalize state geometries national mapping.create map, geometry data obtained tigris must joined election data Cook Political Report. accomplished left_join() function dplyr. dplyr’s *_join() family functions supported simple features objects, work context analogous common “Join” operations desktop GIS software. join functions work matching values one “key fields” two tables merging data two tables single output table. common join functions ’ll use spatial data left_join(), retains rows first dataset fills non-matching rows NA values, inner_join(), drops non-matching rows output dataset.Let’s try obtaining low-resolution state geometry tigris, shifting rescaling shift_geometry(), merging political data shapes, matching NAME column us_states state column vote2020.proceeding ’ll want quality checks. left_join(), values must match exactly NAME state merge correctly - always guaranteed using data different sources. Let’s check see problems:’ve matched 50 states plus District Columbia correctly. turn, joined dataset retained shifted rescaled geometry US states now includes election information tabular dataset can used mapping. achieve structure, specifying directionality join critical. spatial information retained join, spatial object must left-hand side join. pipeline, specified us_states object first used left_join() join election information states object. done reverse, lost spatial class information necessary make map.basic red state/blue state map using ggplot2 geom_sf(), manual color palette can supplied scale_fill_manual() function, filling state polygons based called column represents party state called.\nFigure 6.17: Map 2020 US presidential election results ggplot2\n","code":"\nlibrary(tidyverse)\nlibrary(tigris)\n\n# Data source: https://cookpolitical.com/2020-national-popular-vote-tracker\nvote2020 <- read_csv(\"data/us_vote_2020.csv\")\n\nnames(vote2020)##  [1] \"state\"            \"called\"           \"final\"            \"dem_votes\"       \n##  [5] \"rep_votes\"        \"other_votes\"      \"dem_percent\"      \"rep_percent\"     \n##  [9] \"other_percent\"    \"dem_this_margin\"  \"margin_shift\"     \"vote_change\"     \n## [13] \"stateid\"          \"EV\"               \"X\"                \"Y\"               \n## [17] \"State_num\"        \"Center_X\"         \"Center_Y\"         \"...20\"           \n## [21] \"2016 Margin\"      \"Total 2016 Votes\"\nus_states <- states(cb = TRUE, resolution = \"20m\") %>%\n  filter(NAME != \"Puerto Rico\") %>%\n  shift_geometry()\n\nus_states_joined <- us_states %>%\n  left_join(vote2020, by = c(\"NAME\" = \"state\"))\ntable(is.na(us_states_joined$state))## \n## FALSE \n##    51\nggplot(us_states_joined, aes(fill = called)) + \n  geom_sf(color = \"white\", lwd = 0.2) + \n  scale_fill_manual(values = c(\"blue\", \"red\")) + \n  theme_void() + \n  labs(fill = \"Party\",\n       title = \" 2020 US presidential election results by state\",\n       caption = \"Note: Nebraska and Maine split electoral college votes by congressional district\")"},{"path":"mapping-census-data-with-r.html","id":"understanding-and-working-with-zctas","chapter":"6 Mapping Census data with R","heading":"6.4.2 Understanding and working with ZCTAs","text":"granular geography many agencies release data zip code level. ideal geography visualization, given zip codes represent collections US Postal Service routes (sometimes even single building, Post Office box) guaranteed form coherent geographies. US Census Bureau allows approximation zip code mapping Zip Code Tabulation Areas, ZCTAs. ZCTAs shapes built Census blocks common zip code addresses block determines blocks allocated corresponding ZCTAs. ZCTAs recommended spatial analysis due irregularities, can useful visualizing data distributions granular geographies available.example Internal Revenue Service’s Statistics Income (SOI) data, includes wide range indicators derived tax returns. detailed geography available zip code level dataset, meaning within-county visualizations require using ZCTAs. Let’s read data 2018 IRS website:dataset contains 153 columns identified linked codebook. Geographies identified ZIPCODE column, shows aggregated data state (ZIPCODE == \"000000\") zip code. might interested understanding geography self-employment income within given region. ’ll retain variables N09400, represents number tax returns self-employment tax, N1, represents total number returns., ’ll need identify region zip codes analysis. tigris, zctas() function allows us fetch Zip Code Tabulation Areas shapefile. Given ZCTA geography irregular sometimes stretches across multiple states, shapefile entire United States must first downloaded. recommended shapefile caching options(tigris_use_cache = TRUE) used ZCTAs avoid long data download times.next chapter, ’ll learn use spatial overlay extract geographic data within specific region. said, starts_with parameter zctas() allows users filter ZCTAs based vector prefixes, can identify area without using spatial process. example, can get ZCTA data near Boston, MA using appropriate prefixes.Next, can use mapview() inspect results:\nFigure 6.18: ZCTAs Boston, MA area\nZCTA prefixes 021, 022, 024 cover much Boston metropolitan area; “holes” region represent areas like Boston Common covered ZCTAs. Let’s take quick look attributes:Either ZCTA4CE10 column GEOID10 column can matched appropriate zip code information IRS dataset visualization. code joins IRS data spatial dataset computes new column representing percentage returns self-employment income.\nTable 6.2: Self-employment percentages ZCTA Boston, MA area\nvariety ways visualize information. One method choropleth map, ’ve learned earlier chapter:\nFigure 6.19: Simple choropleth self-employment Boston\nchoropleth map shows self-employment filings common suburban Boston ZCTAs nearer urban core, generally speaking. However, might also interested understanding self-employment income filings located rather share relative total number returns filed. requires visualizing self_emp column directly. discussed earlier chapter, graduated symbol map tm_bubbles() preferable choropleth map purpose.\nFigure 6.20: Graduated symbol map self-employment ZCTA Boston\n","code":"\nirs_data <- read_csv(\"https://www.irs.gov/pub/irs-soi/18zpallnoagi.csv\")\n\nncol(irs_data)## [1] 153\nself_employment <- irs_data %>%\n  select(ZIPCODE, self_emp = N09400, total = N1)\nlibrary(mapview)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE)\n\nboston_zctas <- zctas(\n  cb = TRUE, \n  starts_with = c(\"021\", \"022\", \"024\"),\n  year = 2018\n)\nmapview(boston_zctas)\nnames(boston_zctas)## [1] \"ZCTA5CE10\"  \"AFFGEOID10\" \"GEOID10\"    \"ALAND10\"    \"AWATER10\"  \n## [6] \"geometry\"\nboston_se_data <- boston_zctas %>%\n  left_join(self_employment, by = c(\"GEOID10\" = \"ZIPCODE\")) %>%\n  mutate(pct_self_emp = 100 * (self_emp / total)) %>%\n  select(GEOID10, self_emp, pct_self_emp)\ntm_shape(boston_se_data, projection = 26918) + \n  tm_fill(col = \"pct_self_emp\", \n          palette = \"Purples\", \n          title = \"% self-employed,\\n2018 IRS SOI data\")\ntm_shape(boston_se_data) + \n  tm_polygons() + \n  tm_bubbles(size = \"self_emp\", \n             alpha = 0.5, \n             col = \"navy\",\n             title.size = \"Self-employed filers,\\n2018 IRS SOI data\")"},{"path":"mapping-census-data-with-r.html","id":"interactive-mapping","chapter":"6 Mapping Census data with R","heading":"6.5 Interactive mapping","text":"examples addressed chapter thus far focused static maps, output fixed rendering map. Modern web technologies - integration technologies R htmlwidgets package, discussed Section 4.7.4 - make creation interactive, explorable Census data maps straightforward.","code":""},{"path":"mapping-census-data-with-r.html","id":"interactive-mapping-with-leaflet","chapter":"6 Mapping Census data with R","heading":"6.5.1 Interactive mapping with Leaflet","text":"31,000 GitHub stars July 2021, Leaflet JavaScript library (Agafonkin 2020) one popular frameworks worldwide interactive mapping. RStudio team brought Leaflet R leaflet R package (Cheng, Karambelkar, Xie 2021), now powers several approaches interactive mapping R. following examples cover visualize Census data interactive Leaflet map using approaches mapview, tmap, core leaflet package.Let’s start getting illustrative data percentage population aged 25 bachelor’s degree higher 2016-2020 ACS. ’ll look information Census tract Dallas County, Texas.Chapter 5, learned quickly visualize geographic data obtained tigris interactive map using mapview() function mapview package. mapview() function also includes parameter zcol takes column dataset argument, visualizes column interactive choropleth map.\nFigure 6.21: Interactive mapview choropleth\nConversion tmap maps interactive Leaflet maps also straightforward command tmap_mode(\"view\"). entering command, subsequent tmap maps R session rendered interactive Leaflet maps using tmap syntax ’d use make static maps.\nFigure 6.22: Interactive map tmap view mode\nswitch back static plotting mode, run command tmap_mode(\"plot\").fine-grained control Leaflet maps, core leaflet package can used. , ’ll reproduce mapview/tmap examples using leaflet package’s native syntax. First, color palette defined using colorNumeric() function. function creates function ’re calling pal(), translates data values color values given color palette. chosen color palette example viridis magma palette.map built magrittr pipeline following steps:leaflet() function initializes map. data object can specified function comes later pipeline.leaflet() function initializes map. data object can specified function comes later pipeline.addProviderTiles() helps add basemap map shown beneath data reference. Several providers built-Leaflet package, including popular Stamen reference maps. interested basic basemap, addTiles() function returns standard OpenStreetMap basemap. Use built-providers object try different basemaps; good practice choropleth mapping use greyscale muted basemap.addProviderTiles() helps add basemap map shown beneath data reference. Several providers built-Leaflet package, including popular Stamen reference maps. interested basic basemap, addTiles() function returns standard OpenStreetMap basemap. Use built-providers object try different basemaps; good practice choropleth mapping use greyscale muted basemap.addPolygons() adds tract polygons map styles . code , using series options specify input data; color polygons relative defined color palette; adjust smoothing polygon borders, opacity, line weight. label argument create hover tooltip map additional information polygons.addPolygons() adds tract polygons map styles . code , using series options specify input data; color polygons relative defined color palette; adjust smoothing polygon borders, opacity, line weight. label argument create hover tooltip map additional information polygons.addLegend() creates legend map, providing critical information colors map relate data values.addLegend() creates legend map, providing critical information colors map relate data values.\nFigure 6.23: Interactive leaflet map\nexample scratches surface leaflet R package can accomplish; ’d encourage review documentation examples.","code":"\ndallas_bachelors <- get_acs(\n  geography = \"tract\",\n  variables = \"DP02_0068P\",\n  year = 2020,\n  state = \"TX\",\n  county = \"Dallas\",\n  geometry = TRUE\n)\nlibrary(mapview)\nmapview(dallas_bachelors, zcol = \"estimate\")\nlibrary(tmap)\ntmap_mode(\"view\")\n\ntm_shape(dallas_bachelors) + \n  tm_fill(col = \"estimate\", palette = \"magma\",\n          alpha = 0.5)\nlibrary(leaflet)\n\npal <- colorNumeric(\n  palette = \"magma\",\n  domain = dallas_bachelors$estimate\n)\n\npal(c(10, 20, 30, 40, 50))## [1] \"#170F3C\" \"#420F75\" \"#6E1E81\" \"#9A2D80\" \"#C73D73\"\nleaflet() %>%\n  addProviderTiles(providers$Stamen.TonerLite) %>%\n  addPolygons(data = dallas_bachelors,\n              color = ~pal(estimate),\n              weight = 0.5,\n              smoothFactor = 0.2,\n              fillOpacity = 0.5,\n              label = ~estimate) %>%\n  addLegend(\n    position = \"bottomright\",\n    pal = pal,\n    values = dallas_bachelors$estimate,\n    title = \"% with bachelor's<br/>degree\"\n  )"},{"path":"mapping-census-data-with-r.html","id":"alternative-approaches-to-interactive-mapping","chapter":"6 Mapping Census data with R","heading":"6.5.2 Alternative approaches to interactive mapping","text":"Like interactive mapping platforms, Leaflet uses tiled mapping Web Mercator coordinate reference system. Web Mercator works well tiled web maps need fit within rectangular computer screens, preserves angles large scales (zoomed-areas) useful local navigation. However, grossly distorts area geographic features near poles, making inappropriate small-scale thematic mapping world world regions (Battersby et al. 2014).Let’s illustrate mapping median home value state 1-year ACS using leaflet. ’ll first acquire data geometry using tidycensus, setting output resolution “20m” get low-resolution boundaries speed interactive mapping.acquired ACS data US can mapped using techniques educational attainment map Dallas County.\nFigure 6.24: Interactive US map using Web Mercator\ndisadvantages Web Mercator - well general approach mapping United States - full display. Alaska’s area grossly distorted relative rest United States. also difficult map compare Alaska Hawaii continental US - particularly important example Hawaii’s median home value highest entire country. solution proposed elsewhere book use tigris::shift_geometry() adopts appropriate coordinate reference systems Alaska, Hawaii, continental US arranges better comparative fashion map. However, approach risks losing interactivity Leaflet map.compromise solution can involve R packages allow interactive mapping. excellent option ggiraph package (Gohel Skintzos 2021), like plotly package can convert ggplot2 graphics interactive plots. example , interactivity added ggplot2 plot ggiraph, allowing panning zooming hover tooltip shifted rescaled map US.\nFigure 6.25: Interactive US map ggiraph\n","code":"\nus_value <- get_acs(\n  geography = \"state\",\n  variables = \"B25077_001\",\n  year = 2019,\n  survey = \"acs1\",\n  geometry = TRUE,\n  resolution = \"20m\"\n)\nlibrary(leaflet)\n\nus_pal <- colorNumeric(\n  palette = \"plasma\",\n  domain = us_value$estimate\n)\n\nleaflet() %>%\n  addProviderTiles(providers$Stamen.TonerLite) %>%\n  addPolygons(data = us_value,\n              color = ~us_pal(estimate),\n              weight = 0.5,\n              smoothFactor = 0.2,\n              fillOpacity = 0.5,\n              label = ~estimate) %>%\n  addLegend(\n    position = \"bottomright\",\n    pal = us_pal,\n    values = us_value$estimate,\n    title = \"Median home value\"\n  )\nlibrary(ggiraph)\nlibrary(scales)\n\nus_value_shifted <- us_value %>%\n  shift_geometry(position = \"outside\") %>%\n  mutate(tooltip = paste(NAME, estimate, sep = \": \"))\n\ngg <- ggplot(us_value_shifted, aes(fill = estimate)) + \n  geom_sf_interactive(aes(tooltip = tooltip, data_id = NAME), \n                      size = 0.1) + \n  scale_fill_viridis_c(option = \"plasma\", labels = label_dollar()) + \n  labs(title = \"Median housing value by State, 2019\",\n       caption = \"Data source: 2019 1-year ACS, US Census Bureau\",\n       fill = \"ACS estimate\") + \n  theme_void() \n  \ngirafe(ggobj = gg) %>%\n  girafe_options(opts_hover(css = \"fill:cyan;\"), \n                 opts_zoom(max = 10))"},{"path":"mapping-census-data-with-r.html","id":"advanced-examples","chapter":"6 Mapping Census data with R","heading":"6.6 Advanced examples","text":"examples discussed chapter thus far likely cover large proportion cartographic use cases Census data analysts. However, R allows cartographers go beyond core map types. final section chapter introduces options advanced visualization using data tidycensus.","code":""},{"path":"mapping-census-data-with-r.html","id":"mapping-migration-flows","chapter":"6 Mapping Census data with R","heading":"6.6.1 Mapping migration flows","text":"2021, tidycensus co-author Matt Herman added support ACS Migration Flows API package, covered briefly Section 2.5. One notable feature new functionality, available get_flows() function, built-support flow mapping argument geometry = TRUE. Geometry operates differently migration flows data given geography interest single location given row, rather connection locations. turn, get_flows(), geometry = TRUE returns two POINT geometry columns: one location , one location linked.Let’s take look one popular recent migration destinations United States: Travis County Texas, home Austin.\nTable 6.3: Top origins migrants Travis County, TX\ndataset filtered focus -migration, represented MOVEDIN variable, drops migrations outside United States na.omit() (areas GEOID value).mapdeck R package (Cooley 2020) offers excellent support interactive flow mapping minimal code. mapdeck wrapper deck.gl, tremendous visualization library originally developed Uber offers 3D mapping capabilities built top Mapbox’s GL JS library. Users need sign Mapbox account get Mapbox access token use mapdeck; see mapdeck documentation information.set, flow maps can created initializing mapdeck map mapdeck() using add_arc() function, can link either X/Y coordinate columns POINT geometry columns, shown . example, using top 30 origins migrants Travis County, generating new weight column makes proportional arc widths less bulky.\nFigure 6.26: Flow map -migration Travis County, TX mapdeck\n","code":"\ntravis_inflow <- get_flows(\n  geography = \"county\",\n  state = \"TX\",\n  county = \"Travis\",\n  geometry = TRUE\n) %>%\n  filter(variable == \"MOVEDIN\") %>%\n  na.omit() %>%\n  arrange(desc(estimate))\nlibrary(mapdeck)\n\ntoken <- \"YOUR TOKEN HERE\"\n\ntravis_inflow %>%\n  slice_max(estimate, n = 30) %>%\n  mutate(weight = estimate / 500) %>%\n  mapdeck(token = token) %>%\n  add_arc(origin = \"centroid2\",\n          destination = \"centroid1\",\n          stroke_width = \"weight\",\n          update_view = FALSE) "},{"path":"mapping-census-data-with-r.html","id":"linking-maps-and-charts","chapter":"6 Mapping Census data with R","heading":"6.6.2 Linking maps and charts","text":"chapter Chapter 4 linked many ways. visualization principles discussed chapter apply ; key difference chapter focuses geographic visualization whereas Chapter 4 . many cases, ’ll want take advantage strengths geographic non-geographic visualization. Maps excellent showing patterns trends space, offer familiar reference viewers; charts better showing rankings ordering data values places.example illustrates combine two chart types discussed book: choropleth map margin error plot. Margins error notoriously difficult display maps; possible options include superimposing patterns choropleth maps highlight areas high levels uncertainty (Wong Sun 2013) using bivariate choropleth maps simultaneously visualize estimates MOEs (Lucchesi Wikle 2017).R’s visualization tools offer alternative approach: interactive linking choropleth map chart clearly visualizes uncertainty around estimates. approach involves generating map chart ggplot2, combining plots patchwork rendering interactive, linked graphics ggiraph. key aesthetic used data_id, set code plots highlight corresponding data points plots user hover.Example code generate linked visualization .\nFigure 6.27: Linked map chart ggiraph\nexample largely re-purposes visualization code readers learned examples book. Try hovering cursor county Vermont map - data point chart - notice happens plot. corresponding county data point also highlighted, allowing linked representation geographic location margin error visualization.","code":"\nlibrary(tidycensus)\nlibrary(ggiraph)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(scales)\n\nvt_income <- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\",\n  state = \"VT\",\n  year = 2020,\n  geometry = TRUE\n) %>%\n  mutate(NAME = str_remove(NAME, \" County, Vermont\"))\n\nvt_map <- ggplot(vt_income, aes(fill = estimate)) + \n  geom_sf_interactive(aes(data_id = GEOID)) + \n  scale_fill_distiller(palette = \"Greens\",\n                       direction = 1, \n                       guide = \"none\") + \n  theme_void()\n\nvt_plot <- ggplot(vt_income, aes(x = estimate, y = reorder(NAME, estimate), \n                                 fill = estimate)) +\n  geom_errorbar(aes(xmin = estimate - moe, xmax = estimate + moe)) +\n  geom_point_interactive(color = \"black\", size = 4, shape = 21,\n                         aes(data_id = GEOID)) +\n  scale_fill_distiller(palette = \"Greens\", direction = 1,\n                       labels = label_dollar()) + \n  scale_x_continuous(labels = label_dollar()) + \n  labs(title = \"Household income by county in Vermont\",\n       subtitle = \"2016-2020 American Community Survey\",\n       y = \"\",\n       x = \"ACS estimate (bars represent margin of error)\",\n       fill = \"ACS estimate\") + \n  theme_minimal(base_size = 14)\n\ngirafe(ggobj = vt_map + vt_plot, width_svg = 10, height_svg = 5) %>%\n  girafe_options(opts_hover(css = \"fill:cyan;\"))"},{"path":"mapping-census-data-with-r.html","id":"reactive-mapping-with-shiny","chapter":"6 Mapping Census data with R","heading":"6.6.3 Reactive mapping with Shiny","text":"Advanced Census cartographers may want take examples step build full-fledged data dashboards web-based visualization applications. Fortunately, R users don’t scratch. Shiny (Chang et al. 2021) tremendously popular powerful framework development interactive web applications R can execute R code response user inputs. full treatment Shiny beyond scope book; however, Shiny “must-learn” R data analysts.example Shiny visualization app extends race/ethnicity example chapter shown . includes drop-menu allows users select racial ethnic group Twin Cities visualizes distribution group interactive choropleth map uses Leaflet viridis color palette.\nFigure 6.28: Interactive mapping app Shiny\ncode used create app found ; copy-paste code R script, set Census API key, run ! learn , encourage review Hadley Wickham’s Mastering Shiny book Leaflet package’s documentation Shiny integration.","code":"\n# app.R\nlibrary(tidycensus)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(tidyverse)\n\ncensus_api_key(\"YOUR KEY HERE\")\n\ntwin_cities_race <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    hispanic = \"DP05_0071P\",\n    white = \"DP05_0077P\",\n    black = \"DP05_0078P\",\n    native = \"DP05_0079P\",\n    asian = \"DP05_0080P\",\n    year = 2019\n  ),\n  state = \"MN\",\n  county = c(\"Hennepin\", \"Ramsey\", \"Anoka\", \"Washington\",\n             \"Dakota\", \"Carver\", \"Scott\"),\n  geometry = TRUE\n) \n\ngroups <- c(\"Hispanic\" = \"hispanic\",\n            \"White\" = \"white\",\n            \"Black\" = \"black\",\n            \"Native American\" = \"native\",\n            \"Asian\" = \"asian\")\n\nui <- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\n        inputId = \"group\",\n        label = \"Select a group to map\",\n        choices = groups\n      )\n    ),\n    mainPanel(\n      leafletOutput(\"map\", height = \"600\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  \n  # Reactive function that filters for the selected group in the drop-down menu\n  group_to_map <- reactive({\n    filter(twin_cities_race, variable == input$group)\n  })\n  \n  # Initialize the map object, centered on the Minneapolis-St. Paul area\n  output$map <- renderLeaflet({\n\n    leaflet(options = leafletOptions(zoomControl = FALSE)) %>%\n      addProviderTiles(providers$Stamen.TonerLite) %>%\n      setView(lng = -93.21,\n              lat = 44.98,\n              zoom = 8.5)\n\n  })\n  \n  observeEvent(input$group, {\n    \n    pal <- colorNumeric(\"viridis\", group_to_map()$estimate)\n    \n    leafletProxy(\"map\") %>%\n      clearShapes() %>%\n      clearControls() %>%\n      addPolygons(data = group_to_map(),\n                  color = ~pal(estimate),\n                  weight = 0.5,\n                  fillOpacity = 0.5,\n                  smoothFactor = 0.2,\n                  label = ~estimate) %>%\n      addLegend(\n        position = \"bottomright\",\n        pal = pal,\n        values = group_to_map()$estimate,\n        title = \"% of population\"\n      )\n  })\n  \n}\n\nshinyApp(ui = ui, server = server)"},{"path":"mapping-census-data-with-r.html","id":"working-with-software-outside-of-r-for-cartographic-projects","chapter":"6 Mapping Census data with R","heading":"6.7 Working with software outside of R for cartographic projects","text":"examples shown chapter display maps within R. RStudio users running code chapter, example, display static plots Plots pane, interactive maps interactive Viewer pane. many cases, R users want export maps display website report. cases, R users might interested using R tidycensus “data pipeline” can generate appropriate Census data mapping external software package like Tableau, ArcGIS, QGIS. section covers use-cases.","code":""},{"path":"mapping-census-data-with-r.html","id":"exporting-maps-from-r","chapter":"6 Mapping Census data with R","heading":"6.7.1 Exporting maps from R","text":"Cartographers exporting maps made ggplot2 likely want use ggsave() command. map export process ggsave() similar process described Section 4.2.3. theme_void() used map, however, cartographer may want supply color bg parameter, otherwise default \"transparent\" theme.tmap_save() command best option exporting maps made tmap. tmap_save() requires map stored object first; example re-use map Hennepin County earlier chapter assign variable named hennepin_map.map can saved using similar options ggsave(). tmap_save() allows specification width, height, units, dpi. small values passed width height, tmap assume units inches; large values passed (greater 50), tmap assume units represent pixels.Interactive maps designed leaflet can written HTML documents using saveWidget() function htmlwidgets package. Leaflet map first assigned variable, passed saveWidget() along specified name path output HTML file. Interactive maps created mapview() written HTML files way, though object saved need accessed map slot notation @map, shown .argument selfcontained = TRUE important one consider writing interactive maps HTML files. TRUE shown example, saveWidget() bundle necessary assets (e.g. CSS, JavaScript) base64-encoded string HTML file. makes HTML file portable can lead large file sizes. alternative, selfcontained = FALSE, places assets accompanying directory HTML file written . interactive maps generated tmap, tmap_save() can also used write HTML files way.","code":"\nhennepin_map <- tm_shape(hennepin_black) + \n  tm_polygons(col = \"percent\",\n          style = \"jenks\",\n          n = 5,\n          palette = \"Purples\",\n          title = \"ACS estimate\",\n          legend.hist = TRUE) + \n  tm_layout(title = \"Percent Black\\nby Census tract\",\n            frame = FALSE,\n            legend.outside = TRUE,\n            bg.color = \"grey70\",\n            legend.hist.width = 5,\n            fontfamily = \"Verdana\")\ntmap_save(\n  tm = hennepin_map,\n  filename = \"~/images/hennepin_black_map.png\",\n  height = 5.5,\n  width = 8,\n  dpi = 300\n)\nlibrary(htmlwidgets)\n\ndallas_map <- mapview(dallas_bachelors, zcol = \"estimate\")\n\nsaveWidget(dallas_map@map, \"dallas_mapview_map.html\", selfcontained = TRUE)"},{"path":"mapping-census-data-with-r.html","id":"interoperability-with-other-visualization-software","chapter":"6 Mapping Census data with R","heading":"6.7.2 Interoperability with other visualization software","text":"Although R packages rich capabilities designing static interactive maps well map-based dashboards, analysts want turn specialized tools data visualization. tools might include drag--drop dashboard builders like Tableau, dedicated GIS software like ArcGIS QGIS allow manual control cartographic layouts outputs.workflow often involve use R, packages like tidycensus, data acquisition wrangling, use external visualization program data visualization cartography. workflow, R object produced tidycensus can written external spatial file st_write() function sf package. code illustrates write Census data R shapefile, common vector spatial data format readable desktop GIS software Tableau.output file dc_income.shp written user’s current working directory. spatial data formats like GeoPackage (.gpkg) GeoJSON (.geojson) available specifying appropriate file extension.QGIS cartographers can also take advantage functionality within software run R (consequently tidycensus functions) directly within platform. QGIS, enabled Processing R Provider plugin. QGIS users install plugin Plugins drop-menu, click Processing > Toolbox access QGIS’s suite tools. Clicking R icon Create New R Script… open R script editor.example script translated QGIS GIS tool uses version tidycensus installed user’s system add demographic layer Census tract level ACS QGIS project.Tool parameters defined beginning script ## notation. Variable, State, County accept strings (text) input, result get_acs() tool written Output, added QGIS project vector layer. finished, script saved appropriate name file extension .rsx suggested output directory, located R section Processing Toolbox opened. GIS tool look something like image .\nFigure 6.29: Example tidycensus tool QGIS\nFill text boxes desired ACS variable, state, county, click Run. QGIS tool call user’s R installation execute tool specified inputs. everything runs correctly, layer added user’s QGIS project ready mapping QGIS’s suite cartographic tools.\nFigure 6.30: Styled layer tidycensus QGIS\nexample shown displays Census tracts Cook County, Illinois obtained tidycensus QGIS project; tracts styled choropleth QGIS median household income (requested variable) tool added layer project.","code":"\nlibrary(tidycensus)\nlibrary(sf)\noptions(tigris_use_cache = TRUE)\n\ndc_income <- get_acs(\n  geography = \"tract\", \n  variables = \"B19013_001\",\n  state = \"DC\", \n  year = 2020,\n  geometry = TRUE\n)\n\nst_write(dc_income, \"dc_income.shp\")\n##Variable=string\n##State=string\n##County=string\n##Output=output vector\nlibrary(tidycensus)\n\nOutput = get_acs(\n    geography = \"tract\",\n    variables = Variable,\n    state = State,\n    county = County,\n    geometry = TRUE\n)"},{"path":"mapping-census-data-with-r.html","id":"exercises-4","chapter":"6 Mapping Census data with R","heading":"6.8 Exercises","text":"Using one mapping frameworks introduced chapter (either ggplot2, tmap, leaflet) complete following tasks:just getting started tidycensus/tidyverse, make race/ethnicity map adapting code provided section different county.just getting started tidycensus/tidyverse, make race/ethnicity map adapting code provided section different county.Next, find different variable map tidycensus::load_variables(). Review discussion cartographic choices chapter visualize data appropriately.Next, find different variable map tidycensus::load_variables(). Review discussion cartographic choices chapter visualize data appropriately.","code":""},{"path":"spatial-analysis-with-us-census-data.html","id":"spatial-analysis-with-us-census-data","chapter":"7 Spatial analysis with US Census data","heading":"7 Spatial analysis with US Census data","text":"common use-case spatial data US Census Bureau spatial analysis. Spatial analysis refers performance analytic tasks explicitly incorporate spatial properties dataset. Principles spatial analysis closely related field geographic information science, incorporates theoretical perspectives methodological insights regards use geographic data.Traditionally, geographic information science performed geographic information system, “GIS”, refers integrated software platform management, processing, analysis, visualization geographic data. evidenced book already, R packages exist handling tasks, allowing R function capable substitute desktop GIS software like ArcGIS QGIS.Traditionally, spatial analytic tasks R handled sp package allied packages rgeos. recent years, however, sf package emerged next-generation alternative sp spatial data analysis R. addition simpler representation vector spatial data R, discussed previous chapters, sf also includes significant functionality spatial data analysis integrates seamlessly tidyverse tools.chapter covers perform common spatial analysis tasks Census data using sf package. previous chapters, examples focus data acquired tidycensus tigris packages, cover common workflows use practitioners work US Census Bureau data.","code":""},{"path":"spatial-analysis-with-us-census-data.html","id":"spatial-overlay","chapter":"7 Spatial analysis with US Census data","heading":"7.1 Spatial overlay","text":"Spatial data analysis allows practitioners consider geographic datasets interrelate geographic space. analytic functionality facilitates answering wide range research analytic questions otherwise prove difficult without reference dataset’s geographic properties.One common use-case employed geospatial analyst spatial overlay. Key concept spatial overlay representation geographic datasets layers GIS. representation exemplified graphic (credit Rafael Pereira implementation).\nFigure 7.1: Conceptual view GIS layers\nrepresentation, different components landscape interact real world abstracted different layers, represented different geometries. example, Census tracts might represented polygons; customers points; roads linestrings. Separating components significant utility geospatial analyst, however. using spatial analytic tools, researcher answer questions like “many customers live within given Census tract?” “roads intersect given Census tract?”.","code":""},{"path":"spatial-analysis-with-us-census-data.html","id":"note-aligning-coordinate-reference-systems","chapter":"7 Spatial analysis with US Census data","heading":"7.1.1 Note: aligning coordinate reference systems","text":"Section 5.4 covered coordinate reference systems R, importance spatial data, select appropriate projected coordinate reference systems using crsuggest package. workflow using spatial overlay, including methods discussed chapter, essential layers share CRS overlay methods work.Spatial datasets obtained tigris tidycensus default share geographic CRS, NAD 1983. geographic coordinate reference systems, sf package uses s2 spherical geometry library (Dunnington, Pebesma, Rubak 2021) compute three-dimensional overlay rather assuming planar geometries geographic coordinates. represents significant technical advancement; however found can much slower compute spatial overlay operations way workflow using projected coordinate reference system.turn, recommended spatial analysis data preparation workflow follows:Download datasets plan use spatial analysis;Use suggest_crs() crsuggest package identify appropriate projected CRS layers;Transform data projected CRS using st_transform();Compute spatial overlay operation.avoid redundancy, step 2 implied examples chapter appropriate projected coordinate reference system pre-selected sections.","code":""},{"path":"spatial-analysis-with-us-census-data.html","id":"identifying-geometries-within-a-metropolitan-area","chapter":"7 Spatial analysis with US Census data","heading":"7.1.2 Identifying geometries within a metropolitan area","text":"One example utility spatial overlay Census data analyst use overlay techniques find geographies lie within given metropolitan area. Core-based statistical areas, also known metropolitan micropolitan areas, common geographies defined US Census Bureau regional analysis. Core-based statistical areas defined agglomerations counties oriented around central core cores, significant degree population interaction measured commuting patterns. Metropolitan areas core-based statistical areas population exceeding 50,000.Census data analyst United States often need know Census geographies, Census tracts block groups, fall within given metropolitan area. However, geographies organized state county, don’t metropolitan area identification included default. Given Census spatial datasets designed align one another, spatial overlay can used identify geographic features fall within given metropolitan area extract features.Let’s use example Kansas City metropolitan area, includes Census tracts Kansas Missouri. ’ll first use tigris acquire 2020 Census tracts two states comprise Kansas City region well boundary Kansas City metropolitan area.\nFigure 7.2: Kansas City CBSA relative Kansas Missouri\ncan see visually plot Census tracts within Kansas City metropolitan area, lay outside. spatial relationship represented image can expressed code using spatial subsetting, enabled functionality sf package.","code":"\nlibrary(tigris)\nlibrary(tidyverse)\nlibrary(sf)\noptions(tigris_use_cache = TRUE)\n\n# CRS used: NAD83(2011) Kansas Regional Coordinate System \n# Zone 11 (for Kansas City)\nks_mo_tracts <- map_dfr(c(\"KS\", \"MO\"), ~{\n  tracts(.x, cb = TRUE, year = 2020)\n}) %>%\n  st_transform(8528)  \n\nkc_metro <- core_based_statistical_areas(cb = TRUE, year = 2020) %>%\n  filter(str_detect(NAME, \"Kansas City\")) %>%\n  st_transform(8528)\n\nggplot() + \n  geom_sf(data = ks_mo_tracts, fill = \"white\", color = \"grey\") + \n  geom_sf(data = kc_metro, fill = NA, color = \"red\") + \n  theme_void()"},{"path":"spatial-analysis-with-us-census-data.html","id":"spatial-subsets-and-spatial-predicates","chapter":"7 Spatial analysis with US Census data","heading":"7.1.3 Spatial subsets and spatial predicates","text":"Spatial subsetting uses extent one spatial dataset extract features another spatial dataset based co-location, defined spatial predicate. Spatial subsets can expressed base R indexing notation:\nFigure 7.3: Census tracts intersect Kansas City CBSA\nspatial subsetting operation returns Census tracts intersect extent Kansas City metropolitan area, using default spatial predicate, st_intersects(). gives us back tracts fall within metro area’s boundary cross touch boundary. many analysts, however, insufficient want tabulate statistics exclusively tracts fall within metropolitan area’s boundaries. case, different spatial predicate can used op argument.Generally, Census analysts want use st_within() spatial predicate return tracts within given metropolitan area. long objects within core Census hierarchy obtained year tigris, st_within() spatial predicate cleanly return geographies fall within larger geography requested. example illustrates process using st_filter() function sf, allows spatial subsetting used cleanly within tidyverse-style pipeline. key difference two approaches spatial subsetting argument name spatial predicate (op vs. .predicate).\nFigure 7.4: Census tracts within Kansas City CBSA\n","code":"\nkc_tracts <- ks_mo_tracts[kc_metro, ]\n\nggplot() + \n  geom_sf(data = kc_tracts, fill = \"white\", color = \"grey\") + \n  geom_sf(data = kc_metro, fill = NA, color = \"red\") + \n  theme_void()\nkc_tracts_within <- ks_mo_tracts %>%\n  st_filter(kc_metro, .predicate = st_within)\n\n# Equivalent syntax: \n# kc_metro2 <- kc_tracts[kc_metro, op = st_within]\n\nggplot() + \n  geom_sf(data = kc_tracts_within, fill = \"white\", color = \"grey\") + \n  geom_sf(data = kc_metro, fill = NA, color = \"red\") + \n  theme_void()"},{"path":"spatial-analysis-with-us-census-data.html","id":"spatial-joins","chapter":"7 Spatial analysis with US Census data","heading":"7.2 Spatial joins","text":"Spatial joins extend aforementioned concepts spatial overlay transferring attributes spatial layers. Conceptually, spatial joins can thought like table joins covered Section 6.4.1 equivalent “key field” used match rows spatial relationship defined spatial predicate. Spatial joins R implemented sf’s st_join() function. section covers two common use cases spatial joins Census data. first topic point--polygon spatial join, table coordinates matched Census polygons determine demographic characteristics around locations. second topic covers polygon--polygon spatial joins, smaller Census shapes matched larger shapes.","code":""},{"path":"spatial-analysis-with-us-census-data.html","id":"point-in-polygon-spatial-joins","chapter":"7 Spatial analysis with US Census data","heading":"7.2.1 Point-in-polygon spatial joins","text":"Analysts commonly tasked matching point-level data Census shapes order study demographic differences. example, marketing analyst may dataset customers needs understand characteristics customers’ neighborhoods order target products efficiently. Similarly, health data analyst may need match neighborhood demographic data patient information understand inequalities patient outcomes. scenario explored section.Let’s consider hypothetical task health data analyst Gainesville, Florida needs determine percentage residents age 65 lack health insurance patients’ neighborhoods. analyst dataset patients patient ID along longitude latitude information.\nTable 7.1: Hypothetical dataset patients Gainesville, Florida\nWhereas spatial overlay example previous section used spatial datasets tigris already include geographic information, dataset needs converted simple features object. st_as_sf() function sf package can take R data frame tibble longitude latitude columns like create dataset geometry type POINT. convention, coordinate reference system used longitude / latitude data WGS 1984, represented EPSG code 4326. ’ll need specify CRS st_as_sf() sf can locate points correctly transform appropriate projected coordinate reference system st_transform().prepared spatial dataset, patient information can mapped.\nFigure 7.5: Map hypothetical patient locations Gainesville, Florida\npatient data now formatted simple features object, next step acquire data health insurance American Community Survey. pre-computed percentage ACS Data Profile available Census tract level, used example . Users require granular geography can construct information ACS Detailed Tables block group level using table B27001 techniques learned Section 3.3.2. Gainesville contained within Alachua County, Florida, can obtain data 2015-2019 5-year ACS accordingly.obtaining spatial & demographic data get_acs() geometry = TRUE argument, two additional commands help pre-process data spatial join. call select() retains three non-geometry columns simple features object: GEOID, Census tract ID, ACS estimate MOE renamed pct_insured pct_insured_moe, respectively. formats information appended patient data spatial join. st_transform() command aligns coordinate reference system Census tracts CRS used patient dataset.computing spatial join, spatial relationships patient points Census tract demographics can visualized interactively mapview(), layering two interactive views + operator.\nFigure 7.6: Layered interactive view patients Census tracts Gainesville\ninterrelationships patient points tract neighborhoods can explored map. relationships can formalized spatial join, implemented st_join() function sf package. st_join() returns new simple features object inherits geometry attributes first dataset x attributes second dataset y appended. Rows x matched rows y based spatial relationship defined spatial predicate, defaults st_join() st_intersects(). point--polygon spatial joins, default sufficient cases unless point falls directly boundary polygons (true example).\nTable 7.2: Patients dataset spatial join Census tracts\noutput dataset includes patient ID original POINT feature geometry, also now includes GEOID information Census tract dataset along neighborhood demographic information ACS. workflow can used analyses neighborhood characteristics wide variety applications generate data suitable hierarchical modeling.issue avoid interpreting results point--polygon spatial joins ecological fallacy, individual-level characteristics inferred neighborhood. neighborhood demographics useful inferring characteristics environment observation located, necessarily provide information demographics observation - particularly important observations represent people.","code":"\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(mapview)\n\ngainesville_patients <- tibble(\n  patient_id = 1:10,\n  longitude = c(-82.308131, -82.311972, -82.361748, -82.374377, \n                -82.38177, -82.259461, -82.367436, -82.404031, \n                -82.43289, -82.461844),\n  latitude = c(29.645933, 29.655195, 29.621759, 29.653576, \n               29.677201, 29.674923, 29.71099, 29.711587, \n               29.648227, 29.624037)\n)\n# CRS: NAD83(2011) / Florida North\ngainesville_sf <- gainesville_patients %>%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"),\n           crs = 4326) %>%\n  st_transform(6440)\nmapview(\n  gainesville_sf, \n  col.regions = \"red\",\n  legend = FALSE\n)\nalachua_insurance <- get_acs(\n  geography = \"tract\",\n  variables = \"DP03_0096P\",\n  state = \"FL\",\n  county = \"Alachua\",\n  year = 2019,\n  geometry = TRUE\n) %>%\n  select(GEOID, pct_insured = estimate, \n         pct_insured_moe = moe) %>%\n  st_transform(6440)\nmapview(\n  alachua_insurance,\n  zcol = \"pct_insured\", \n  layer.name = \"% with health<br/>insurance\"\n) + \n  mapview(\n    gainesville_sf,\n    col.regions = \"red\",\n    legend = FALSE\n  )\npatients_joined <- st_join(\n  gainesville_sf,\n  alachua_insurance\n)"},{"path":"spatial-analysis-with-us-census-data.html","id":"spatial-joins-and-group-wise-spatial-analysis","chapter":"7 Spatial analysis with US Census data","heading":"7.2.2 Spatial joins and group-wise spatial analysis","text":"Spatial data operations can also embedded workflows analysts interested understanding characteristics vary group. example, demographic data metropolitan areas can readily acquired using tidycensus functions, might also interested learning demographic characteristics neighborhoods within metropolitan areas vary across United States. example illustrates important new concepts spatial data analysts. involves polygon--polygon spatial join attention spatial predicate used important. Additionally, polygons involved acquired tidycensus get_acs(), section show st_join() handles column names duplicated datasets.","code":""},{"path":"spatial-analysis-with-us-census-data.html","id":"spatial-join-data-setup","chapter":"7 Spatial analysis with US Census data","heading":"7.2.2.1 Spatial join data setup","text":"Let’s say interested analyzing distributions neighborhoods (defined Census tracts) Hispanic population four largest metropolitan areas Texas. ’ll use variable B01003_001 2019 1-year ACS acquire population data core-based statistical area (CBSA) along simple feature geometry eventually used spatial join.\nTable 7.3: Large CBSAs Texas\nfiltering steps used merit additional explanation. expression filter(str_detect(NAME, \"TX\")) first subsets core-based statistical area data metropolitan micropolitan areas (partially ) Texas. Given string matching str_detect() case-sensitive, using \"TX\" search string match rows correctly. slice_max(), introduced Section 4.1, retains four rows largest population values, found estimate column. Finally, spatial dataset transformed appropriate projected coordinate reference system state Texas.Given four metropolitan areas completely contained within state Texas, can obtain data percent Hispanic tract ACS Data Profile 2015-2019.\nTable 7.4: Percent Hispanic Census tract Texas\nreturned dataset covers Census tracts entirety state Texas; however need retain tracts fall within four metropolitan areas interest. can accomplish spatial join using st_join().","code":"\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(sf)\n\n# CRS: NAD83(2011) / Texas Centric Albers Equal Area\ntx_cbsa <- get_acs(\n  geography = \"cbsa\",\n  variables = \"B01003_001\",\n  year = 2019,\n  survey = \"acs1\",\n  geometry = TRUE\n) %>%\n  filter(str_detect(NAME, \"TX\")) %>%\n  slice_max(estimate, n = 4) %>%\n  st_transform(6579)\npct_hispanic <- get_acs(\n  geography = \"tract\",\n  variables = \"DP05_0071P\",\n  state = \"TX\",\n  year = 2019,\n  geometry = TRUE\n) %>%\n  st_transform(6579)"},{"path":"spatial-analysis-with-us-census-data.html","id":"computing-and-visualizing-the-spatial-join","chapter":"7 Spatial analysis with US Census data","heading":"7.2.2.2 Computing and visualizing the spatial join","text":"know st_join(), request given spatial dataset x, geometry retained, gains attributes second spatial dataset y based spatial relationship. spatial relationship, examples, defined spatial predicate passed join parameter. argument suffix defines suffixes used columns share names, important given datasets came tidycensus. argument left = FALSE requests inner spatial join, returning tracts fall within four metropolitan areas.\nTable 7.5: Census tracts spatial join operation\noutput dataset reduced 5,265 Census tracts 3,189 result inner spatial join. Notably, output dataset now includes information Census tract metropolitan area falls within. enables group-wise data visualization analysis across metro areas faceted plot:\nFigure 7.7: Faceted density plot tract Hispanic populations CBSA Texas\nOutput spatial join operation can also “rolled ” larger geography group-wise data analysis. example, let’s say want know median value four distributions visualized plot . explained Section 3.3, can accomplish grouping dataset metro area summarizing using median() function.\nTable 7.6: Summarized median Hispanic population metro\ngrouping column (NAME_metro) output summarize() (median_hispanic) returned expected. However, group_by() %>% summarize() operations also return dataset simple features object geometry, case 4 rows. Let’s take look output geometry:\nFigure 7.8: Dissolved geometry Census tracts identified within Austin CBSA\nreturned geometry represents extent given metropolitan area (example, Austin-Round Rock). analytic process carried summarized data group, also summarized geometry group. typical name geometric process geographic information systems dissolve operation, geometries identified group combined return single larger geometry. case, Census tracts dissolved metropolitan area, returning metropolitan area geometries. type process extremely useful creating custom geographies (e.g. sales territories) Census geometry building blocks may belong group.","code":"\nhispanic_by_metro <- st_join(\n  pct_hispanic,\n  tx_cbsa,\n  join = st_within,\n  suffix = c(\"_tracts\", \"_metro\"),\n  left = FALSE\n) \nhispanic_by_metro %>%\n  mutate(NAME_metro = str_replace(NAME_metro, \", TX Metro Area\", \"\")) %>%\n  ggplot() + \n  geom_density(aes(x = estimate_tracts), color = \"navy\", fill = \"navy\", \n               alpha = 0.4) + \n  theme_minimal() + \n  facet_wrap(~NAME_metro) + \n  labs(title = \"Distribution of Hispanic/Latino population by Census tract\",\n       subtitle = \"Largest metropolitan areas in Texas\",\n       y = \"Kernel density estimate\",\n       x = \"Percent Hispanic/Latino in Census tract\")\nmedian_by_metro <- hispanic_by_metro %>%\n  group_by(NAME_metro) %>%\n  summarize(median_hispanic = median(estimate_tracts, na.rm = TRUE))\nplot(median_by_metro[1,]$geometry)"},{"path":"spatial-analysis-with-us-census-data.html","id":"small-area-time-series-analysis","chapter":"7 Spatial analysis with US Census data","heading":"7.3 Small area time-series analysis","text":"Previous chapters book covered techniques methods analyzing demographic change time US Census. Section 3.4 introduced ACS Comparison Profile along use iteration get multiple years data ACS Detailed Tables; Section 4.4 illustrated visualize time-series ACS data. techniques, however, typically appropriate larger geographies like counties rarely change shape time. contrast, smaller geographies like Census tracts block groups re-drawn US Census Bureau every decennial US Census, making time-series analysis smaller areas difficult.example, can compare Census tract boundaries fast-growing area Gilbert, Arizona (southeast Phoenix) 2010 2020.\nFigure 7.9: Comparison Census tracts Gilbert, AZ 2010 2020 Census\ndiscussed Section 5.3.3, US Census Bureau tries keep Census tract sizes relatively consistent around 4,000 people. tract grows large Census years, Census Bureau subdivide multiple Census tracts re-drawing tracts next decennial Census. example Arizona, tract shown divided five tracts 2020.partitioning Census tracts makes practical sense allows granular demographic analysis 2020, also makes time-series comparisons difficult. particularly important release 2016-2020 ACS, first ACS dataset use 2020 Census boundaries. common method resolving issue geographic information science areal interpolation. Areal interpolation refers allocation data one set zones second overlapping set zones may may perfectly align spatially. cases mis-alignment, type weighting scheme needs specified determine allocate partial data areas overlap. Two approaches interpolation outlined : area-weighted interpolation population-weighted interpolation.get started, let’s obtain comparison data Maricopa County, AZ number people working home 2011-2015 ACS (uses 2010 boundaries) 2016-2020 ACS (uses 2020 boundaries). use interpolation methods allocate 2011-2015 data 2020 Census tracts.","code":"\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(sf)\noptions(tigris_use_cache = TRUE)\n\n# CRS: NAD 83 / Arizona Central\nwfh_15 <- get_acs(\n  geography = \"tract\",\n  variables = \"B08006_017\",\n  year = 2015,\n  state = \"AZ\",\n  county = \"Maricopa\",\n  geometry = TRUE\n) %>%\n  select(estimate) %>%\n  st_transform(26949)\n\nwfh_20 <- get_acs(\n  geography = \"tract\",\n  variables = \"B08006_017\",\n  year = 2020,\n  state = \"AZ\",\n  county = \"Maricopa\",\n  geometry = TRUE\n ) %>%\n  st_transform(26949)"},{"path":"spatial-analysis-with-us-census-data.html","id":"area-weighted-areal-interpolation","chapter":"7 Spatial analysis with US Census data","heading":"7.3.1 Area-weighted areal interpolation","text":"Area-weighted areal interpolation implemented sf st_interpolate_aw() function. method uses area overlap geometries interpolation weights. technical standpoint, intersection computed origin geometries destination geometries. Weights computed proportion overall origin area comprised intersection. Area weights used estimate data 2020 geographies Census tract Gilbert illustrated .\nFigure 7.10: Illustration area weights\nweights applied target variables (case, information workers home) accordance value extensive argument. extensive = TRUE, used , weighted sums computed. Alternatively, extensive = FALSE, function returns weighted means.","code":"\nwfh_interpolate_aw <- st_interpolate_aw(\n  wfh_15,\n  wfh_20,\n  extensive = TRUE\n) %>%\n  mutate(GEOID = wfh_20$GEOID)"},{"path":"spatial-analysis-with-us-census-data.html","id":"population-weighted-areal-interpolation","chapter":"7 Spatial analysis with US Census data","heading":"7.3.2 Population-weighted areal interpolation","text":"user computes area-weighted areal interpolation st_interpolate_aw(), function prints following warning: st_interpolate_aw assumes attributes constant uniform areas x. assumption proportionally larger areas also proportionally people often incorrect respect geography human settlements, can source error using method. alternative method, population-weighted areal interpolation, can represent improvement. opposed using area-based weights, population-weighted techniques estimate populations intersections origin destination third dataset, use values interpolation weights.method implemented tidycensus interpolate_pw() function. function specified similar way st_interpolate_aw(), also requires third dataset used weights, optionally weight column determine relative influence feature weights dataset. many purposes, tidycensus users want use Census blocks weights dataset, though users can bring alternative datasets well. 2020 Census blocks acquired tigris package added benefit POP20 HU20 columns dataset represent population housing unit counts, respectively, either one used weight block.interpolate_pw() uses weighted centroid approach interpolation, input Census blocks first converted centroids, joined origin/destination intersections produce population weights. illustration process found ; map left-hand side shows block centroid weights, map right-hand side shows population weights used 2020 Census tract.\nFigure 7.11: Illustration block centroids population weights\npopulation-based weights differ significantly area-based weights Census tract Gilbert. Notably, southern-Census tract example area weight 0.167, whereas population weighting revealed 30 percent origin tract’s population actually located . leads substantive differences results area- population-weighted approaches, illustrated figure .\nFigure 7.12: Comparison area-weighted population-weighted interpolation results\narea-weighted method -estimates population geographically smaller tracts, -estimates larger ones; contrast, population-weighted method takes underlying population distribution account.","code":"\nmaricopa_blocks <- blocks(\n  state = \"AZ\",\n  county = \"Maricopa\",\n  year = 2020\n)\n\nwfh_interpolate_pw <- interpolate_pw(\n  wfh_15,\n  wfh_20,\n  to_id = \"GEOID\",\n  extensive = TRUE, \n  weights = maricopa_blocks,\n  weight_column = \"POP20\",\n  crs = 26949\n)"},{"path":"spatial-analysis-with-us-census-data.html","id":"making-small-area-comparisons","chapter":"7 Spatial analysis with US Census data","heading":"7.3.3 Making small-area comparisons","text":"methods interpolated 2011-2015 ACS estimates 2020 Census tracts, 2011-2015 2016-2020 ACS data can now compared consistent geographies. , join population-weighted interpolated 2011-2015 data original 2016-2020 data using left_join() (covered Section 6.4), taking care drop geometry dataset right-hand side join specify suffix argument distinguish two ACS estimates. calculate change time estimates map result.\nFigure 7.13: Map shift workers home, Maricopa County Arizona\nNotable increases tract-level working home found locations like Gilbert, Scottsdale, Tempe eastern side metropolitan area. said, results may simply function overall population growth tracts, means follow-analysis examine change share population working home. require interpolating total workforce denominator column calculating percentage. Fortunately, interpolation methods introduced section interpolate numeric columns input dataset, wide-form data data summary variable acquired tidycensus work well purpose.advantage using either area-weighted population-weighted areal interpolation covered section can implemented entirely data available tidycensus tigris. users may interested alternative weights using datasets included packages, like land use/land cover data, may want use sophisticated regression-based approaches. covered , Schroeder Van Riper (2013) provides good overview methods.discussed Section 3.5.1, derived margins error (even sums) require special methods. Given complexity interpolation methods covered , direct interpolation margin error columns take methods account. Analysts interpret columns caution.","code":"\nlibrary(mapboxapi)\n\nwfh_shift <- wfh_20 %>%\n  left_join(st_drop_geometry(wfh_interpolate_pw), \n            by = \"GEOID\",\n            suffix = c(\"_2020\", \"_2015\")) %>%\n  mutate(wfh_shift = estimate_2020 - estimate_2015)\n\nmaricopa_basemap <- layer_static_mapbox(\n  location = wfh_shift,\n  style_id = \"dark-v9\",\n  username = \"mapbox\"\n)\n\nggplot() + \n  maricopa_basemap + \n  geom_sf(data = wfh_shift, aes(fill = wfh_shift), color = NA, \n          alpha = 0.8) + \n  scale_fill_distiller(palette = \"PuOr\", direction = -1) + \n  labs(fill = \"Shift, 2011-2015 to\\n2016-2020 ACS\",\n       title = \"Change in work-from-home population\",\n       subtitle = \"Maricopa County, Arizona\") + \n  theme_void()"},{"path":"spatial-analysis-with-us-census-data.html","id":"distance-and-proximity-analysis","chapter":"7 Spatial analysis with US Census data","heading":"7.4 Distance and proximity analysis","text":"common use case spatially-referenced demographic data analysis accessibility. might include studying relative accessibility different demographic groups resources within given region, analyzing characteristics potential customers live within given distance store. Conceptually, variety ways measure accessibility. straightforward method, computationally, using straight-line (Euclidean) distances geographic data projected coordinate system. computationally complex - potentially accurate - method involves use transportation networks model accessibility, proximity measured based distance given location instead based travel times given transit mode, walking, cycling, driving. section illustrate types approaches. Let’s consider topic accessibility Level Level II trauma hospitals Census tract state Iowa. 2019 Census tract boundaries acquired tigris, use st_read() read shapefile hospital locations acquired US Department Homeland Security.","code":"\nlibrary(tigris)\nlibrary(sf)\nlibrary(tidyverse)\noptions(tigris_use_cache = TRUE)\n\n# CRS: NAD83 / Iowa North\nia_tracts <- tracts(\"IA\", cb = TRUE, year = 2019) %>%\n  st_transform(26975)\n\nhospital_url <- \"https://opendata.arcgis.com/api/v3/datasets/6ac5e325468c4cb9b905f1728d6fbf0f_0/downloads/data?format=geojson&spatialRefId=4326\"\n\ntrauma <- st_read(hospital_url) %>%\n  filter(str_detect(TRAUMA, \"LEVEL I\\\\b|LEVEL II\\\\b|RTH|RTC\")) %>%\n  st_transform(26975) %>%\n  distinct(ID, .keep_all = TRUE)## Reading layer `Hospitals' from data source \n##   `https://opendata.arcgis.com/api/v3/datasets/6ac5e325468c4cb9b905f1728d6fbf0f_0/downloads/data?format=geojson&spatialRefId=4326' \n##   using driver `GeoJSON'\n## Simple feature collection with 68775 features and 33 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -176.6403 ymin: -14.29024 xmax: 145.7245 ymax: 71.29773\n## Geodetic CRS:  WGS 84\nnames(trauma)##  [1] \"OBJECTID\"   \"ID\"         \"NAME\"       \"ADDRESS\"    \"CITY\"      \n##  [6] \"STATE\"      \"ZIP\"        \"ZIP4\"       \"TELEPHONE\"  \"TYPE\"      \n## [11] \"STATUS\"     \"POPULATION\" \"COUNTY\"     \"COUNTYFIPS\" \"COUNTRY\"   \n## [16] \"LATITUDE\"   \"LONGITUDE\"  \"NAICS_CODE\" \"NAICS_DESC\" \"SOURCE\"    \n## [21] \"SOURCEDATE\" \"VAL_METHOD\" \"VAL_DATE\"   \"WEBSITE\"    \"STATE_ID\"  \n## [26] \"ALT_NAME\"   \"ST_FIPS\"    \"OWNER\"      \"TTL_STAFF\"  \"BEDS\"      \n## [31] \"TRAUMA\"     \"HELIPAD\"    \"GlobalID\"   \"geometry\""},{"path":"spatial-analysis-with-us-census-data.html","id":"calculating-distances","chapter":"7 Spatial analysis with US Census data","heading":"7.4.1 Calculating distances","text":"determine accessibility Iowa Census tracts Level II trauma centers, need identify hospitals located Iowa, also states near Iowa border, Omaha, Nebraska Rock Island, Illinois. can accomplish applying distance threshold st_filter(). example, use spatial predicate st_is_within_distance, set 100km distance threshold dist = 100000 argument (specified meters, base measurement unit coordinate system used).\nFigure 7.14: Level II trauma centers within 100km Iowa\nillustrated visualization, st_filter() operation retained Level II trauma centers within state Iowa, also within 100km threshold beyond state’s borders.Census tract hospital data hand, can calculate distances Census tracts trauma centers using st_distance() function sf package. st_distance(x, y) default returns dense matrix distances computed geometries x geometries y. example, calculate distances centroids Iowa Census tracts (reflecting center points tract geometry) trauma center.glimpse matrix shows distances (meters) first five Census tracts dataset first five hospitals. considering accessibility, may interested distance nearest hospital Census tract. code extracts minimum distance matrix row, converts vector, divides value 1000 convert values kilometers. quick histogram visualizes distribution minimum distances.\nFigure 7.15: Base R histogram minimum distances trauma centers\ncode extracts minimum distances distance matrix includes notation may unfamiliar readers.apply() function base R used iterate rows matrix. Matrices data structure handled map_*() family functions tidyverse, base R methods must used. example pipeline, apply() function inherits dist matrix object first argument. second argument, 1, refers margin matrix apply() iterate ; 1 references rows (want), whereas 2 used columns. min function applied row, giving us minimum distance hospital Census tract.apply() function base R used iterate rows matrix. Matrices data structure handled map_*() family functions tidyverse, base R methods must used. example pipeline, apply() function inherits dist matrix object first argument. second argument, 1, refers margin matrix apply() iterate ; 1 references rows (want), whereas 2 used columns. min function applied row, giving us minimum distance hospital Census tract.divide_by() function magrittr package convenience arithmetic function used analytic pipelines R’s arithmetic operators (e.g. / division) won’t work way. example, divides values 1000 convert meters kilometers.divide_by() function magrittr package convenience arithmetic function used analytic pipelines R’s arithmetic operators (e.g. / division) won’t work way. example, divides values 1000 convert meters kilometers.many tracts within 10km trauma center, around 16 percent Iowa Census tracts 2019 beyond 100km Level II trauma center, suggesting significant accessibility issues areas.","code":"\nia_trauma <- trauma %>%\n  st_filter(ia_tracts, \n            .predicate = st_is_within_distance,\n            dist = 100000)\n\nggplot() + \n  geom_sf(data = ia_tracts, color = \"NA\", fill = \"grey50\") + \n  geom_sf(data = ia_trauma, color = \"red\") + \n  theme_void()\ndist <- ia_tracts %>%\n  st_centroid() %>%\n  st_distance(ia_trauma) \n\ndist[1:5, 1:5]## Units: [m]\n##           [,1]      [,2]     [,3]     [,4]     [,5]\n## [1,] 279570.18 279188.81 385140.7 383863.7 257745.5\n## [2,] 298851.01 298409.46 400428.5 399022.9 276955.8\n## [3,] 350121.53 347800.57 353428.8 350263.3 404616.0\n## [4,] 361742.20 360450.59 421691.6 419364.9 369415.7\n## [5,]  66762.19  63479.67 143552.1 143935.5 194001.0\nmin_dist <- dist %>%\n  apply(1, min) %>%\n  as.vector() %>%\n  magrittr::divide_by(1000)\n\nhist(min_dist)"},{"path":"spatial-analysis-with-us-census-data.html","id":"calculating-travel-times","chapter":"7 Spatial analysis with US Census data","heading":"7.4.2 Calculating travel times","text":"alternative way model accessibility hospitals travel times rather distance, way people experience access locations time expended given transportation network. network-based accessibility may accurate representation people’s lived experiences, computationally complex requires additional tools. perform spatial network analyses, R users either need obtain network data (like roadways) use appropriate tools can model network; set routing engine R can connect ; connect hosted routing engine via web API. example, ’ll use mapboxapi R package (K. Walker 2021c) perform network analysis using Mapbox’s travel-time Matrix API.function mb_matrix() mapboxapi works much like st_distance() requires arguments origins destinations, return dense matrix travel times default. turn, much computational complexity routing abstracted away function. However, routes computed across state Iowa API usage subject rate-limitations, function can take several minutes compute larger matrices like one.using mapboxapi first time, visit mapbox.com, register account, obtain access token. function mb_access_token() installs token .Renviron future use.glimpse travel-time matrix shows similar format distance matrix, travel times minutes used instead meters. distance-based example, can determine minimum travel time tract Level Level II trauma center. instance, visualize result map.\nFigure 7.16: Map travel-times trauma centers Census tract Iowa\nmap illustrates considerable accessibility gaps trauma centers across state. Whereas urban residents typically live within 20 minutes trauma center, travel times rural Iowa can exceed two hours.advantage using package like mapboxapi routing travel times users can connect directly hosted routing engine using API. Due rate limitations, however, web APIs likely inadequate advanced users need compute travel times scale. several R packages can connect user-hosted routing engines may better-suited tasks. packages include osrm Open Source Routing Machine; opentripplanner OpenTripPlanner; r5r R5.","code":"\nlibrary(mapboxapi)\n# mb_access_token(\"pk.eybcasq...\", install = TRUE)\n\ntimes <- mb_matrix(ia_tracts, ia_trauma)\ntimes[1:5, 1:5]##           [,1]      [,2]     [,3]     [,4]     [,5]\n## [1,] 211.43833 212.50667 278.5733 284.9717 212.1050\n## [2,] 218.15167 214.06000 280.1267 286.5250 226.7733\n## [3,] 274.84500 270.75333 291.8367 290.7117 292.2767\n## [4,] 274.58333 270.49167 291.5750 290.4500 292.0150\n## [5,]  56.80333  52.71167 122.3017 128.7000 161.2617\nmin_time <- apply(times, 1, min)\n\nia_tracts$time <- min_time\n\nggplot(ia_tracts, aes(fill = time)) + \n  geom_sf(color = NA) + \n  scale_fill_viridis_c(option = \"magma\") + \n  theme_void() + \n  labs(fill = \"Time (minutes)\",\n       title = \"Travel time to nearest Level I or Level II trauma hospital\",\n       subtitle = \"Census tracts in Iowa\",\n       caption = \"Data sources: US Census Bureau, US DHS, Mapbox\")"},{"path":"spatial-analysis-with-us-census-data.html","id":"catchment-areas-with-buffers-and-isochrones","chapter":"7 Spatial analysis with US Census data","heading":"7.4.3 Catchment areas with buffers and isochrones","text":"example considers broader accessibility analysis across state Iowa. many cases, however, ’ll want analyze accessibility local way. common use case might involve study demographic characteristics hospital catchment area, defined area around hospital patients likely come.matrix-based accessibility approach outlined , catchment area-based proximity can modeled either Euclidean distances network travel times well. Let’s consider example Iowa Methodist Medical Center Des Moines, one two Level trauma centers state Iowa.example illustrates distance-based approach using buffer, implemented st_buffer() function sf. buffer common GIS operation represents area within given distance location. code creates 5km buffer around Iowa Methodist Medical Center using argument dist = 5000.alternative option create network-based isochrones, polygons represent accessible area around given location within given travel time given travel mode. Isochrones implemented mapboxapi package mb_isochrone() function. Mapbox isochrones default traffic conditions associated date time function called; can adjusted depart_at parameter historical traffic. example draws 10-minute driving isochrone around Iowa Methodist Tuesday evening rush hour.can visualize comparative extents two methods Des Moines. Run code computer get synced interactive map showing two methods. makeAwesomeIcon() function leaflet creates custom icon appropriate medical facility; many icons available common points interest.\nFigure 7.17: Synced map showing buffer isochrone-based catchment areas Des Moines\ncomparative maps illustrate differences two methods quite clearly. Many areas equal distance hospital level access; particularly true areas south Raccoon/Des Moines River. Conversely, due location highways, areas outside 5km buffer area can reach hospital within 10 minutes.","code":"\niowa_methodist <- filter(ia_trauma, NAME == \"IOWA METHODIST MEDICAL CENTER\")\n\nbuf5km <- st_buffer(iowa_methodist, dist = 5000) \niso10min <- mb_isochrone(\n  iowa_methodist, \n  time = 10, \n  depart_at = \"2022-04-05T17:00\"\n  )\nlibrary(leaflet)\nlibrary(leafsync)\n\nhospital_icon <- makeAwesomeIcon(icon = \"ios-medical\", \n                                 markerColor = \"red\",\n                                 library = \"ion\")\n\n# The Leaflet package requires data be in CRS 4326\nmap1 <- leaflet() %>% \n  addTiles() %>%\n  addPolygons(data = st_transform(buf5km, 4326)) %>% \n  addAwesomeMarkers(data = st_transform(iowa_methodist, 4326),\n                    icon = hospital_icon)\n\nmap2 <- leaflet() %>% \n  addTiles() %>%\n  addPolygons(data = iso10min) %>% \n  addAwesomeMarkers(data = st_transform(iowa_methodist, 4326),\n                    icon = hospital_icon)\n\nsync(map1, map2)"},{"path":"spatial-analysis-with-us-census-data.html","id":"computing-demographic-estimates-for-zones-with-areal-interpolation","chapter":"7 Spatial analysis with US Census data","heading":"7.4.4 Computing demographic estimates for zones with areal interpolation","text":"Common methods, however, mis-alignment geometries Census geographies may use infer catchment area demographics. opposed spatial overlay analysis matching Census tracts metropolitan areas earlier chapter, Census tracts block groups edge catchment area partially included catchment. Areal interpolation methods like introduced Section 7.3 can used estimate demographics buffer zone isochrone.Let’s produce interpolated estimates percentage population poverty catchment area definitions. require obtaining block group-level poverty information ACS Polk County, Iowa, encompasses buffer isochrone. variables requested ACS include number family households incomes poverty line along total number family households serve denominator.can use population-weighted areal interpolation interpolate_pw() function tidycensus estimate family poverty buffer zone isochrone. Block weights Polk County obtained tigris, numerator denominator columns interpolated.\nTable 7.7: Comparison buffer isochrone catchment areas\ntwo methods return slightly different results, illustrating definition catchment area impacts downstream analyses.","code":"\npolk_poverty <- get_acs(\n  geography = \"block group\",\n  variables = c(poverty_denom = \"B17010_001\",\n                poverty_num = \"B17010_002\"),\n  state = \"IA\",\n  county = \"Polk\",\n  geometry = TRUE,\n  output = \"wide\",\n  year = 2020\n) %>%\n  select(poverty_denomE, poverty_numE) %>%\n  st_transform(26975)\nlibrary(glue)\n\npolk_blocks <- blocks(\n  state = \"IA\",\n  county = \"Polk\",\n  year = 2020\n)\n\nbuffer_pov <- interpolate_pw(\n  from = polk_poverty, \n  to = buf5km,\n  extensive = TRUE,\n  weights = polk_blocks,\n  weight_column = \"POP20\",\n  crs = 26975\n) %>%\n  mutate(pct_poverty = 100 * (poverty_numE / poverty_denomE))\n\niso_pov <- interpolate_pw(\n  from = polk_poverty, \n  to = iso10min,\n  extensive = TRUE,\n  weights = polk_blocks,\n  weight_column = \"POP20\",\n  crs = 26975\n) %>%\n  mutate(pct_poverty = 100 * (poverty_numE / poverty_denomE))"},{"path":"spatial-analysis-with-us-census-data.html","id":"better-cartography-with-spatial-overlay","chapter":"7 Spatial analysis with US Census data","heading":"7.5 Better cartography with spatial overlay","text":"discussed Section 6.1, one major benefits working tidycensus package get Census data R ability retrieve pre-joined feature geometry Census geographies argument geometry = TRUE. tidycensus uses tigris package fetch geometries, default Census Bureau’s cartographic boundary shapefiles. Cartographic boundary shapefiles preferred core TIGER/Line shapefiles tidycensus smaller size speeds processing pre-clipped US coastline.However, may circumstances mapping requires detail. good example maps New York City, even cartographic boundary shapefiles include water area. example, take example median household income Census tract Manhattan (New York County), NY:\nFigure 7.18: Map Manhattan default CB geometries\nillustrated graphic, boundaries Manhattan include water boundaries - stretching Hudson East Rivers. turn, accurate representation Manhattan’s land area might desired. accomplish , tidycensus user can use core TIGER/Line shapefiles instead, erase water area Manhattan’s geometry.","code":"\nlibrary(tidycensus)\nlibrary(tidyverse)\noptions(tigris_use_cache = TRUE)\n\nny <- get_acs(\n  geography = \"tract\", \n  variables = \"B19013_001\", \n  state = \"NY\", \n  county = \"New York\", \n  year = 2020,\n  geometry = TRUE\n)\n\nggplot(ny) + \n  geom_sf(aes(fill = estimate)) + \n  scale_fill_viridis_c(labels = scales::label_dollar()) + \n  theme_void() + \n  labs(fill = \"Median household\\nincome\")"},{"path":"spatial-analysis-with-us-census-data.html","id":"erasing-areas-from-census-polygons","chapter":"7 Spatial analysis with US Census data","heading":"7.5.1 “Erasing” areas from Census polygons","text":"tidycensus allows users get TIGER/Line instead cartographic boundary shapefiles keyword argument cb = FALSE. argument familiar users tigris package, used tigris distinguish cartographic boundary TIGER/Line shapefiles package.Next, erase_water() function tigris package used remove water area Census tracts. erase_water() works auto-detecting US counties surround input dataset, obtaining area water shapefile Census Bureau counties, computing erase operation remove water areas input dataset. Using TIGER/Line geometries cb = FALSE recommended align input water areas minimize creation sliver polygons, small polygons can created overlay inconsistent spatial datasets.Although used , erase_water() optional argument, area_threshold, defines area percentile threshold water areas kept erase operation. default 0.75, used , erases water areas size percentile 75 percent (, top 25 percent). lower area threshold can produce accurate shapes, can slow operation substantially.erasing water area Manhattan’s Census tracts erase_water(), can map result:\nFigure 7.19: Map Manhattan water areas erased\nmap appears , now familiar representation extent Manhattan.","code":"\nny2 <- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\", \n  state = \"NY\", \n  county = \"New York\", \n  geometry = TRUE, \n  year = 2020,\n  cb = FALSE\n) %>%\n  st_transform(6538)\nny_erase <- erase_water(ny2)\nggplot(ny_erase) + \n  geom_sf(aes(fill = estimate)) + \n  scale_fill_viridis_c(labels = scales::label_dollar()) + \n  theme_void() + \n  labs(fill = \"Median household\\nincome\")"},{"path":"spatial-analysis-with-us-census-data.html","id":"spatial-neighborhoods-and-spatial-weights-matrices","chapter":"7 Spatial analysis with US Census data","heading":"7.6 Spatial neighborhoods and spatial weights matrices","text":"spatial capabilities tidycensus also allow exploratory spatial data analysis (ESDA) within R. ESDA refers use datasets’ spatial properties addition attributes explore patterns relationships. may involve exploration spatial patterns datasets identification spatial clustering given demographic attribute.illustrate analyst can apply ESDA Census data, let’s acquire dataset median age Census tract Dallas-Fort Worth, TX metropolitan area. Census tracts metro area identified using methods introduced earlier chapter.","code":"\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(sf)\nlibrary(spdep)\noptions(tigris_use_cache = TRUE)\n\n# CRS: NAD83 / Texas North Central\ndfw <- core_based_statistical_areas(cb = TRUE, year = 2020) %>%\n  filter(str_detect(NAME, \"Dallas\")) %>%\n  st_transform(32138)\n\ndfw_tracts <- get_acs(\n  geography = \"tract\",\n  variables = \"B01002_001\",\n  state = \"TX\",\n  year = 2020,\n  geometry = TRUE\n) %>%\n  st_transform(32138) %>%\n  st_filter(dfw, .predicate = st_within) %>%\n  na.omit()\n\nggplot(dfw_tracts) + \n  geom_sf(aes(fill = estimate), color = NA) + \n  scale_fill_viridis_c() + \n  theme_void()"},{"path":"spatial-analysis-with-us-census-data.html","id":"understanding-spatial-neighborhoods","chapter":"7 Spatial analysis with US Census data","heading":"7.6.1 Understanding spatial neighborhoods","text":"Exploratory spatial data analysis relies concept neighborhood, representation given geographic feature (e.g. given point, line, polygon) interrelates features nearby. workhorse package exploratory spatial data analysis R spdep, includes wide range tools exploring modeling spatial data. part framework, spdep supports variety neighborhood definitions. definitions include:Proximity-based neighbors, neighboring features identified based measure distance. Neighbors might defined fall within given distance threshold (e.g. features within 2km given feature) k-nearest neighbors (e.g. nearest eight features given feature).Graph-based neighbors, neighbors defined network relationships (e.g. along street network).Contiguity-based neighbors, used geographic features polygons. Options contiguity-based spatial relationships include queen’s case neighbors, polygons share least one vertex considered neighbors; rook’s case neighbors, polygons must share least one line segment considered neighbors.example, ’ll choose queen’s case contiguity-based neighborhood definition Census tracts. implement function poly2nb(), can take sf object argument produce neighbors list object. use argument queen = TRUE request queen’s case neighbors explicitly (though function default).average, Census tracts Dallas-Fort Worth metropolitan area 6.43 neighbors. minimum number neighbors dataset 2 (eight tracts), maximum number neighbors 17 (tract row index 1635). important caveat keep mind tracts neighbors may actually neighbors listed given restricted tract dataset tracts within Dallas-Fort Worth metropolitan area. turn, analysis influenced edge effects neighborhoods edge metropolitan area artificially restricted.Neighborhood relationships can visualized using plotting functionality spdep, blue lines connecting polygon neighbors.\nFigure 7.20: Visualization queens-case neighborhood relationships\nAdditionally, row indices neighbors given feature can readily extracted neighbors list object.","code":"\nneighbors <- poly2nb(dfw_tracts, queen = TRUE)\n\nsummary(neighbors)## Neighbour list object:\n## Number of regions: 1699 \n## Number of nonzero links: 10930 \n## Percentage nonzero weights: 0.378646 \n## Average number of links: 6.433196 \n## Link number distribution:\n## \n##   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17 \n##   8  50 173 307 395 343 220 109  46  28  11   5   2   1   1 \n## 8 least connected regions:\n## 33 620 697 753 1014 1358 1579 1642 with 2 links\n## 1 most connected region:\n## 1635 with 17 links\ndfw_coords <- dfw_tracts %>%\n  st_centroid() %>%\n  st_coordinates()\n\nplot(dfw_tracts$geometry)\nplot(neighbors, \n     coords = dfw_coords, \n     add = TRUE, \n     col = \"blue\", \n     points = FALSE)\n# Get the row indices of the neighbors of the Census tract at row index 1\nneighbors[[1]]## [1]   45  585  674 1152 1580"},{"path":"spatial-analysis-with-us-census-data.html","id":"generating-the-spatial-weights-matrix","chapter":"7 Spatial analysis with US Census data","heading":"7.6.2 Generating the spatial weights matrix","text":"perform exploratory spatial data analysis, can convert neighbors list object spatial weights. Spatial weights define metrics associated feature’s neighbors weighted. Weight generation implemented nb2listw() function, pass neighbors object specify style weights. default, style = \"W\", produces row-standardized weights object weights neighbors given feature sum 1. option choose analyzing neighborhood means. alternative option, style = \"B\", produces binary weights neighbors given weight 1 non-neighbors take weight 0. style weights useful producing neighborhood sums.example , create row-standardized spatial weights Dallas-Fort Worth Census tracts check values feature row index 1.Given Census tract row index 1 five neighbors, neighbor assigned weight 0.2.","code":"\nweights <- nb2listw(neighbors, style = \"W\")\n\nweights$weights[[1]]## [1] 0.2 0.2 0.2 0.2 0.2"},{"path":"spatial-analysis-with-us-census-data.html","id":"global-and-local-spatial-autocorrelation","chapter":"7 Spatial analysis with US Census data","heading":"7.7 Global and local spatial autocorrelation","text":"row-standardized spatial weights object named weights provides needed information perform exploratory spatial data analysis median age Dallas-Fort Worth metropolitan area. many cases, analyst may interested understanding attributes geographic features relate neighbors. Formally, concept called spatial autocorrelation. concept spatial autocorrelation relates Waldo Tobler’s famous “first law geography,” reads (Tobler 1970):Everything related everything else, near things related distant things.formulation informs much theory behind spatial data science geographical inquiry broadly. respect exploratory spatial analysis Census data, might interested degree given Census variable clusters spatially, subsequently clusters found. One way assess clustering assess degree ACS estimates similar differ neighbors defined weights matrix. Patterns can turn explained follows:Spatial clustering: data values tend similar neighboring data values;Spatial uniformity: data values tend differ neighboring data values;Spatial randomness: apparent relationship data values neighbors.Given Tobler’s first law geography, tend expect geographic phenomena exhibit degree spatial clustering. section introduces variety methods available R evaluate spatial clustering using ESDA spdep package (R. S. Bivand Wong 2018).","code":""},{"path":"spatial-analysis-with-us-census-data.html","id":"spatial-lags-and-morans-i","chapter":"7 Spatial analysis with US Census data","heading":"7.7.1 Spatial lags and Moran’s I","text":"Spatial weights matrices can used calculate spatial lag given attribute observation dataset. spatial lag refers neighboring values observation given spatial weights matrix. discussed , row-standardized weights matrices produce lagged means, binary weights matrices produce lagged sums. Spatial lag calculations implemented function lag.listw(), requires spatial weights list object numeric vector compute lag.code creates new column dfw_tracts, lag_estimate, represents average median age neighbors Census tract Dallas-Fort Worth metropolitan area. Using information, can draw scatterplot ACS estimate vs. lagged mean preliminary assessment spatial clustering data.\nFigure 7.21: Scatterplot median age relative spatial lag\nscatterplot suggests positive correlation ACS estimate spatial lag, representative spatial autocorrelation data. relationship can evaluated using test global spatial autocorrelation. common method used spatial autocorrelation evaluation Moran’s \\(\\), can interpreted similar correlation coefficient relationship observations neighbors. statistic computed :\\[\n= \\frac{N}{W}\\frac{\\sum_i\\sum_j w_{ij}(x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_i(x_i - \\bar{x})^2}\n\\]\\(w_{ij}\\) represents spatial weights matrix, \\(N\\) number spatial units denoted \\(\\) \\(j\\), \\(W\\) sum spatial weights.Moran’s \\(\\) implemented spdep moran.test() function, requires numeric vector spatial weights list object.Moran’s \\(\\) statistic 0.292 positive, small p-value suggests reject null hypothesis spatial randomness dataset. (See Section 8.2.4 additional discussion p-values). statistic positive, suggests data spatially clustered; negative statistic suggest spatial uniformity. practical sense, means Census tracts older populations tend located near one another, Census tracts younger populations also tend found areas.","code":"\ndfw_tracts$lag_estimate <- lag.listw(weights, dfw_tracts$estimate)\nggplot(dfw_tracts, aes(x = estimate, y = lag_estimate)) + \n  geom_point(alpha = 0.3) + \n  geom_abline(color = \"red\") + \n  theme_minimal() + \n  labs(title = \"Median age by Census tract, Dallas-Fort Worth TX\",\n       x = \"Median age\",\n       y = \"Spatial lag, median age\", \n       caption = \"Data source: 2016-2020 ACS via the tidycensus R package.\\nSpatial relationships based on queens-case polygon contiguity.\")\nmoran.test(dfw_tracts$estimate, weights)## \n##  Moran I test under randomisation\n## \n## data:  dfw_tracts$estimate  \n## weights: weights    \n## \n## Moran I statistic standard deviate = 21.264, p-value < 2.2e-16\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##      0.2924898552     -0.0005889282      0.0001899598"},{"path":"spatial-analysis-with-us-census-data.html","id":"local-spatial-autocorrelation","chapter":"7 Spatial analysis with US Census data","heading":"7.7.2 Local spatial autocorrelation","text":"can explore local spatial autocorrelation analysis. Local measures spatial autocorrelation disaggregate global results identify “hot spots” similar values within given spatial dataset. One example Getis-Ord local G statistic (Getis Ord 1992), computed follows:\\[\nG_{} = \\dfrac{\\sum\\limits_{j}w_{ij}x_j}{\\sum\\limits_{j=1}^{n}x_j} \\text{ }\\neq j\n\\]summary, equation computes ratio weighted average neighborhood values total sum values dataset. default version local G (represented equation ) omits location \\(\\) calculation, variant local G statistic, \\(G_i*\\), includes location. Results returned z-scores, implemented localG() function spdep.code calculates local G variant \\(G_i*\\) re-generating weights matrix include.self(), passing weights matrix localG() function.\nFigure 7.22: Map local Gi* scores\nGiven returned results z-scores, analyst can choose hot spot thresholds statistic, calculate case_when(), plot accordingly.\nFigure 7.23: Map local Gi* scores significant clusters highlighted\nred areas resulting map representative “high” clustering median age, neighborhoods older populations surrounded older-age neighborhoods. “Low” clusters represented blue, reflect clustering Census tracts comparatively youthful populations.","code":"\n# For Gi*, re-compute the weights with `include.self()`\nlocalg_weights <- nb2listw(include.self(neighbors))\n\ndfw_tracts$localG <- localG(dfw_tracts$estimate, localg_weights)\n\nggplot(dfw_tracts) + \n  geom_sf(aes(fill = localG), color = NA) + \n  scale_fill_distiller(palette = \"RdYlBu\") + \n  theme_void() + \n  labs(fill = \"Local Gi* statistic\")\ndfw_tracts <- dfw_tracts %>%\n  mutate(hotspot = case_when(\n    localG >= 2.56 ~ \"High cluster\",\n    localG <= -2.56 ~ \"Low cluster\",\n    TRUE ~ \"Not significant\"\n  ))\n\nggplot(dfw_tracts) + \n  geom_sf(aes(fill = hotspot), color = \"grey90\", size = 0.1) + \n  scale_fill_manual(values = c(\"red\", \"blue\", \"grey\")) + \n  theme_void()"},{"path":"spatial-analysis-with-us-census-data.html","id":"identifying-clusters-and-spatial-outliers-with-local-indicators-of-spatial-association-lisa","chapter":"7 Spatial analysis with US Census data","heading":"7.7.3 Identifying clusters and spatial outliers with local indicators of spatial association (LISA)","text":"alternative method calculation local spatial autocorrelation local indicators spatial association statistic, commonly referred LISA local form Moran’s \\(\\) (Anselin 1995). extension Global Moran’s \\(\\) statistic, local statistic \\(I_i\\) given local feature \\(\\) neighbors \\(j\\) computed follows:\\[\nI_i = z_i \\sum_j w_{ij} z_j,\n\\]\\(z_i\\) \\(z_j\\) expressed deviations mean.LISA popular method exploratory spatial data analysis spatial social sciences implemented variety software packages. ArcGIS implements LISA Cluster Outlier Analysis geoprocessing tool; Anselin’s open-source GeoDa software graphical interface calculating LISA statistics; Python users can compute LISA using PySAL library.R, LISA can computed using localmoran() family functions spdep package. users familiar using LISA software packages, localmoran_perm() function implements LISA statistical significance calculated based conditional permutation-based approach.example calculates local Moran’s \\(\\) statistics way resembles output GeoDa, returns cluster map Moran scatterplot. One major benefits using LISA exploratory analysis ability identify spatial clusters, observations surrounded similar values, spatial outliers, observations surrounded dissimilar values. ’ll use method explore clustering possible presence spatial outliers respect Census tract median age Dallas-Fort Worth.code uses following steps:First, random number seed set given using conditional permutation approach calculating statistical significance. ensure reproducibility results process re-run.ACS estimate median age converted z-score using scale(), subtracts mean estimate divides standard deviation. follows convention GeoDa.LISA computed localmoran_perm() scaled value median age, using contiguity-based spatial weights matrix. 999 conditional permutation simulations used calculate statistical significance, argument alternative = \"two.sided\" identify statistically significant clusters statistically significant spatial outliers.LISA data frame attached Census tract shapes computing lagged value median age.result appears follows:\nTable 7.8: Local Moran’s results\ninformation returned localmoran_perm() can used compute GeoDa-style LISA quadrant plot well cluster map. LISA quadrant plot similar Moran scatterplot, also identifies “quadrants” observations respect spatial relationships identified LISA. code uses case_when() recode data appropriate categories LISA quadrant plot, using significance level p = 0.05.LISA quadrant plot appears follow:\nFigure 7.24: LISA quadrant scatterplot\nObservations falling top-right quadrant represent “high-high” clusters, Census tracts higher median ages also surrounded Census tracts older populations. Statistically significant clusters - p-value less equal 0.05 - colored red chart. bottom-left quadrant also represents spatial clusters, instead includes lower-median-age tracts also surrounded tracts similarly low median ages. top-left bottom-right quadrants home spatial outliers, values dissimilar neighbors.GeoDa also implements “cluster map” observations visualized relationship cluster membership statistical significance. code reproduces GeoDa cluster map using ggplot2 geom_sf().\nFigure 7.25: LISA cluster map\nmap illustrates distinctive patterns spatial clustering age Dallas-Fort Worth region. Older clusters colored red; includes areas like wealthy Highland Park community north downtown Dallas. Younger clusters colored dark blue, found areas like east Fort Worth, east Dallas, Arlington center metropolitan area. Spatial outliers appear scattered throughout map well; Dallas area, low-high clusters Census tracts large quantities multifamily housing adjacent predominantly single-family neighborhoods.One useful feature GeoDa exploratory spatial data analysis ability perform linked brushing LISA quadrant plot cluster map. allows users click drag either plot highlight corresponding observations plot. Building chart linking example using ggiraph introduced Section 6.6.2, linked brushing approach similar GeoDa can implemented Shiny, represented image available https://walkerke.shinyapps.io/linked-brushing/.\nFigure 7.26: View LISA Shiny app linked brushing enabled\nUsing lasso select tool, can click drag either scatterplot map view corresponding observations highlighted chart panel. Code reproduce Shiny app available scripts/linked_brushing book’s GitHub repository.","code":"\nset.seed(1983)\n\ndfw_tracts$scaled_estimate <- as.numeric(scale(dfw_tracts$estimate))\n\ndfw_lisa <- localmoran_perm(\n  dfw_tracts$scaled_estimate, \n  weights, \n  nsim = 999L, \n  alternative = \"two.sided\"\n) %>%\n  as_tibble() %>%\n  set_names(c(\"local_i\", \"exp_i\", \"var_i\", \"z_i\", \"p_i\",\n              \"p_i_sim\", \"pi_sim_folded\", \"skewness\", \"kurtosis\"))\n\ndfw_lisa_df <- dfw_tracts %>%\n  select(GEOID, scaled_estimate) %>%\n  mutate(lagged_estimate = lag.listw(weights, scaled_estimate)) %>%\n  bind_cols(dfw_lisa)\ndfw_lisa_clusters <- dfw_lisa_df %>%\n  mutate(lisa_cluster = case_when(\n    p_i >= 0.05 ~ \"Not significant\",\n    scaled_estimate > 0 & local_i > 0 ~ \"High-high\",\n    scaled_estimate > 0 & local_i < 0 ~ \"High-low\",\n    scaled_estimate < 0 & local_i > 0 ~ \"Low-low\",\n    scaled_estimate < 0 & local_i < 0 ~ \"Low-high\"\n  ))\ncolor_values <- c(`High-high` = \"red\", \n                  `High-low` = \"pink\", \n                  `Low-low` = \"blue\", \n                  `Low-high` = \"lightblue\", \n                  `Not significant` = \"white\")\n\nggplot(dfw_lisa_clusters, aes(x = scaled_estimate, \n                              y = lagged_estimate,\n                              fill = lisa_cluster)) + \n  geom_point(color = \"black\", shape = 21, size = 2) + \n  theme_minimal() + \n  geom_hline(yintercept = 0, linetype = \"dashed\") + \n  geom_vline(xintercept = 0, linetype = \"dashed\") + \n  scale_fill_manual(values = color_values) + \n  labs(x = \"Median age (z-score)\",\n       y = \"Spatial lag of median age (z-score)\",\n       fill = \"Cluster type\")\nggplot(dfw_lisa_clusters, aes(fill = lisa_cluster)) + \n  geom_sf(size = 0.1) + \n  theme_void() + \n  scale_fill_manual(values = color_values) + \n  labs(fill = \"Cluster type\")"},{"path":"spatial-analysis-with-us-census-data.html","id":"exercises-5","chapter":"7 Spatial analysis with US Census data","heading":"7.8 Exercises","text":"Identify different core-based statistical area interest use methods introduced chapter extract Census tracts block groups CBSA.Replicate erase_water() cartographic workflow different county significant water area; good choice King County, Washington. sure transform data appropriate projected coordinate system (selected suggest_crs()) first. operation slow, try re-running higher area threshold seeing get back.Acquire spatial dataset tidycensus region interest variable interest . Follow instructions chapter generate spatial weights matrix, compute hot-spot analysis localG().","code":""},{"path":"modeling-us-census-data.html","id":"modeling-us-census-data","chapter":"8 Modeling US Census data","heading":"8 Modeling US Census data","text":"previous chapter included range examples illustrating methods analyzing exploring spatial datasets. Census data can also used derive models explaining patterns occur across regions within cities. models draw concepts introduced prior chapters, can also used part explanatory frameworks within broader analytic pipelines statistical inference machine learning. chapter introduces series frameworks. first section looks segregation diversity indices widely used across social sciences explain demographic patterns. second section explores topics statistical modeling, including methods spatial regression take account spatial autocorrelation inherent Census variables. third final section explores concepts classification, clustering, regionalization common unsupervised supervised machine learning. Examples illustrate use Census data generate neighborhood typologies, widely used business marketing applications, generate spatially coherent sales territories Census data regionalization.","code":""},{"path":"modeling-us-census-data.html","id":"indices-of-segregation-and-diversity","chapter":"8 Modeling US Census data","heading":"8.1 Indices of segregation and diversity","text":"large body research social sciences concerned neighborhood segregation diversity. Segregation addressed generally refers measurement extent two groups live apart ; diversity companion metric measures neighborhood heterogeneity among groups. wide range indices developed social scientists measure segregation diversity, many cases inherently linked spatial Census data often best way measure concepts. Segregation diversity indices implemented variety different R packages; package recommended book segregation package (Elbers 2021), includes R functions variety regional local indices.","code":""},{"path":"modeling-us-census-data.html","id":"data-setup-with-spatial-analysis","chapter":"8 Modeling US Census data","heading":"8.1.1 Data setup with spatial analysis","text":"Much segregation diversity literature focuses race ethnicity, explored example . data setup code uses spatial methods covered previous three chapters acquire Census tract-level data population estimates non-Hispanic white, non-Hispanic black, non-Hispanic Asian, Hispanic populations California, filters Census tracts intersect largest urbanized areas population state using inner spatial join. turn, illustrative example spatial analysis tools can important parts data setup workflows analysis. urbanized areas 2020 Census yet defined time writing, ’ll using urbanized areas 2019 data 2015-2019 ACS.summarize, spatial analysis workflow detailed uses following steps:Data race & ethnicity 2015-2019 5-year ACS four largest demographic groups California acquired tidycensus’s get_acs() Census tract level feature geometry included. Depending goals study, racial/ethnic groups (e.g. native American, native Hawaiian/Pacific Islander) added removed needed.urban areas defined Census Bureau often cross state boundaries, urban areas must obtained entire US get_acs(). obtained, urban areas filtered areas populations 750,000 greater, transmute() used retain new column representing area name (along simple feature geometry column).spatial join Census tract data urban area data computed st_join(). argument left = FALSE computes inner spatial join, retains Census tracts intersect urban area boundaries, appends corresponding urban_name column Census tract.data structure appears follows:\nTable 8.1: Prepared data segregation analysis\ndata long (tidy) form, default used tidycensus; data structure ideal computing indices segregation package.","code":"\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(segregation)\nlibrary(tigris)\nlibrary(sf)\n\n# Get California tract data by race/ethnicity\nca_acs_data <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    white = \"B03002_003\",\n    black = \"B03002_004\",\n    asian = \"B03002_006\",\n    hispanic = \"B03002_012\"\n  ), \n  state = \"CA\",\n  geometry = TRUE,\n  year = 2019\n) \n\n# Use tidycensus to get urbanized areas by population with geometry, \n# then filter for those that have populations of 750,000 or more\nus_urban_areas <- get_acs(\n  geography = \"urban area\",\n  variables = \"B01001_001\",\n  geometry = TRUE,\n  year = 2019,\n  survey = \"acs1\"\n) %>%\n  filter(estimate >= 750000) %>%\n  transmute(urban_name = str_remove(NAME, \n                                    fixed(\", CA Urbanized Area (2010)\")))\n\n# Compute an inner spatial join between the California tracts and the \n# urbanized areas, returning tracts in the largest California urban \n# areas with the urban_name column appended\nca_urban_data <- ca_acs_data %>%\n  st_join(us_urban_areas, left = FALSE) %>%\n  select(-NAME) %>%\n  st_drop_geometry()"},{"path":"modeling-us-census-data.html","id":"the-dissimilarity-index","chapter":"8 Modeling US Census data","heading":"8.1.2 The dissimilarity index","text":"dissimilarity index widely used assess neighborhood segregation two groups within region. computed follows:\\[\nD = \\frac{1}{2}\\sum\\limits_{=1}^{N}\\left |\\frac{a_i}{}-\\frac{b_i}{B}  \\right |\n\\]\\(a_i\\) represents population group \\(\\) given areal unit \\(\\); \\(\\) total population group study region (e.g. metropolitan area); \\(b_i\\) \\(B\\) equivalent metrics second group. index ranges low 0 high 1, 0 represents perfect integration two groups 1 represents complete segregation. index implemented segregation package dissimilarity() function.example computes dissimilarity index non-Hispanic white Hispanic populations San Francisco/Oakland urbanized area. data filtered rows represent target populations San Francisco/Oakland area, piped dissimilarity() function. function requires identification group column, ’ll use variable; unit column representing neighborhood unit, ’ll use GEOID represent Census tract; weight column tells function many people group.\\(D\\) index segregation non-Hispanic white Hispanic populations San Francisco-Oakland area 0.51. statistic, however, meaningful comparison cities. compute dissimilarity urban area, can creatively apply tidyverse techniques covered earlier chapters introduce new function, group_modify(), group-wise calculation. example follows recommended workflow segregation package documentation. code filters data non-Hispanic white Hispanic populations Census tract, groups dataset values urban_name column. group_modify() function dplyr allows calculation dissimilarity indices group, example Census tracts within respective urban area. returns combined dataset sorted descending order arrange() make comparisons.\nTable 8.2: Dissimilarity indices Hispanic non-Hispanic white populations, large California urbanized areas\nLos Angeles area segregated large urbanized areas California respect non-Hispanic white Hispanic populations Census tract level, followed San Francisco/Oakland. Riverside/San Bernardino Sacramento least segregated large urban areas state.","code":"\nca_urban_data %>%\n  filter(variable %in% c(\"white\", \"hispanic\"),\n         urban_name == \"San Francisco--Oakland\") %>%\n  dissimilarity(\n    group = \"variable\",\n    unit = \"GEOID\",\n    weight = \"estimate\"\n  )##    stat       est\n## 1:    D 0.5135526\nca_urban_data %>%\n  filter(variable %in% c(\"white\", \"hispanic\")) %>%\n  group_by(urban_name) %>%\n  group_modify(~\n    dissimilarity(.x,\n      group = \"variable\",\n      unit = \"GEOID\",\n      weight = \"estimate\"\n    )\n  ) %>% \n  arrange(desc(est))"},{"path":"modeling-us-census-data.html","id":"multi-group-segregation-indices","chapter":"8 Modeling US Census data","heading":"8.1.3 Multi-group segregation indices","text":"One disadvantage dissimilarity index measures segregation two groups. state diverse California, may interested measuring segregation diversity multiple groups time. segregation package implements two indices: Mutual Information Index \\(M\\), Theil’s Entropy Index \\(H\\) (Mora Ruiz-Castillo 2011). Following Elbers (2021), \\(M\\) computed follows dataset \\(T\\):\\[\nM(\\mathbf{T})=\\sum_{u=1}^U\\sum_{g=1}^Gp_{ug}\\log\\frac{p_{ug}}{p_{u}p_{g}}\n\\]\\(U\\) total number units \\(u\\), \\(G\\) total number groups \\(g\\), \\(p_{ug}\\) joint probability unit \\(u\\) group \\(g\\), \\(p_u\\) \\(p_g\\) referring unit group probabilities. Theil’s \\(H\\) dataset \\(T\\) can written :\\[\nH(\\mathbf{T})=\\frac{M(\\mathbf{T})}{E(\\mathbf{T})}\n\\]\\(E(T)\\) entropy \\(T\\), normalizing \\(H\\) range values 0 1.Computing indices straightforward segregation package. mutual_total() function computes indices; different regions considered (like multiple urban areas, example) mutual_within() function compute \\(M\\) \\(H\\) urban area within argument appropriately specified. ’ll using full ca_urban_data dataset, includes population estimates non-Hispanic white, non-Hispanic Black, non-Hispanic Asian, Hispanic populations.\nTable 8.3: Multi-group segregation results California urban areas\nmulti-group segregation considered using indices, Los Angeles remains segregated urban area, whereas Riverside/San Bernardino least segregated.segregation package also offers function local segregation analysis, mutual_local(), decomposes \\(M\\) unit-level segregation scores, represented ls. example , use mutual_local() examine patterns segregation across segregated urban area, Los Angeles.results can mapped joining data dataset Census tracts tigris; inner_join() function used retain tracts Los Angeles area .\nFigure 8.1: Map local multi-group segregation scores Los Angeles\n","code":"\nmutual_within(\n  data = ca_urban_data,\n  group = \"variable\",\n  unit = \"GEOID\",\n  weight = \"estimate\",\n  within = \"urban_name\",\n  wide = TRUE\n)\nla_local_seg <- ca_urban_data %>%\n  filter(urban_name == \"Los Angeles--Long Beach--Anaheim\") %>%\n  mutual_local(\n    group = \"variable\",\n    unit = \"GEOID\",\n    weight = \"estimate\", \n    wide = TRUE\n  )\nla_tracts_seg <- tracts(\"CA\", cb = TRUE) %>%\n  inner_join(la_local_seg, by = \"GEOID\") \n\nla_tracts_seg %>%\n  ggplot(aes(fill = ls)) + \n  geom_sf(color = NA) + \n  coord_sf(crs = 26946) + \n  scale_fill_viridis_c(option = \"inferno\") + \n  theme_void() + \n  labs(fill = \"Local\\nsegregation index\")"},{"path":"modeling-us-census-data.html","id":"visualizing-the-diversity-gradient","chapter":"8 Modeling US Census data","heading":"8.1.4 Visualizing the diversity gradient","text":"diversity gradient concept uses scatterplot smoothing visualize neighborhood diversity varies distance travel-time core urban region (K. Walker 2016b). Historically, literature suburbanization social sciences assumes heterogeneous urban core relative segregated homogeneous suburban neighborhoods. diversity gradient visual heuristic used evaluate validity demographic model.entropy index given geographic unit calculated follows:\\[\nE = \\sum\\limits_{r=1}^{n}Q_rln\\frac{1}{Q_r}\n\\]\\(Q_r\\) calculation represents group \\(r\\)’s proportion population geographic unit.statistic implemented entropy() function segregation package. entropy() function calculates statistic specific unit time, group data tract, use group_modify() calculate entropy tract separately. argument base = 4 set convention number groups calculation; sets maximum value statistic 1, represents perfect evenness four groups area. computed, indices joined dataset Census tracts California; inner_join() used retain tracts Los Angeles urbanized area.Visualization diversity gradient requires relative measurement far Census tract urban core. travel-time methods available mapboxapi package introduced Chapter 7 used calculate driving distance Los Angeles City Hall Census tracts Los Angeles urbanized area.computed, travel times stored vector minutes_to_downtown, assigned new column minutes entropy data frame. tract diversity index visualized using ggplot2 relative travel time downtown Los Angeles, LOESS smoother superimposed scatterplot represent diversity gradient.\nFigure 8.2: Diversity gradient visualization Los Angeles, CA urbanized area\nvisualization diversity gradient shows neighborhood diversity increases driving time urban core Los Angeles, peaking 35 minutes free-flowing traffic urban core leveling . structure diversity gradient suggests Census tracts near downtown tend segregated, suburban tracts likely integrated.","code":"\nla_entropy <- ca_urban_data %>%\n  filter(urban_name == \"Los Angeles--Long Beach--Anaheim\") %>%\n  group_by(GEOID) %>%\n  group_modify(~data.frame(entropy = entropy(\n      data = .x,\n      group = \"variable\",\n      weight = \"estimate\",\n      base = 4)))\n\nla_entropy_geo <- tracts(\"CA\", cb = TRUE, year = 2019) %>%\n  inner_join(la_entropy, by = \"GEOID\")\nlibrary(mapboxapi)\n\nla_city_hall <- mb_geocode(\"City Hall, Los Angeles CA\")\n\nminutes_to_downtown <- mb_matrix(la_entropy_geo, la_city_hall)\nla_entropy_geo$minutes <- as.numeric(minutes_to_downtown)\n\nggplot(la_entropy_geo, aes(x = minutes_to_downtown, y = entropy)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"loess\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 80)) + \n  labs(title = \"Diversity gradient, Los Angeles urbanized area\",\n       x = \"Travel-time to downtown Los Angeles in minutes, Census tracts\",\n       y = \"Entropy index\")"},{"path":"modeling-us-census-data.html","id":"regression-modeling-with-us-census-data","chapter":"8 Modeling US Census data","heading":"8.2 Regression modeling with US Census data","text":"Regression modeling widely used industry social sciences understand social processes. social sciences, goal regression modeling commonly understand relationships variable study, termed outcome variable, one predictors believed influence outcome variable. Following James et al. (2013), model can represented following general notation:\\[\nY = f(X) + \\epsilon\n\\]\\(Y\\) represents outcome variable; \\(X\\) represents one predictors hypothesized influence outcome variable; \\(f\\) function represents relationships \\(X\\) \\(Y\\); \\(\\epsilon\\) represents error terms residuals, differences modeled values \\(Y\\) actual values. function \\(f\\) estimated using method appropriate structure data selected analyst.complete treatment regression modeling beyond scope book; recommended resources include James et al. (2013), Boehmke Greenwell (2019), Çetinkaya-Rundel Hardin (2021), Matloff (2017). purpose section illustrate example workflow using regression modeling analyze data American Community Survey. section start simple linear model extend discussion . , problems application linear model aggregated Census data discussed. First, demographic statistics often highly correlated one another, meaning Census data-based models risk collinearity predictors independent one another. Second, spatial demographic data commonly exhibit spatial autocorrelation, may lead violation assumption independent identically distributed error terms (\\(..d\\)) linear model. Suggested approaches addressing problems discussed section include dimension reduction spatial regression.","code":""},{"path":"modeling-us-census-data.html","id":"data-setup-and-exploratory-data-analysis","chapter":"8 Modeling US Census data","heading":"8.2.1 Data setup and exploratory data analysis","text":"topic study illustrative applied workflow median home value Census tract Dallas-Fort Worth metropolitan area. get started, ’ll define several counties north Texas ’ll use represent DFW region, use named vector variables acquire data represent outcome variable predictors. Data returned tidycensus argument output = \"wide\", giving one column per variable. geometry also transformed appropriate coordinate reference system North Texas, EPSG code 32138 (NAD83 / Texas North Central meters measurement units).\nTable 8.4: Data acquired tidycensus regression modeling\nACS estimates ’ve acquired include:median_valueE: median home value Census tract (outcome variable);median_valueE: median home value Census tract (outcome variable);median_roomsE: median number rooms homes Census tract;median_roomsE: median number rooms homes Census tract;total_populationE: total population;total_populationE: total population;median_ageE: median age population Census tract;median_ageE: median age population Census tract;median_year_builtE: median year built housing structures tract;median_year_builtE: median year built housing structures tract;median_incomeE: median income households Census tract;median_incomeE: median income households Census tract;pct_collegeE: percentage population age 25 four-year college degree;pct_collegeE: percentage population age 25 four-year college degree;pct_foreign_bornE: percentage population born outside United States;pct_foreign_bornE: percentage population born outside United States;pct_whiteE: percentage population identifies non-Hispanic white;pct_whiteE: percentage population identifies non-Hispanic white;percent_oohE: percentage housing units tract owner-occupied.percent_oohE: percentage housing units tract owner-occupied.","code":"\nlibrary(tidycensus)\nlibrary(sf)\n\ndfw_counties <- c(\"Collin County\", \"Dallas\", \"Denton\", \n                  \"Ellis\", \"Hunt\", \"Kaufman\", \"Rockwall\", \n                  \"Johnson\", \"Parker\", \"Tarrant\", \"Wise\")\n\nvariables_to_get <- c(\n  median_value = \"B25077_001\",\n  median_rooms = \"B25018_001\",\n  median_income = \"DP03_0062\",\n  total_population = \"B01003_001\",\n  median_age = \"B01002_001\",\n  pct_college = \"DP02_0068P\",\n  pct_foreign_born = \"DP02_0094P\",\n  pct_white = \"DP05_0077P\",\n  median_year_built = \"B25037_001\",\n  percent_ooh = \"DP04_0046P\"\n)\n\ndfw_data <- get_acs(\n  geography = \"tract\",\n  variables = variables_to_get,\n  state = \"TX\",\n  county = dfw_counties,\n  geometry = TRUE,\n  output = \"wide\",\n  year = 2020\n) %>%\n  select(-NAME) %>%\n  st_transform(32138) # NAD83 / Texas North Central"},{"path":"modeling-us-census-data.html","id":"inspecting-the-outcome-variable-with-visualization","chapter":"8 Modeling US Census data","heading":"8.2.2 Inspecting the outcome variable with visualization","text":"get started, examine geographic data distributions outcome variable, median home value, quick map geom_sf() histogram.\nFigure 8.3: Median home value charts\ncommon home values metropolitan regions, data distribution right-skewed clustering Census tracts lower end distribution values long tail expensive areas, generally located north downtown Dallas. can lead downstream violations normality model residuals. turn, might consider log-transforming outcome variable, make distribution closer normal better capture geographic variations home values trying model.\nFigure 8.4: Logged median home value charts\nexpensive areas north Dallas still stand , log-transformation makes distribution values normal better shows geographic variation home values map. suggests require data preparation prior fitting model.","code":"\nlibrary(tidyverse)\nlibrary(patchwork)\n\nmhv_map <- ggplot(dfw_data, aes(fill = median_valueE)) + \n  geom_sf(color = NA) + \n  scale_fill_viridis_c(labels = scales::label_dollar()) + \n  theme_void() + \n  labs(fill = \"Median home value \")\n\nmhv_histogram <- ggplot(dfw_data, aes(x = median_valueE)) + \n  geom_histogram(alpha = 0.5, fill = \"navy\", color = \"navy\",\n                 bins = 100) + \n  theme_minimal() + \n  scale_x_continuous(labels = scales::label_number_si(accuracy = 0.1)) + \n  labs(x = \"Median home value\")\n\nmhv_map + mhv_histogram\nlibrary(tidyverse)\nlibrary(patchwork)\n\nmhv_map_log <- ggplot(dfw_data, aes(fill = log(median_valueE))) + \n  geom_sf(color = NA) + \n  scale_fill_viridis_c() + \n  theme_void() + \n  labs(fill = \"Median home\\nvalue (log)\")\n\nmhv_histogram_log <- ggplot(dfw_data, aes(x = log(median_valueE))) + \n  geom_histogram(alpha = 0.5, fill = \"navy\", color = \"navy\",\n                 bins = 100) + \n  theme_minimal() + \n  scale_x_continuous() + \n  labs(x = \"Median home value (log)\")\n\nmhv_map_log + mhv_histogram_log"},{"path":"modeling-us-census-data.html","id":"feature-engineering","chapter":"8 Modeling US Census data","heading":"8.2.3 “Feature engineering”","text":"common term used preparing data regression modeling “feature engineering,” refers transformation predictors ways better represent relationships predictors outcome variable. Many variables acquired ACS steps already “pre-engineered” returned percentages ACS data profile, saving steps. However, variables benefit additional transformation.code creates two new variables: pop_density, represents number people Census tract per square kilometer, median_structure_age, represents median age housing structures tract.calculation pop_density column appears complicated, helpful read inside . st_area() function sf package calculates area Census tract; default square meters, using base measurement unit data’s coordinate reference system. total_population column divided area tract. Next, set_units() function used convert measurement population per square kilometer using \"1/km2\". Finally, calculation converted units vector numeric vector .numeric(). Calculating median structure age straightforward, median_year_builtE column subtracted 2017, mid-point 5-year ACS period data derived. Finally, simplify dataset, margin error columns dropped, E end estimate columns removed rename_with(), tracts NA values dropped well na.omit().can examine modified dataset:\nTable 8.5: Engineered predictors regression modeling\n","code":"\nlibrary(sf)\nlibrary(units)\n\ndfw_data_for_model <- dfw_data %>%\n  mutate(pop_density = as.numeric(set_units(total_populationE / st_area(.), \"1/km2\")),\n         median_structure_age = 2018 - median_year_builtE) %>%\n  select(!ends_with(\"M\")) %>% \n  rename_with(.fn = ~str_remove(.x, \"E$\")) %>%\n  na.omit()"},{"path":"modeling-us-census-data.html","id":"a-first-regression-model","chapter":"8 Modeling US Census data","heading":"8.2.4 A first regression model","text":"inspecting distribution outcome variable completing feature engineering respect predictors, ready fit first linear model. linear model log-transformed outcome variable can written follows:\\[\n\\begin{align*}\\operatorname{log(median\\_value)} &= \\alpha + \\beta_{1}(\\operatorname{median\\_rooms}) + \\beta_{2}(\\operatorname{median\\_income})\\ + \\\\&\\quad \\beta_{3}(\\operatorname{pct\\_college}) + \\beta_{4}(\\operatorname{pct\\_foreign\\_born}) + \\\\&\\quad\\beta_{5}(\\operatorname{pct\\_white})\\ + \\beta_{6}(\\operatorname{median\\_age}) + \\\\&\\quad\\beta_{7}(\\operatorname{median\\_structure\\_age}) + \\beta_{8}(\\operatorname{percent\\_ooh})\\ + \\\\&\\quad \\beta_{9}(\\operatorname{pop\\_density}) + \\beta_{10}(\\operatorname{total\\_population}) + \\epsilon\\end{align*}\n\\] \\(\\alpha\\) model intercept, \\(\\beta_{1}\\) change log median home value 1-unit increase median number rooms (forth model predictors) holding predictors constant, \\(\\epsilon\\) error term.Model formulas R generally written outcome ~ predictor_1 + predictor_2 + ... + predictor_k, k number model predictors. formula can supplied character string model function (shown ) supplied unquoted call function. use lm() fit linear model, check results summary().printed summary gives us information model fit. Estimate column represents model parameters (\\(\\beta\\) values), followed standard error, t-value test statistic, p-value helps us assess relative statistical significance. James et al. (2013) (p. 67) provides concise summary p-values interpreted:Roughly speaking, interpret p-value follows: small p-value indicates unlikely observe substantial association predictor response (outcome variable) due chance, absence real association predictor response. Hence, see small p-value, can infer association predictor response. reject null hypothesis – , declare relationship exist X Y – p-value small enough.convention, researchers use p-value cutoffs 0.05, 0.01, 0.001, depending topic study; values highlighted asterisks model summary printout. Examining model parameters p-values suggests median household income, bachelor’s degree attainment, percentage non-Hispanic white population, median age positively associated median home values, whereas percentage owner-occupied housing negatively associated median home values. R-squared value 0.78, suggesting model explains around 78 percent variance median_value.Somewhat surprisingly, median_rooms appear significant relationship median home value per model. one hand, can interpreted “effect median rooms median home value predictors held constant” - also suggestive model mis-specification. mentioned earlier, models using ACS data predictors highly vulnerable collinearity. Collinearity occurs two predictors highly correlated one another, can lead misinterpretation actual relationships predictors outcome variable.One way inspect collinearity visualize correlation matrix predictors, correlations predictors calculated one another. correlate() function corrr package (Kuhn, Jackson, Cimentada 2020) offers straightforward method calculating correlation matrix rectangular data frame.One calculated, correlation matrix can visualized network_plot():\nFigure 8.5: Network plot correlations model predictors\nnotice predictors correlated one another degree, unsurprising given represent social demographic data. Collinearity can diagnosed calculating variance inflation factor (VIF) model, takes account just pairwise correlations extent predictors collinear predictors. VIF value 1 indicates collinearity; VIF values 5 suggest level collinearity problematic influence model interpretation (James et al. 2013). VIF implemented vif() function car package (Fox Weisberg 2019).problematic variable median_income, VIF value 6. potential solution involves removing variable re-running model; highly correlated predictors model, effect median household income theory captured remaining predictors.model R-squared drops slightly substantially 0.76. Notably, effect median_rooms median home value now comes strongly positive statistically significant, suggesting collinearity median household income suppressing relationship first model. diagnostic, can re-compute VIF second model:VIF values predictors model now 5.","code":"\nformula <- \"log(median_value) ~ median_rooms + median_income + pct_college + pct_foreign_born + pct_white + median_age + median_structure_age + percent_ooh + pop_density + total_population\"\n\nmodel1 <- lm(formula = formula, data = dfw_data_for_model)\n\nsummary(model1)## \n## Call:\n## lm(formula = formula, data = dfw_data_for_model)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.03015 -0.14250  0.00033  0.14794  1.45712 \n## \n## Coefficients:\n##                        Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)           1.123e+01  6.199e-02 181.093  < 2e-16 ***\n## median_rooms          8.800e-03  1.058e-02   0.832 0.405711    \n## median_income         5.007e-06  4.202e-07  11.915  < 2e-16 ***\n## pct_college           1.325e-02  5.994e-04  22.108  < 2e-16 ***\n## pct_foreign_born      2.877e-03  8.005e-04   3.594 0.000336 ***\n## pct_white             3.961e-03  4.735e-04   8.365  < 2e-16 ***\n## median_age            4.782e-03  1.372e-03   3.485 0.000507 ***\n## median_structure_age  1.202e-05  2.585e-05   0.465 0.642113    \n## percent_ooh          -4.761e-03  5.599e-04  -8.504  < 2e-16 ***\n## pop_density          -7.946e-06  6.160e-06  -1.290 0.197216    \n## total_population      8.960e-06  4.460e-06   2.009 0.044733 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2695 on 1548 degrees of freedom\n## Multiple R-squared:  0.7818, Adjusted R-squared:  0.7804 \n## F-statistic: 554.6 on 10 and 1548 DF,  p-value: < 2.2e-16\nlibrary(corrr)\n\ndfw_estimates <- dfw_data_for_model %>%\n  select(-GEOID, -median_value, -median_year_built) %>%\n  st_drop_geometry()\n\ncorrelations <- correlate(dfw_estimates, method = \"pearson\")\nnetwork_plot(correlations)\nlibrary(car)\n\nvif(model1)##         median_rooms        median_income          pct_college \n##             5.450436             6.210615             3.722434 \n##     pct_foreign_born            pct_white           median_age \n##             2.013411             3.233142             1.833625 \n## median_structure_age          percent_ooh          pop_density \n##             1.055760             3.953587             1.537508 \n##     total_population \n##             1.174613\nformula2 <- \"log(median_value) ~ median_rooms + pct_college + pct_foreign_born + pct_white + median_age + median_structure_age + percent_ooh + pop_density + total_population\"\n\nmodel2 <- lm(formula = formula2, data = dfw_data_for_model)\n\nsummary(model2)## \n## Call:\n## lm(formula = formula2, data = dfw_data_for_model)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.91753 -0.15318 -0.00224  0.16192  1.58948 \n## \n## Coefficients:\n##                        Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)           1.101e+01  6.203e-02 177.566  < 2e-16 ***\n## median_rooms          7.326e-02  9.497e-03   7.713 2.18e-14 ***\n## pct_college           1.775e-02  4.862e-04  36.506  < 2e-16 ***\n## pct_foreign_born      4.170e-03  8.284e-04   5.034 5.38e-07 ***\n## pct_white             4.996e-03  4.862e-04  10.274  < 2e-16 ***\n## median_age            3.527e-03  1.429e-03   2.468   0.0137 *  \n## median_structure_age  2.831e-05  2.696e-05   1.050   0.2939    \n## percent_ooh          -3.888e-03  5.798e-04  -6.705 2.81e-11 ***\n## pop_density          -5.474e-06  6.430e-06  -0.851   0.3947    \n## total_population      9.711e-06  4.658e-06   2.085   0.0373 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2815 on 1549 degrees of freedom\n## Multiple R-squared:  0.7618, Adjusted R-squared:  0.7604 \n## F-statistic: 550.4 on 9 and 1549 DF,  p-value: < 2.2e-16\nvif(model2)##         median_rooms          pct_college     pct_foreign_born \n##             4.025411             2.245227             1.976425 \n##            pct_white           median_age median_structure_age \n##             3.124400             1.822825             1.052805 \n##          percent_ooh          pop_density     total_population \n##             3.885779             1.535763             1.174378"},{"path":"modeling-us-census-data.html","id":"dimension-reduction-with-principal-components-analysis","chapter":"8 Modeling US Census data","heading":"8.2.5 Dimension reduction with principal components analysis","text":"example , dropping median household income model fairly negligible impact overall model fit significantly improved model’s problems collinearity. However, always best solution analysts, especially dropping variables significant impact model fit. alternative approach resolving problems collinearity dimension reduction, transforms predictors series dimensions represent variance predictors uncorrelated one another. Dimension reduction also useful technique analyst dealing massive number predictors (hundreds even thousands) needs reduce predictors model manageable number still retaining ability explain variance outcome variable.One popular methods dimension reduction principal components analysis. Principal components analysis (PCA) reduces higher-dimensional dataset lower-dimensional representation based linear combinations variables used. first principal component linear combination variables explains overall variance data; second principal component explains second-overall variance also constrained uncorrelated first component; forth.PCA can computed prcomp() function. use dfw_estimates object used compute correlation data frame includes predictors regression model, use notation formula = ~. compute PCA predictors. convention, scale. center set TRUE normalizes variables dataset computing PCA given measured differently.Printing summary() PCA model shows 10 components collectively explain 100% variance original predictors. first principal component explains 40.8 percent overall variance; second explains 14 percent; forth.understand different principal components now mean, helpful plot variable loadings. represents relationships original variables model derived components. approach derived Julia Silge’s blog post topic (Silge 2021).First, variable loading matrix (stored rotation element pca object) converted tibble can view easier.\nTable 8.6: PCA variable loadings\nPositive values given row mean original variable positively loaded onto given component, negative values mean variable negatively loaded. Larger values direction interest us; values near 0 mean variable meaningfully explained given component. explore , can visualize first five components ggplot2:\nFigure 8.6: Loadings first five principal components\nrespect PC1, explains nearly 41 percent variance overall predictor set, variables percent_ooh, pct_white, pct_college, median_rooms, median_income, median_age load negatively, whereas pop_density pct_foreign_born load positively. can attach principal components original data predict() cbind(), make map PC1 exploration:\nFigure 8.7: Map principal component 1\nmap, along bar chart, helps us understand multiple variables represent latent social processes play Dallas-Fort Worth. brighter yellow areas, higher values PC1, located communities like east Fort Worth, east Arlington, Grand Prairie, south Dallas. Generally speaking, low--middle income areas larger nonwhite populations. locations lowest values PC1 Southlake (northeast Fort Worth) Highland Park (north downtown Dallas); communities segregated, predominantly non-Hispanic white, among wealthiest neighborhoods entire United States. turn, PC1 captures gradient represents social differences, multiple demographic characteristics associated.principal components can used principal components regression, derived components used model predictors. Generally, components chosen account least 90 percent original variance predictors, though often discretion analyst. example , fit model using first six principal components log median home value outcome variable.model fit, represented R-squared value, similar models fit earlier chapter. One possible disadvantage principal components regression, however, interpretation results different variables comprehensible now spread across components. can helpful think different components indices sense.discussed , PC1 represents gradient segregated, older, wealthy, white communities low end diverse, lower-income, younger communities high end; PC negatively associated median home values, tracks expectations. Reviewing plot , PC2 associated lower population densities levels educational attainment; turn, can thought urban (low end) rural (high end) gradient. negative association median home value expected home values higher urban core rural fringe metropolitan area.","code":"\npca <- prcomp(\n  formula = ~., \n  data = dfw_estimates, \n  scale. = TRUE, \n  center = TRUE\n)\n\nsummary(pca)## Importance of components:\n##                          PC1    PC2    PC3    PC4     PC5     PC6     PC7\n## Standard deviation     2.020 1.1832 1.1307 1.0093 0.89917 0.70312 0.67686\n## Proportion of Variance 0.408 0.1400 0.1278 0.1019 0.08085 0.04944 0.04581\n## Cumulative Proportion  0.408 0.5481 0.6759 0.7778 0.85860 0.90803 0.95385\n##                            PC8     PC9    PC10\n## Standard deviation     0.48099 0.36127 0.31567\n## Proportion of Variance 0.02314 0.01305 0.00997\n## Cumulative Proportion  0.97698 0.99003 1.00000\npca_tibble <- pca$rotation %>%\n  as_tibble(rownames = \"predictor\")\npca_tibble %>%\n  select(predictor:PC5) %>%\n  pivot_longer(PC1:PC5, names_to = \"component\", values_to = \"value\") %>%\n  ggplot(aes(x = value, y = predictor)) + \n  geom_col(fill = \"darkgreen\", color = \"darkgreen\", alpha = 0.5) + \n  facet_wrap(~component, nrow = 1) + \n  labs(y = NULL, x = \"Value\") + \n  theme_minimal()\ncomponents <- predict(pca, dfw_estimates)\n\ndfw_pca <- dfw_data_for_model %>%\n  select(GEOID, median_value) %>%\n  cbind(components) \n\nggplot(dfw_pca, aes(fill = PC1)) +\n  geom_sf(color = NA) +\n  theme_void() +\n  scale_fill_viridis_c()\npca_formula <- paste0(\"log(median_value) ~ \", \n                      paste0('PC', 1:6, collapse = ' + '))\n\npca_model <- lm(formula = pca_formula, data = dfw_pca)\n\nsummary(pca_model)## \n## Call:\n## lm(formula = pca_formula, data = dfw_pca)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.78888 -0.16854 -0.00726  0.16941  1.60089 \n## \n## Coefficients:\n##              Estimate Std. Error  t value Pr(>|t|)    \n## (Intercept) 12.301439   0.007483 1643.902   <2e-16 ***\n## PC1         -0.180706   0.003706  -48.765   <2e-16 ***\n## PC2         -0.247181   0.006326  -39.072   <2e-16 ***\n## PC3         -0.077097   0.006621  -11.645   <2e-16 ***\n## PC4         -0.084417   0.007417  -11.382   <2e-16 ***\n## PC5          0.111525   0.008325   13.397   <2e-16 ***\n## PC6          0.003787   0.010646    0.356    0.722    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2955 on 1552 degrees of freedom\n## Multiple R-squared:  0.737,  Adjusted R-squared:  0.736 \n## F-statistic: 724.9 on 6 and 1552 DF,  p-value: < 2.2e-16"},{"path":"modeling-us-census-data.html","id":"spatial-regression","chapter":"8 Modeling US Census data","heading":"8.3 Spatial regression","text":"core assumption linear model errors independent one another normally distributed. Log-transforming right-skewed outcome variable, median home values, indented resolve latter; can check adding residuals model2 dataset drawing histogram check distribution.\nFigure 8.8: Distribution model residuals ggplot2 histogram\nformer assumption independence residuals commonly violated models use spatial data, however. models spatial processes commonly characterized spatial autocorrelation error term, meaning model’s performance depends geographic location. can assess using techniques learned previous chapter Moran’s \\(\\).Moran’s \\(\\) test statistic modest positive (0.21) statistically significant. can visualized Moran scatterplot:\nFigure 8.9: Moran scatterplot residual spatial autocorrelation\nplot illustrates positive spatial autocorrelation residuals, suggesting assumption independence model error term violated. resolve issue, can turn spatial regression methods.","code":"\ndfw_data_for_model$residuals <- residuals(model2)\n\nggplot(dfw_data_for_model, aes(x = residuals)) + \n  geom_histogram(bins = 100, alpha = 0.5, color = \"navy\",\n                 fill = \"navy\") + \n  theme_minimal()\nlibrary(spdep)\n\nwts <- dfw_data_for_model %>%\n  poly2nb() %>%\n  nb2listw()\n\nmoran.test(dfw_data_for_model$residuals, wts)## \n##  Moran I test under randomisation\n## \n## data:  dfw_data_for_model$residuals  \n## weights: wts    \n## \n## Moran I statistic standard deviate = 14.023, p-value < 2.2e-16\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##      0.2101748515     -0.0006418485      0.0002259981\ndfw_data_for_model$lagged_residuals <- lag.listw(wts, dfw_data_for_model$residuals)\n\nggplot(dfw_data_for_model, aes(x = residuals, y = lagged_residuals)) + \n  theme_minimal() + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", color = \"red\")"},{"path":"modeling-us-census-data.html","id":"methods-for-spatial-regression","chapter":"8 Modeling US Census data","heading":"8.3.1 Methods for spatial regression","text":"field spatial econometrics broadly concerned estimation specification models appropriate handling spatial dependence statistical processes. general, two families models used address concerns respect regression: spatial lag models spatial error models.","code":""},{"path":"modeling-us-census-data.html","id":"spatial-lag-models","chapter":"8 Modeling US Census data","heading":"8.3.1.1 Spatial lag models","text":"Spatial lag models account spatial dependence including spatial lag outcome variable model. , accounts spatial spillover effects – possibility values neighboring areas influence values given location. spatial lag model can written follows (Rey, Arribas-Bel, Wolf 2020):\\[\n{Y_i} = \\alpha + \\rho{Y_{lag-}} + \\sum_k \\beta_k X_{ki} + \\epsilon_i,\n\\]\\[\nY_{lag-} = \\sum\\limits_{j}w_{ij}Y_j\n\\]\\(w_{ij}\\) representing spatial weights. notation, \\(\\rho\\) parameter measuring effect spatial lag outcome variable, \\(k\\) number predictors model. However, inclusion spatially lagged outcome variable right-hand side equation violates exogeneity assumption linear model. turn, special methods required estimating spatial lag model, implemented R spatialreg package (R. S. Bivand, Pebesma, Gomez-Rubio 2013). , use function lagsarlm() estimate relationship logged median home value predictors spatial lag model:general statistical relationships observed non-spatial model preserved spatial lag model, though effect sizes (model parameters) smaller, illustrating importance controlling spatial lag. Additionally, \\(\\rho\\) parameter positive statistically significant, suggesting presence spatial spillover effects. finding makes practical sense, median home values may influenced values homes neighboring Census tracts along characteristics neighborhood . argument Nagelkerke = TRUE computes pseudo-R-squared value, slightly higher corresponding value non-spatial model.","code":"\nlibrary(spatialreg)\n\nlag_model <- lagsarlm(\n  formula = formula2, \n  data = dfw_data_for_model, \n  listw = wts\n)\n\nsummary(lag_model, Nagelkerke = TRUE)## \n## Call:lagsarlm(formula = formula2, data = dfw_data_for_model, listw = wts)\n## \n## Residuals:\n##        Min         1Q     Median         3Q        Max \n## -2.0647421 -0.1377312 -0.0032552  0.1386914  1.4820482 \n## \n## Type: lag \n## Coefficients: (asymptotic standard errors) \n##                         Estimate  Std. Error z value  Pr(>|z|)\n## (Intercept)           7.0184e+00  2.6898e-01 26.0927 < 2.2e-16\n## median_rooms          6.2027e-02  8.8554e-03  7.0045 2.480e-12\n## pct_college           1.2858e-02  5.4696e-04 23.5083 < 2.2e-16\n## pct_foreign_born      2.0118e-03  7.7482e-04  2.5964  0.009420\n## pct_white             2.7112e-03  4.7183e-04  5.7461 9.133e-09\n## median_age            3.4421e-03  1.3163e-03  2.6150  0.008922\n## median_structure_age  2.6093e-05  2.4827e-05  1.0510  0.293267\n## percent_ooh          -3.0428e-03  5.4316e-04 -5.6021 2.118e-08\n## pop_density          -1.3573e-05  5.9323e-06 -2.2879  0.022143\n## total_population      8.3762e-06  4.2928e-06  1.9512  0.051031\n## \n## Rho: 0.35319, LR test value: 210.86, p-value: < 2.22e-16\n## Asymptotic standard error: 0.023376\n##     z-value: 15.109, p-value: < 2.22e-16\n## Wald statistic: 228.29, p-value: < 2.22e-16\n## \n## Log likelihood: -125.2882 for lag model\n## ML residual variance (sigma squared): 0.067179, (sigma: 0.25919)\n## Nagelkerke pseudo-R-squared: 0.79193 \n## Number of observations: 1559 \n## Number of parameters estimated: 12 \n## AIC: 274.58, (AIC for lm: 483.43)\n## LM test for residual autocorrelation\n## test value: 6.9225, p-value: 0.0085118"},{"path":"modeling-us-census-data.html","id":"spatial-error-models","chapter":"8 Modeling US Census data","heading":"8.3.1.2 Spatial error models","text":"contrast spatial lag models, spatial error models include spatial lag model’s error term. designed capture latent spatial processes currently accounted model estimation turn show model’s residuals. spatial error model can written follows:\\[\nY_i = \\alpha + \\sum\\limits_k\\beta_kX_{ki} + u_i,\n\\]\\[\nu_i = \\lambda u_{lag-} + \\epsilon_i\n\\]\\[\nu_{lag-} = \\sum\\limits_jw_{ij}u_j\n\\]Like spatial lag model, estimating spatial error model requires special methods, implemented errorsarlm() function spatialreg.\\(\\lambda\\) (lambda) value large statistically significant, illustrating importance accounting spatial autocorrelation model.","code":"\nerror_model <- errorsarlm(\n  formula = formula2, \n  data = dfw_data_for_model, \n  listw = wts\n)\n\nsummary(error_model, Nagelkerke = TRUE)## \n## Call:errorsarlm(formula = formula2, data = dfw_data_for_model, listw = wts)\n## \n## Residuals:\n##         Min          1Q      Median          3Q         Max \n## -1.97990245 -0.13702534 -0.00030105  0.13933507  1.54937871 \n## \n## Type: error \n## Coefficients: (asymptotic standard errors) \n##                         Estimate  Std. Error  z value  Pr(>|z|)\n## (Intercept)           1.1098e+01  6.6705e-02 166.3753 < 2.2e-16\n## median_rooms          8.2815e-02  9.7089e-03   8.5298 < 2.2e-16\n## pct_college           1.5857e-02  5.7427e-04  27.6120 < 2.2e-16\n## pct_foreign_born      3.6601e-03  9.6570e-04   3.7901 0.0001506\n## pct_white             4.6754e-03  6.1175e-04   7.6426 2.132e-14\n## median_age            3.9346e-03  1.4130e-03   2.7845 0.0053605\n## median_structure_age  2.6093e-05  2.5448e-05   1.0254 0.3051925\n## percent_ooh          -4.7538e-03  5.6726e-04  -8.3803 < 2.2e-16\n## pop_density          -1.4999e-05  6.8731e-06  -2.1823 0.0290853\n## total_population      1.0497e-05  4.4668e-06   2.3499 0.0187796\n## \n## Lambda: 0.46765, LR test value: 164.17, p-value: < 2.22e-16\n## Asymptotic standard error: 0.031997\n##     z-value: 14.615, p-value: < 2.22e-16\n## Wald statistic: 213.61, p-value: < 2.22e-16\n## \n## Log likelihood: -148.6309 for error model\n## ML residual variance (sigma squared): 0.067878, (sigma: 0.26053)\n## Nagelkerke pseudo-R-squared: 0.7856 \n## Number of observations: 1559 \n## Number of parameters estimated: 12 \n## AIC: 321.26, (AIC for lm: 483.43)"},{"path":"modeling-us-census-data.html","id":"choosing-between-spatial-lag-and-spatial-error-models","chapter":"8 Modeling US Census data","heading":"8.3.2 Choosing between spatial lag and spatial error models","text":"spatial lag spatial error models offer alternative approaches accounting processes spatial autocorrelation fitting models. raises question: one two models analyst choose? one hand, thought context topic study. example, spatial spillover effects related hypotheses evaluated analysts (e.g. effect neighboring home values focal home values), spatial lag model may preferable; alternatively, spatially autocorrelated factors likely influence outcome variable difficult measure quantitatively (e.g. discrimination racial bias housing market), spatial error model might preferred.two types models can also evaluated respect quantitative metrics. example, can re-compute Moran’s \\(\\) model residuals see spatial model resolved problems spatial dependence. First, ’ll check spatial lag model:Next, spatial error model:models reduce Moran’s \\(\\); however, error model better job eliminating spatial autocorrelation residuals entirely. can also use Lagrange multiplier tests evaluate appropriateness models together (Anselin et al. 1996). tests check spatial error dependence, whether spatially lagged dependent variable missing, robustness presence .lm.LMtests() function can used input linear model compute tests. ’ll use model2, home value model median household income omitted, compute tests.test statistics large statistically significant; case, robust versions statistics compared. lag error models appropriate data, test statistic robust version lag model larger, suggesting spatial lag model preferred spatial error model example.","code":"\nmoran.test(lag_model$residuals, wts)## \n##  Moran I test under randomisation\n## \n## data:  lag_model$residuals  \n## weights: wts    \n## \n## Moran I statistic standard deviate = 2.0436, p-value = 0.0205\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##      0.0300648348     -0.0006418485      0.0002257748\nmoran.test(error_model$residuals, wts)## \n##  Moran I test under randomisation\n## \n## data:  error_model$residuals  \n## weights: wts    \n## \n## Moran I statistic standard deviate = -1.6126, p-value = 0.9466\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##     -0.0248732656     -0.0006418485      0.0002257849\nlm.LMtests(\n  model2, \n  wts, \n  test = c(\"LMerr\", \"LMlag\", \"RLMerr\", \"RLMlag\")\n)## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: lm(formula = formula2, data = dfw_data_for_model)\n## weights: wts\n## \n## LMerr = 194.16, df = 1, p-value < 2.2e-16\n## \n## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: lm(formula = formula2, data = dfw_data_for_model)\n## weights: wts\n## \n## LMlag = 223.37, df = 1, p-value < 2.2e-16\n## \n## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: lm(formula = formula2, data = dfw_data_for_model)\n## weights: wts\n## \n## RLMerr = 33.063, df = 1, p-value = 8.921e-09\n## \n## \n##  Lagrange multiplier diagnostics for spatial dependence\n## \n## data:  \n## model: lm(formula = formula2, data = dfw_data_for_model)\n## weights: wts\n## \n## RLMlag = 62.276, df = 1, p-value = 2.998e-15"},{"path":"modeling-us-census-data.html","id":"geographically-weighted-regression","chapter":"8 Modeling US Census data","heading":"8.4 Geographically weighted regression","text":"models addressed previous sections – regular linear model spatial adaptations – estimate global relationships outcome variable, median home values, predictors. lends conclusions like “Dallas-Fort Worth metropolitan area, higher levels educational attainment associated higher median home values.” However, metropolitan regions like Dallas-Fort Worth diverse multifaceted. possible relationship predictor outcome variable observed entire region average may vary significantly neighborhood neighborhood. type phenomenon called spatial non-stationarity, can explored geographically weighted regression, GWR (Brunsdon, Fotheringham, Charlton 1996).GWR technique designed evaluate local variations results regression models given kernel (distance-decay) weighting function. Following Lu et al. (2014), basic form GWR given location \\(\\) can written :\\[\nY_i = \\alpha_i + \\sum\\limits_{k=1}^m\\beta_{ik}X_{ik} + \\epsilon_i\n\\]model intercept, parameters, error term location-specific. Notably, \\(\\beta_{ik}\\) represents local regression coefficient predictor \\(k\\) (total number predictors \\(m\\)) specific location \\(\\).GWR implemented GWmodel R package (Gollini et al. 2015) well spgwr package (R. Bivand Yu 2020). packages offer interface wider family geographically weighted methods, binomial, generalized linear, robust geographically weighted regression; geographically weighted PCA; geographically weighted summary statistics. example adapt regression model used earlier examples locally-variation model GWR.","code":""},{"path":"modeling-us-census-data.html","id":"choosing-a-bandwidth-for-gwr","chapter":"8 Modeling US Census data","heading":"8.4.1 Choosing a bandwidth for GWR","text":"GWR relies concept “kernel bandwidth” compute local regression model location. kernel bandwidth based kernel type (fixed adaptive) distance-decay function. fixed kernel uses cutoff distance determine observations included local model given location \\(\\), whereas adaptive kernel uses nearest neighbors given location. circumstances Census tract data size tracts region vary widely, adaptive kernel preferred fixed kernel ensure consistency neighborhoods across region. distance-decay function governs observations weighted local model relative distance location \\(\\). Closer tracts \\(\\) greater influence results location \\(\\), influence falling distance.Bandwidth sizes (either distance cutoff number nearest neighbors) can selected directly user; GWmodel R package, bw.gwr() function also helps analysts choose appropriate kernel bandwidth using cross-validation. code computes bandwidth bw using method. Note must first convert data legacy SpatialPolygonsDataFrame object sp package GWmodel yet support sf objects.bw.gwr() chose 187 number nearest neighbors based cross-validation. means Census tract, nearest 187 total 1559 Census tracts Dallas-Fort Worth region used estimate local model, weights calculated using bisquare distance-decay function follows:\\[\nw_{ij} = 1-(\\frac{d_{ij}^2}{h^2})^2\n\\]\\(d_{ij}\\) distance local observation \\(\\) neighbor \\(j\\), \\(h\\) kernel bandwidth. using adaptive kernel, \\(h\\) vary observation take distance location \\(\\) “neighbor” furthest location.","code":"\nlibrary(GWmodel)\nlibrary(sf)\n\ndfw_data_sp <- dfw_data_for_model %>%\n  as_Spatial()\n\nbw <- bw.gwr(\n  formula = formula2, \n  data = dfw_data_sp, \n  kernel = \"bisquare\",\n  adaptive = TRUE\n)"},{"path":"modeling-us-census-data.html","id":"fitting-and-evaluating-the-gwr-model","chapter":"8 Modeling US Census data","heading":"8.4.2 Fitting and evaluating the GWR model","text":"basic form GWR can fit gwr.basic() function, uses similar argument structure models fit chapter. formula can passed formula parameter character string; ’ll use original formula median household income omitted include refresher. derived bandwidth bw.gwr() used bisquare, adaptive kernel.Printing object gw_model show results global model ranges locally varying parameter estimates. model object following elements:element provides information model fit, perhaps interesting analyst SDF element, case SpatialPolygonsDataFrame containing mappable model results. extract element model object convert simple features, take look columns object.sf object includes columns local parameter estimates, standard errors, t-values predictor, along local diagnostic elements Local R-squared, giving information well model performs particular location. , use ggplot2 geom_sf() map local R-squared, though may useful well use mapview::mapview(gw_model_results, zcol = \"Local_R2\" explore results interactively.\nFigure 8.10: Local R-squared values GWR model\nmap suggests model performs well Fort Worth, Collin County, eastern edge metropolitan area, local R-squared values exceeding 0.9. performs worse northwestern Denton County rural areas west Fort Worth.can examine locally varying parameter estimates much way. first map visualizes local relationships percentage owner-occupied housing median home values. Recall global model coefficient negative statistically significant.\nFigure 8.11: Local parameter estimates percent owner-occupied housing\ndark purple areas map areas global relationship model reflects local relationship, local parameter estimates negative. areas stick include high-density area uptown Dallas, renter-occupied housing common median home values high. However, rural areas fringe metropolitan area relationship reverses, returning cases positive parameter estimates (yellow parts map). means local areas, greater percentage owner-occupied housing associated higher home values.can explore investigating local parameter estimates population density, significant global model:\nFigure 8.12: Local parameter estimates median structure age\nlarge portions metropolitan area, relationship population density median home values negligible, drives global relationship. However, observe variations different parts metropolitan area. Bright yellow locations high population densities associated higher home values. Conversely, darker blue purple areas represent several affluent enclaves suburbs Dallas Fort Worth lower densities associated higher home values.","code":"\nformula2 <- \"log(median_value) ~ median_rooms + pct_college + pct_foreign_born + pct_white + median_age + median_structure_age + percent_ooh + pop_density + total_population\"\n\ngw_model <- gwr.basic(\n  formula = formula2, \n  data = dfw_data_sp, \n  bw = bw,\n  kernel = \"bisquare\",\n  adaptive = TRUE\n)\nnames(gw_model)## [1] \"GW.arguments\"  \"GW.diagnostic\" \"lm\"            \"SDF\"          \n## [5] \"timings\"       \"this.call\"     \"Ftests\"\ngw_model_results <- gw_model$SDF %>%\n  st_as_sf() \n\nnames(gw_model_results)##  [1] \"Intercept\"               \"median_rooms\"           \n##  [3] \"pct_college\"             \"pct_foreign_born\"       \n##  [5] \"pct_white\"               \"median_age\"             \n##  [7] \"median_structure_age\"    \"percent_ooh\"            \n##  [9] \"pop_density\"             \"total_population\"       \n## [11] \"y\"                       \"yhat\"                   \n## [13] \"residual\"                \"CV_Score\"               \n## [15] \"Stud_residual\"           \"Intercept_SE\"           \n## [17] \"median_rooms_SE\"         \"pct_college_SE\"         \n## [19] \"pct_foreign_born_SE\"     \"pct_white_SE\"           \n## [21] \"median_age_SE\"           \"median_structure_age_SE\"\n## [23] \"percent_ooh_SE\"          \"pop_density_SE\"         \n## [25] \"total_population_SE\"     \"Intercept_TV\"           \n## [27] \"median_rooms_TV\"         \"pct_college_TV\"         \n## [29] \"pct_foreign_born_TV\"     \"pct_white_TV\"           \n## [31] \"median_age_TV\"           \"median_structure_age_TV\"\n## [33] \"percent_ooh_TV\"          \"pop_density_TV\"         \n## [35] \"total_population_TV\"     \"Local_R2\"               \n## [37] \"geometry\"\nggplot(gw_model_results, aes(fill = Local_R2)) + \n  geom_sf(color = NA) + \n  scale_fill_viridis_c() + \n  theme_void()\nggplot(gw_model_results, aes(fill = percent_ooh)) + \n  geom_sf(color = NA) + \n  scale_fill_viridis_c() + \n  theme_void() + \n  labs(fill = \"Local β for \\npercent_ooh\")\nggplot(gw_model_results, aes(fill = pop_density)) + \n  geom_sf(color = NA) + \n  scale_fill_viridis_c() + \n  theme_void() + \n  labs(fill = \"Local β for \\npopulation density\")"},{"path":"modeling-us-census-data.html","id":"limitations-of-gwr","chapter":"8 Modeling US Census data","heading":"8.4.3 Limitations of GWR","text":"GWR excellent method exploring spatial non-stationarity regression model results, limitations. spgwr package loaded R environment, prints following warning:NOTE: package constitute approval GWR method spatial analysis.R package warn user use? GWR particularly susceptible problems plague regression models using spatial data. Earlier sections chapter covered topic collinearity, parameter estimates biased due high correlations predictors. Given predictor values tend cluster spatially, GWR models often suffer local multicollinearity predictors highly correlated local areas.Additionally, impact edge effects can acute GWR models. Edge effects - present spatial models analysis techniques - refer misleading results observations edge dataset. example used chapter, Dallas-Fort Worth metropolitan area represents region study. artificially restricts neighborhoods around Census tracts edge metropolitan area tracts also within metropolitan area, omits rural tracts border . local models, particular problem results observations edge area based incomplete information may reflect true circumstances location.concerns mind, GWR generally recommended exploratory technique serves useful companion estimation global model. example, global parameter estimate may suggest Dallas-Fort Worth, median home values tend higher areas lower percentages owner-occupied housing, controlling predictors model. GWR, used follow-, helps analyst understand global relationship driven higher home values near urban cores uptown Dallas area, may necessarily characterize dynamics rural exurban areas elsewhere metropolitan region.","code":""},{"path":"modeling-us-census-data.html","id":"classification-and-clustering-of-acs-data","chapter":"8 Modeling US Census data","heading":"8.5 Classification and clustering of ACS data","text":"statistical models discussed earlier chapter fit purpose understanding relationships outcome variable series predictors. social sciences, models generally used inference, researcher tests hypotheses respect relationships understand social processes. industry, regression models commonly used instead prediction, model trained relationship observed outcome predictors used make predictions --sample data. machine learning terminology, referred supervised learning, prediction target known.cases, researcher interested discovering structure dataset generating meaningful labels rather making predictions based known outcome. type approach termed unsupervised machine learning. section explore two common applications unsupervised machine learning respect demographic data: geodemographic classification, identifies “clusters” similar areas based common demographic characteristics, regionalization, partitions area salient regions spatially contiguous share common demographic attributes.","code":""},{"path":"modeling-us-census-data.html","id":"geodemographic-classification","chapter":"8 Modeling US Census data","heading":"8.5.1 Geodemographic classification","text":"Geodemographic classification refers grouping geographic observations based similar demographic () characteristics (Singleton Spielman 2013). commonly used generate neighborhood “typologies” can help explain general similarities differences among neighborhoods broader region. geodemographic approach criticized essentializing neighborhoods (Goss 1995), also widely used understand dynamics urban systems (Vicino, Hanlon, Short 2011) proposed possible solution problems large margins error individual variables ACS (Spielman Singleton 2015). industry, geodemographics widely used marketing customer segmentation purposes. Popular frameworks include Esri’s Tapestry Segmentation Experian’s Mosaic product.exact methodology produce geodemographic classification system varies implementation implementation, general process used involves dimension reduction applied high-dimensional input dataset model features, followed clustering algorithm partition observations groups based derived dimensions. already employed principal components analysis dimension reduction Dallas-Fort Worth dataset, can re-use components purpose.k-means clustering algorithm one common unsupervised algorithms used partition data way. K-means works attempting generate \\(K\\) clusters internally similar dissimilar clusters. Following James et al. (2013) Boehmke Greenwell (2019), goal K-means clustering can written :\\[\n\\underset{C_1...C_k}{\\text{minimize}}\\left \\{ \\sum\\limits_{k=1}^KW(C_k) \\right \\}\n\\]within-cluster variation \\(W(C_k)\\) computed \\[\nW(C_k) = \\sum_{x_i \\C_k}(x_{} - \\mu_k) ^ 2\n\\]\\(x_i\\) representing observation cluster \\(C_k\\) \\(\\mu_k\\) representing mean value observations cluster \\(C_k\\).compute k-means, analyst must first choose number desired clusters, represented \\(k\\). analyst specifies \\(k\\) initial “centers” data (generally done random) seed algorithm. algorithm iteratively assigns observations clusters total within-cluster variation minimized, returning cluster solution.R, k-means can computed kmeans() function. example solution generate 6 cluster groups. Given algorithm relies random seeding cluster centers, set.seed() used ensure stability solution.algorithm partitioned data six clusters; smallest (Cluster 4) 83 Census tracts, whereas largest (Cluster 3) 456 Census tracts. stage, useful explore data geographic space variable space understand clusters differ one another. can assign cluster ID original dataset new column map geom_sf().\nFigure 8.13: Map geodemographic clusters Dallas-Fort Worth\nnotable geographic patterns clusters evident map, even viewer local knowledge Dallas-Fort Worth region. Cluster 1 represents rural communities edges metropolitan area, whereas Cluster 6 tends located core counties Tarrant Dallas well higher-density tracts outer counties. Cluster 2 covers big-city downtowns Fort Worth Dallas along scattering suburban tracts.useful companion visualization map color-coded scatterplot using two principal components PCA dataset. use two components discussed “indices” Section 8.2.5: PC1, gradient affluent/older/white lower-income/younger/nonwhite, PC2, represents areas high population densities educational attainment low end lower-density, less educated areas high end. Given data density, ggplotly() plotly package convert scatterplot graphic interactive legend, allowing analyst turn cluster groups .\nFigure 8.14: Interactive scatterplot PC1 PC3 colored cluster\nclusters overlap one another degree, occupies distinct feature space. Double-click cluster legend isolate . Cluster 1, covers much rural fringe Dallas-Fort Worth, scores high “rurality” index PC2 (low density / educational attainment) modestly negative “diversity” index PC1. Cluster 2, includes two downtowns, scores lower “rurality” index PC2 scores higher “diversity” index PC1. geodemographic analyst may adopt visualization approaches explore proposed typology greater depth, aim produce informative “labels” cluster.","code":"\nset.seed(1983)\n\ndfw_kmeans <- dfw_pca %>%\n  st_drop_geometry() %>%\n  select(PC1:PC8) %>%\n  kmeans(centers = 6)\n\ntable(dfw_kmeans$cluster)## \n##   1   2   3   4   5   6 \n## 456 193 172  83 228 427\ndfw_clusters <- dfw_pca %>%\n  mutate(cluster = as.character(dfw_kmeans$cluster))\n\nggplot(dfw_clusters, aes(fill = cluster)) + \n  geom_sf(size = 0.1) + \n  scale_fill_brewer(palette = \"Set1\") + \n  theme_void() + \n  labs(fill = \"Cluster \")\nlibrary(plotly)\n\ncluster_plot <- ggplot(dfw_clusters, \n                       aes(x = PC1, y = PC2, color = cluster)) + \n  geom_point() + \n  scale_color_brewer(palette = \"Set1\") + \n  theme_minimal()\n\nggplotly(cluster_plot) %>%\n  layout(legend = list(orientation = \"h\", y = -0.15, \n                       x = 0.2, title = \"Cluster\"))"},{"path":"modeling-us-census-data.html","id":"spatial-clustering-regionalization","chapter":"8 Modeling US Census data","heading":"8.5.2 Spatial clustering & regionalization","text":"geodemographic classification outlined previous section offers useful methodology identifying similar types Census tracts varying parts metropolitan region. However, approach aspatial take geographic properties Census tracts account. applications, analyst may want generate meaningful clusters constrained neighboring contiguous areas. application workflow might sales territory generation, sales representatives assigned communities local market knowledge also want minimize overall travel time.suite regionalization algorithms available adapt clustering approach introducing spatial constraints. spatial constraint might additional requirement algorithm minimize overall geographic distance observations, even derived clusters must geographically contiguous. latter scenario explored subsection.workflow illustrates SKATER algorithm (Assunção et al. 2006), acronym stands “Spatial ’K’luster Analysis Tree Edge Removal.” algorithm implemented R skater() function spdep package, also available PySAL, GeoDa, ArcGIS “Spatially Constrained Multivariate Clustering” tool.SKATER relies concept minimum spanning trees, connectivity graph drawn observations dataset graph edges weighted attribute similarity observations. graph “pruned” removing edges connect observations similar one another.setup SKATER involves similar steps clustering algorithms used Chapter 7, queens-case contiguity weights matrix generated. key differences include use costs - represent differences neighbors based input set variables, example principal components 1 8 - use binary weights matrix style = \"B\".weights generated, minimum spanning tree created mstree(), used call skater().ncuts parameter dictates many times algorithm prune minimum spanning tree; value 7 create 8 groups. crit parameter used determine minimum number observations per group; , set value 10, requiring region least 10 Census tracts.solution can extracted assigned spatial dataset, visualized geom_sf():\nFigure 8.15: Map contiguous regions derived SKATER algorithm\nalgorithm partitioned data eight contiguous regions. regions largely geographic nature (region 5 covers northwestern portion metropolitan area, whereas region 2 covers east south), also incorporate demographic variations data. example, region 6 covers downtown uptown Dallas along Bishop Arts neighborhood; represent highest-density traditionally “urban” parts metropolitan area. Additionally, region 4 represents “northeast Tarrant County” suburban community along similar suburbs Denton County, socially meaningful sub-region north Texas.","code":"\nlibrary(spdep)\n\ninput_vars <- dfw_pca %>%\n  select(PC1:PC8) %>%\n  st_drop_geometry() %>%\n  as.data.frame() \n\nskater_nbrs <- poly2nb(dfw_pca, queen = TRUE)\ncosts <- nbcosts(skater_nbrs, input_vars)\nskater_weights <- nb2listw(skater_nbrs, costs, style = \"B\")\nmst <- mstree(skater_weights)\n\nregions <- skater(\n  mst[,1:2], \n  input_vars, \n  ncuts = 7,\n  crit = 10\n)\ndfw_clusters$region <- as.character(regions$group)\n\nggplot(dfw_clusters, aes(fill = region)) + \n  geom_sf(size = 0.1) + \n  scale_fill_brewer(palette = \"Set1\") + \n  theme_void()"},{"path":"modeling-us-census-data.html","id":"exercises-6","chapter":"8 Modeling US Census data","heading":"8.6 Exercises","text":"Identify different region United States interest . Complete following tasks:Acquire race/ethnicity data tidycensus chosen region compute dissimilarity index. segregation chosen region compare urban areas California?Reproduce regression modeling workflow outlined chapter chosen region. residual spatial autocorrelation , less, issue region Dallas-Fort Worth?Create geodemographic classification region using sample code chapter. typology ’ve generated resemble Dallas-Fort Worth, differ?","code":""},{"path":"introduction-to-census-microdata.html","id":"introduction-to-census-microdata","chapter":"9 Introduction to Census microdata","heading":"9 Introduction to Census microdata","text":"previous chapters book focus aggregate-level analysis US Census Bureau data. However, analyses limited pre-tabulated estimates provided Census Bureau. estimates voluminous, may include level detail required researchers, limited analyses appropriate aggregate-level data. turn, many researchers turn Census microdata, anonymized individual-level Census records, help answer demographic questions. 2020, tidycensus added support American Community Survey microdata along series tools assist analysis datasets. next two chapters provide overview functionality tidycensus help users get started analyzing modeling ACS microdata appropriately.","code":""},{"path":"introduction-to-census-microdata.html","id":"what-is-microdata","chapter":"9 Introduction to Census microdata","heading":"9.1 What is “microdata?”","text":"Microdata refer individual-level data made available researchers. many cases, microdata reflect responses surveys de-identified anonymized, prepared datasets include rich detail survey responses. US Census microdata available decennial Census American Community Survey; datasets, named Public Use Microdata Series (PUMS), allow detailed cross-tabulations available aggregated data.ACS PUMS available, like aggregate data, 1-year 5-year versions. 1-year PUMS covers 1 percent US population, whereas 5-year PUMS covers 5 percent; means microdata represent smaller subset US population regular ACS. Public use microdata downloads available bulk Census FTP server data.census.gov’s MDAT tool.Census Bureau also operates network Federal Statistical Research Data Centers (FSRDCs) around country grant access microdata larger sample sizes greater demographic detail. work one centers, researchers must get special government clearance approved proposal US Census Bureau. following chapter focus public use microdata product, much accessible researchers analysts.","code":""},{"path":"introduction-to-census-microdata.html","id":"microdata-resources-ipums","chapter":"9 Introduction to Census microdata","heading":"9.1.1 Microdata resources: IPUMS","text":"One popular comprehensive repositories research microdata University Minnesota’s IPUMS project (Ruggles et al. 2020). IPUMS includes Decennial US Census ACS microdata (IPUMS USA), microdata Current Population Survey (IPUMS CPS), 100 countries around world (IPUMS International).\nFigure 9.1: IPUMS home page\nIPUMS releases microdata harmonized, means changing variable definitions time aligned IPUMS team allow coherent longitudinal analysis. Using IPUMS requires signing account making request web interface, downloading data extract; API development. IPUMS data products covered detail Chapter 11, also introduce ipumsr R package (Ellis Burk 2020) working IPUMS data R.","code":""},{"path":"introduction-to-census-microdata.html","id":"microdata-and-the-census-api","chapter":"9 Introduction to Census microdata","heading":"9.1.2 Microdata and the Census API","text":"migration US Census data American FactFinder data.census.gov tool integrated Census Bureau’s data download interface API. Census Bureau’s MDAT tool allows flat file downloads microdata along API queries microdata, marking first time microdata available via API.\nFigure 9.2: MDAT tool data.census.gov\nmeans microdata can accessed httr::GET() requests R, also made ACS microdata accessible tidycensus. 2020, tidycensus released range features support ACS microdata R users; functionality covered remainder chapter.","code":""},{"path":"introduction-to-census-microdata.html","id":"using-microdata-in-tidycensus","chapter":"9 Introduction to Census microdata","heading":"9.2 Using microdata in tidycensus","text":"American Community Survey microdata available tidycensus using get_pums() function, communicates Census API much like tidycensus functions returns PUMS data. Given unique properties Census microdata different structure individual-level records opposed aggregate data, data returned get_pums() differs tidycensus functions. section covers basics requesting microdata extracts tidycensus.","code":""},{"path":"introduction-to-census-microdata.html","id":"basic-usage-of-get_pums","chapter":"9 Introduction to Census microdata","heading":"9.2.1 Basic usage of get_pums()","text":"get_pums() requires specifying one variables state ’d like request data. national-level analyses, state = '' can get data entire USA iterating US states, data can take time download depending user’s internet connection. get_pums() function defaults 5-year ACS survey = \"acs5\"; 1-year ACS data available survey = \"acs1\". time writing, data available 2006 2019 1-year ACS 2005-2009 2016-2020 5-year ACS.Let’s take look first example using get_pums() request microdata Wyoming 1-year 2019 ACS information sex, age (AGEP), household type (HHT).\nTable 9.1: 1-year ACS PUMS data Wyoming\nfunction returns just 6,000 rows data requested variables columns. However, variables also returned request; default variables covered .","code":"\nlibrary(tidycensus)\n\nwy_pums <- get_pums(\n  variables = c(\"SEX\", \"AGEP\", \"HHT\"),\n  state = \"WY\",\n  survey = \"acs1\",\n  year = 2019\n)"},{"path":"introduction-to-census-microdata.html","id":"understanding-default-data-from-get_pums","chapter":"9 Introduction to Census microdata","heading":"9.2.2 Understanding default data from get_pums()","text":"get_pums() returns technical variables default without user needing request specifically. technical variables essential uniquely identifying observations dataset eventually performing analysis modeling. default technical variables include:SERIALNO: serial number uniquely identifies households sample;SERIALNO: serial number uniquely identifies households sample;SPORDER: order person household, combined SERIALNO uniquely identifies person;SPORDER: order person household, combined SERIALNO uniquely identifies person;WGTP: household weight;WGTP: household weight;PWGTP: person weight;PWGTP: person weight;ST: state FIPS code.ST: state FIPS code.Given PUMS data sample US population, weights columns must used analysis. general terms, can interpret weights “number observations general population represented particular row dataset.” turn, row PWGTP value 50 represents 50 people Wyoming demographic characteristics “person” row.Inferences population characteristics can made summing weights columns. example, let’s say want get estimate number people Wyoming 50 years old 2019, compare total population Wyoming. can filter dataset rows match condition AGEP == 50, sum PWGTP column.data suggest 578,759 people Wyoming 2019, 4,756 50 years old. course, estimate subject margin error; topic error calculations PUMS data covered next chapter.important note get_pums() returns two separate weights columns: one households one persons. Let’s take look single household Wyoming dataset examine .household includes woman aged 40, man aged 45, two children: girl aged 8 boy aged 5. HHT value 1, tells us married-couple household. Notably, household weight value, WGTP, identical household members, whereas person weight value, PWGTP, .Microdata retrieved Census API hybrid household-level data person-level data, means analysts need take care use appropriate weights filters household-level person-level analyses. example, determine number households Wyoming, dataset filtered records SPORDER column equal 1 summed WGTP column. Persons living group quarters excluded automatically household weight 0.Housing unit rather simply household-level analyses introduce additional level complexity, housing units can occupied vacant. Vacant housing units returned different format Census API makes special case tidycensus. return vacant housing units along person household records, use argument return_vacant = TRUE.\nTable 9.2: 1-year ACS PUMS data vacant housing units\nVacant housing units included dataset, person-level characteristics (due lack occupancy) person-level variables like age sex values NA.","code":"\nlibrary(tidyverse)\n\nwy_age_50 <- filter(wy_pums, AGEP == 50)\n\nprint(sum(wy_pums$PWGTP))## [1] 578759\nprint(sum(wy_age_50$PWGTP))## [1] 4756\nwy_hh_example <- filter(wy_pums, SERIALNO == \"2019HU0456721\")\n\nwy_hh_example## # A tibble: 4 × 8\n##   SERIALNO      SPORDER  WGTP PWGTP  AGEP ST    HHT   SEX  \n##   <chr>           <dbl> <dbl> <dbl> <dbl> <chr> <chr> <chr>\n## 1 2019HU0456721       1   146   146    40 56    1     2    \n## 2 2019HU0456721       2   146   132    45 56    1     1    \n## 3 2019HU0456721       3   146    94     8 56    1     2    \n## 4 2019HU0456721       4   146   154     5 56    1     1\nwy_households <- filter(wy_pums, SPORDER == 1)\n\nsum(wy_households$WGTP)## [1] 233126\nwy_with_vacant <- get_pums(\n  variables = c(\"SEX\", \"AGEP\", \"HHT\"),\n  state = \"WY\",\n  survey = \"acs1\",\n  year = 2019,\n  return_vacant = TRUE\n) %>%\n  arrange(VACS)## \nDownloading: 16 kB     \nDownloading: 16 kB     \nDownloading: 16 kB     \nDownloading: 16 kB     \nDownloading: 33 kB     \nDownloading: 33 kB     \nDownloading: 33 kB     \nDownloading: 33 kB     \nDownloading: 33 kB     \nDownloading: 33 kB     \nDownloading: 36 kB     \nDownloading: 36 kB     \nDownloading: 43 kB     \nDownloading: 43 kB     \nDownloading: 45 kB     \nDownloading: 45 kB     \nDownloading: 61 kB     \nDownloading: 61 kB     \nDownloading: 63 kB     \nDownloading: 63 kB     \nDownloading: 63 kB     \nDownloading: 63 kB     \nDownloading: 63 kB     \nDownloading: 63 kB     \nDownloading: 63 kB     \nDownloading: 63 kB     \nDownloading: 72 kB     \nDownloading: 72 kB     \nDownloading: 72 kB     \nDownloading: 72 kB     \nDownloading: 72 kB     \nDownloading: 72 kB     \nDownloading: 72 kB     \nDownloading: 72 kB     \nDownloading: 81 kB     \nDownloading: 81 kB     \nDownloading: 88 kB     \nDownloading: 88 kB     \nDownloading: 90 kB     \nDownloading: 90 kB     \nDownloading: 99 kB     \nDownloading: 99 kB     \nDownloading: 110 kB     \nDownloading: 110 kB     \nDownloading: 110 kB     \nDownloading: 110 kB     \nDownloading: 120 kB     \nDownloading: 120 kB     \nDownloading: 120 kB     \nDownloading: 120 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 140 kB     \nDownloading: 140 kB     \nDownloading: 140 kB     \nDownloading: 140 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 160 kB     \nDownloading: 160 kB     \nDownloading: 160 kB     \nDownloading: 160 kB     \nDownloading: 170 kB     \nDownloading: 170 kB     \nDownloading: 170 kB     \nDownloading: 170 kB     \nDownloading: 180 kB     \nDownloading: 180 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 210 kB     \nDownloading: 210 kB     \nDownloading: 210 kB     \nDownloading: 210 kB     \nDownloading: 220 kB     \nDownloading: 220 kB     \nDownloading: 220 kB     \nDownloading: 220 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 270 kB     \nDownloading: 270 kB     \nDownloading: 270 kB     \nDownloading: 270 kB     \nDownloading: 270 kB     \nDownloading: 270 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 8.6 kB     \nDownloading: 8.6 kB     \nDownloading: 13 kB     \nDownloading: 13 kB     \nDownloading: 13 kB     \nDownloading: 13 kB     \nDownloading: 13 kB     \nDownloading: 13 kB"},{"path":"introduction-to-census-microdata.html","id":"working-with-pums-variables","chapter":"9 Introduction to Census microdata","heading":"9.3 Working with PUMS variables","text":"ACS PUMS dataset include tens thousands variables choices like aggregate counterpart, nonetheless includes variables variable codes can difficult understand without data dictionary. Wyoming example , interpretation AGEP column age straightforward. HHT, household type, SEX, sex, coded integers represented character strings. help users understand meanings codes, tidycensus includes built-dataset, pums_variables, can viewed, filtered, browsed.","code":""},{"path":"introduction-to-census-microdata.html","id":"variables-available-in-the-acs-pums","chapter":"9 Introduction to Census microdata","heading":"9.3.1 Variables available in the ACS PUMS","text":"data dictionaries decennial Census aggregate ACS obtained load_variables(), advisable browse PUMS data dictionary, pums_variables, View() function RStudio.pums_variables long-form dataset organizes specific value codes variable know can get get_pums(). ’ll use information var_code column fetch variables, pay close attention var_label, val_min, val_max, val_label, data_type columns. columns interpreted follows:var_code gives variable codes supplied variables parameter (character vector) get_pums(). variables represented columns output dataset.var_label informative description variable’s topic.data_type one \"chr\", categorical variables returned R character strings, \"num\", variables returned numeric.val_min val_max provide information meaning data values. categorical variables, two columns ; numeric variables, give possible range data values.val_label contains value labels, particularly important understanding content categorical variables.","code":"\nView(pums_variables)"},{"path":"introduction-to-census-microdata.html","id":"recoding-pums-variables","chapter":"9 Introduction to Census microdata","heading":"9.3.2 Recoding PUMS variables","text":"typical tidycensus workflow covered earlier book involves browsing appropriate data dictionary, choosing variable IDs, using IDs scripts workflows. Analysts likely follow process get_pums(), can also use argument recode = TRUE return additional contextual information requested data. recode = TRUE instructs get_pums() append recoded columns returned dataset based information available pums_variables. Let’s take look Wyoming example recode = TRUE.\nTable 9.3: Recoded PUMS data Wyoming\nNote dataset returns three new columns: ST_label, HHT_label, SEX_label include longer informative descriptions value labels. columns returned ordered factors preserve original ordering columns independent alphabetical order. Numeric columns like AGEP recoded data values reflect numbers, categorical label.","code":"\nwy_pums_recoded <- get_pums(\n  variables = c(\"SEX\", \"AGEP\", \"HHT\"),\n  state = \"WY\",\n  survey = \"acs1\",\n  year = 2019,\n  recode = TRUE\n)"},{"path":"introduction-to-census-microdata.html","id":"using-variables-filters","chapter":"9 Introduction to Census microdata","heading":"9.3.3 Using variables filters","text":"PUMS datasets, especially 5-year ACS, can get quite large. Even users speedy internet connections need patient downloading millions records Census API potentially risk internet hiccups. subsets data required analysis, variables_filter argument can return subset data API, reducing long download times.variables_filter argument supplied named list variable names (can quoted unquoted) paired data value vector data values requested API. “filter” works passing special query Census API return subset data, meaning entire dataset need first downloaded filtered R side. leads substantial time savings targeted queries.example , Wyoming request modified variables_filter return women (SEX = 2) ages 30 49, time 5-year ACS PUMS.\nTable 9.4: Filtered extract Wyoming PUMS data\nreturned dataset reflects filter request, data values AGEP column ranging 30 49 SEX column including 2, female.","code":"\nwy_pums_filtered <- get_pums(\n  variables = c(\"SEX\", \"AGEP\", \"HHT\"),\n  state = \"WY\",\n  survey = \"acs5\",\n  variables_filter = list(\n    SEX = 2,\n    AGEP = 30:49\n  ),\n  year = 2019\n)"},{"path":"introduction-to-census-microdata.html","id":"public-use-microdata-areas-pumas","chapter":"9 Introduction to Census microdata","heading":"9.4 Public Use Microdata Areas (PUMAs)","text":"One steps Census Bureau takes preserve anonymity PUMS datasets limiting geographical detail data. Granular Census geographical information like Census tract block group residence individuals PUMS samples available. said, geographical information available PUMS samples form Public Use Microdata Area, PUMA.","code":""},{"path":"introduction-to-census-microdata.html","id":"what-is-a-puma","chapter":"9 Introduction to Census microdata","heading":"9.4.1 What is a PUMA?","text":"Public Use Microdata Areas (PUMAs) smallest available geographies records identifiable PUMS datasets. PUMAs redrawn decennial US Census, typically home 100,000 - 200,000 people drawn, although may much larger end Census cycle. large cities, PUMA represent collection nearby neighborhoods; rural areas, might represent several counties across large area state.time writing, PUMA geographies correspond 2010 Census definitions, even. PUMAs redrawn decennial US Census, release lags decennial Census couple years. determine appropriate PUMA geographies, Census Bureau consults State Data Centers (SDCs) incorporates suggested revisions geographies. 2020 PUMAs, process takes place 2021 calendar year. means updated PUMA geographies, incorporation PUMS datasets, planned public release summer 2022. information, visit https://www.census.gov/programs-surveys/geography/guidance/geo-areas/pumas/2020pumas.html.PUMA geographies can obtained reviewed pumas() function tigris package. Let’s take look PUMA geographies state Wyoming:\nFigure 9.3: Basic map PUMAs Wyoming\nfive PUMAs Wyoming, largely covering large rural areas state, although smallest PUMA area covers urban southeast corner state. returned object includes NAME10 column informative description PUMAs:denser urban area, PUMAs reflect subsections major cities drawn attempts reflect meaningful local areas. New York City, example, PUMAs drawn align recognized community districts city.\nFigure 9.4: Map PUMAs New York City\nnames NYC’s PUMAs reflect community districts geographic data returned tigris.","code":"\nlibrary(tigris)\noptions(tigris_use_cache = TRUE)\n\nwy_pumas <- pumas(state = \"WY\", cb = TRUE, year = 2019)\n\nggplot(wy_pumas) + \n  geom_sf() + \n  theme_void()\nwy_pumas$NAME10## [1] \"Sheridan, Park, Teton, Lincoln & Big Horn Counties\"                                 \n## [2] \"Sweetwater, Fremont, Uinta, Sublette & Hot Springs Counties--Wind River Reservation\"\n## [3] \"Campbell, Goshen, Platte, Johnson, Washakie, Weston, Crook & Niobrara Counties\"     \n## [4] \"Natrona, Carbon & Converse Counties\"                                                \n## [5] \"Laramie & Albany Counties\"\nnyc_pumas <- pumas(state = \"NY\", cb = TRUE, year = 2019) %>%\n  filter(str_detect(NAME10, \"NYC\"))\n\nggplot(nyc_pumas) + \n  geom_sf() + \n  theme_void()\nnyc_pumas$NAME10[1:5]## [1] \"NYC-Manhattan Community District 11--East Harlem\"                                  \n## [2] \"NYC-Manhattan Community District 8--Upper East Side\"                               \n## [3] \"NYC-Bronx Community District 11--Pelham Parkway, Morris Park & Laconia\"            \n## [4] \"NYC-Manhattan Community District 9--Hamilton Heights, Manhattanville & West Harlem\"\n## [5] \"NYC-Brooklyn Community District 12--Borough Park, Kensington & Ocean Parkway\""},{"path":"introduction-to-census-microdata.html","id":"working-with-pumas-in-pums-data","chapter":"9 Introduction to Census microdata","heading":"9.4.2 Working with PUMAs in PUMS data","text":"PUMA information available variable code PUMA get_pums(). Use PUMA like variable return information PUMA residence individual records.\nTable 9.5: Wyoming microdata PUMA age information\nPUMA IDs replicated across states, PUMA column combined ST column uniquely identify PUMAs performing multi-state analyses.puma argument get_pums() can also used obtain data specific PUMA multiple PUMAs. Like variables_filter parameter, puma uses query API side reduce long download times users interested geographical subset data.\nTable 9.6: Microdata PUMA 00500 Wyoming\nmulti-state geographical queries, puma argument must adapted slightly due aforementioned possibility PUMA IDs replicated across states. perform multi-state query PUMA, specify state = \"multiple\" pass named vector state/PUMA pairs puma parameter.\nTable 9.7: Data PUMA 00500 Wyoming PUMA 05001 Utah\nreturned data include neighboring areas Wyoming Utah.","code":"\nwy_age_by_puma <- get_pums(\n  variables = c(\"PUMA\", \"AGEP\"),\n  state = \"WY\",\n  survey = \"acs5\",\n  year = 2019\n)\nwy_puma_subset <- get_pums(\n  variables = \"AGEP\",\n  state = \"WY\",\n  survey = \"acs5\",\n  puma = \"00500\",\n  year = 2019\n)\ntwostate_puma_subset <- get_pums(\n  variables = \"AGEP\",\n  state = \"multiple\",\n  survey = \"acs5\",\n  puma = c(\"WY\" = \"00500\", \"UT\" = \"05001\"),\n  year = 2019\n)"},{"path":"introduction-to-census-microdata.html","id":"exercises-7","chapter":"9 Introduction to Census microdata","heading":"9.5 Exercises","text":"Try requesting PUMS data using get_pums() , state Wyoming.Try requesting PUMS data using get_pums() , state Wyoming.Use pums_variables dataset browse available variables PUMS. Create custom query get_pums() request data variables ’ve used examples.Use pums_variables dataset browse available variables PUMS. Create custom query get_pums() request data variables ’ve used examples.","code":""},{"path":"analyzing-census-microdata.html","id":"analyzing-census-microdata","chapter":"10 Analyzing Census microdata","heading":"10 Analyzing Census microdata","text":"major benefit using individual-level microdata returned get_pums() ability create detailed, granular estimates ACS data. aggregate ACS data available get_acs() includes tens thousands indicators choose , researchers analysts still may interested cross-tabulations available aggregate files. Additionally, microdata helps researchers design statistical models assess demographic relationships individual level way possible aggregate data.Analysts must pay careful attention structure PUMS datasets order produce accurate estimates handle errors appropriately. PUMS datasets weighted samples, person household considered unique individual, rather representative multiple persons households. turn, analyses tabulations using PUMS data must use appropriate tools handling weighting variables accurately produce estimates. Fortunately, tidyverse tools like dplyr, covered elsewhere book, excellent producing tabulations handling survey weightsAs covered Chapter 3, data American Community Survey based sample turn characterized error. means ACS data acquired get_pums() similarly characterized error, can substantial cross-tabulations highly specific. Fortunately, US Census Bureau provides replicate weights help analysts generate standard errors around tabulated estimates PUMS data take account complex structure survey sample. working replicate weights traditionally cumbersome analysts, tidycensus help survey (Lumley 2010) srvyr (Freedman Ellis Schneider 2021) R packages integrated tools handling replicate weights correctly estimating standard errors tabulating modeling data. workflows covered later chapter.","code":""},{"path":"analyzing-census-microdata.html","id":"pums-data-and-the-tidyverse","chapter":"10 Analyzing Census microdata","heading":"10.1 PUMS data and the tidyverse","text":"discussed Chapter 9, get_pums() automatically returns data household (WGTP) person (PWGTP) weights. weights can loosely interpreted number households persons represented individual row PUMS data. Appropriate use weights columns essential tabulating accurate estimates population characteristics PUMS data. Fortunately, weighted tabulations work quite well within familiar tidyverse workflows, covered Chapter 3.","code":""},{"path":"analyzing-census-microdata.html","id":"basic-tabulation-of-weights-with-tidyverse-tools","chapter":"10 Analyzing Census microdata","heading":"10.1.1 Basic tabulation of weights with tidyverse tools","text":"Let’s get basic sample PUMS data 2016-2020 ACS Mississippi information sex age.Let’s take quick look data:\nTable 10.1: PUMS data Mississippi\nlearned Chapter 10, number people Mississippi can tabulated summing person-weight column:can perform similar calculations tidyverse tools. count() function dplyr package performs simple tabulation data. optional wt argument count() allows specify weight column, case person-weight.count() additional benefit allowing specification one columns grouped tabulated. example, tabulate data unique values age sex Mississippi. wt argument count() specifies PWGTP column appropriate weight data tabulation.can also perform custom analyses, tabulating number people age 65 sex Mississippi. involves specifying filter condition retain rows records age 65 , tabulating sex.can use get_acs() check answer:notice tabulations close ACS estimates available get_acs(), well within margin error. tabulations microdata, important remember tabulating data based smaller subsample information available aggregate ACS estimates. turn, US Census Bureau reminds us (American Community Survey Office 2021):PUMS data consist subset full ACS sample, tabulations ACS PUMS match published tables ACS data.Analysts often want use PUMS data tabulated aggregate ACS data tandem appropriate, data type offers complimentary strengths. aggregate ACS data based larger sample, data aggregations preferable produced PUMS data. However, PUMS data offer ability compute detailed cross-tabulations available aggregate ACS tables fit models demographic relationships individual level. Examples follow chapter.","code":"\nlibrary(tidycensus)\nlibrary(tidyverse)\n\nms_pums <- get_pums(\n  variables = c(\"SEX\", \"AGEP\"),\n  state = \"MS\",\n  survey = \"acs5\",\n  year = 2020,\n  recode = TRUE\n)\nsum(ms_pums$PWGTP)## [1] 2981835\nms_pums %>% count(wt = PWGTP)## # A tibble: 1 × 1\n##         n\n##     <dbl>\n## 1 2981835\nms_pums %>%\n  count(SEX_label, AGEP, wt = PWGTP) ## # A tibble: 186 × 3\n##    SEX_label  AGEP     n\n##    <ord>     <dbl> <dbl>\n##  1 Male          0 18111\n##  2 Male          1 19206\n##  3 Male          2 18507\n##  4 Male          3 18558\n##  5 Male          4 20054\n##  6 Male          5 17884\n##  7 Male          6 18875\n##  8 Male          7 18775\n##  9 Male          8 19316\n## 10 Male          9 20866\n## # … with 176 more rows\nms_pums %>%\n  filter(AGEP >= 65) %>%\n  count(SEX, wt = PWGTP)## # A tibble: 2 × 2\n##   SEX        n\n##   <chr>  <dbl>\n## 1 1     206504\n## 2 2     267707\nget_acs(geography = \"state\",\n        state = \"MS\",\n        variables = c(\"DP05_0030\", \"DP05_0031\"),\n        year = 2020)## # A tibble: 2 × 5\n##   GEOID NAME        variable  estimate   moe\n##   <chr> <chr>       <chr>        <dbl> <dbl>\n## 1 28    Mississippi DP05_0030   206518   547\n## 2 28    Mississippi DP05_0031   267752   466"},{"path":"analyzing-census-microdata.html","id":"group-wise-data-tabulation","chapter":"10 Analyzing Census microdata","heading":"10.1.2 Group-wise data tabulation","text":"combined tidyverse tools introduced Chapter 3, PUMS data can produce highly detailed estimates available regular aggregate ACS. example acquires data rent burden, family type, race/ethnicity examine intersections variables households Mississippi. PUMA variable also included use later chapter.guiding research question follows: rent burden vary race/ethnicity household type Mississippi households? requires obtaining data rent burden (gross rent percentage household income) variable GRPIP; race ethnicity variables RAC1P HISP; household type variable HHT. variables_filter argument used filter sample renter-occupied households paying cash rent, speeding download times.can take quick look data:\nTable 10.2: Household microdata Mississippi\nanalyze rent burdens respect marital status race/ethnicity householder, useful additional recoding using dplyr’s case_when() function. new race_ethnicity column identify householders general categories, married column identify whether household married-couple household.information can summarized respect household weight variable WGTP rent burden variable GRPIP within group_by() %>% summarize() workflow. dataset filtered non-Hispanic white, non-Hispanic Black, Hispanic householders focus groups, grouped race/ethnicity marital status. Within summarize() call, percentage subgroup paying 40 percent household incomes rent calculated summing household weight column WGTP, filtering households rent burdens 40 percent numerator.can now check result:\nTable 10.3: Tabulated PUMS data Mississippi\ndemographic group example largest rent burden Black, married; nearly 41 percent households group pay 40 percent incomes gross rent. least rent-burdened group White, Married, value 13 percent. three racial/ethnic groups, distinctive financial advantage married-couple households non-married households; particularly pronounced Black householders.","code":"\nhh_variables <- c(\"PUMA\", \"GRPIP\", \"RAC1P\", \n                  \"HISP\", \"HHT\")\n\nms_hh_data <- get_pums(\n  variables = hh_variables, \n  state = \"MS\",\n  year = 2020,\n  variables_filter = list(\n    SPORDER = 1,\n    TEN = 3\n  ),\n  recode = TRUE\n)\nms_hh_recoded <- ms_hh_data %>%\n  mutate(\n    race_ethnicity = case_when(\n      HISP != \"01\" ~ \"Hispanic\",\n      HISP == \"01\" & RAC1P == \"1\" ~ \"White\",\n      HISP == \"01\" & RAC1P == \"2\" ~ \"Black\",\n      TRUE ~ \"Other\"\n    ),\n    married = case_when(\n      HHT == \"1\" ~ \"Married\",\n      TRUE ~ \"Not married\"\n    )\n  )\nms_hh_summary <- ms_hh_recoded %>%\n  filter(race_ethnicity != \"Other\") %>%\n  group_by(race_ethnicity, married) %>%\n  summarize(\n    prop_above_40 = sum(WGTP[GRPIP >= 40]) / sum(WGTP)\n  )"},{"path":"analyzing-census-microdata.html","id":"mapping-pums-data","chapter":"10 Analyzing Census microdata","heading":"10.2 Mapping PUMS data","text":"previous example, see rent burdens Black, unmarried households particularly acute Mississippi. follow-question may involve examination trend varies geographically. discussed previous chapter, granular geography available PUMS data PUMA, generally includes 100,000-200,000 people. PUMA geographies available tigris package function pumas().\nFigure 10.1: Basic plot PUMAs Mississippi\ngeographical visualization rent burdens Mississippi requires slight adaptation code. Instead returning comparative table, dataset also grouped PUMA column filtered combination variables represent group analyst wants visualize. case, focus unmarried Black households PUMA.output dataset one row per PUMA suitable joining spatial dataset visualization.\nFigure 10.2: Map rent-burdened unmarried Black household share PUMA Mississippi\nmap illustrates geographic variations indicator interest. particular, unmarried Black households particularly rent-burdened along Gulf Coast, half households paying least 40 percent household incomes gross rent. least rent-burdened areas demographic group suburban PUMAs around Jackson.","code":"\nlibrary(tigris)\nlibrary(tmap)\noptions(tigris_use_cache = TRUE)\n\nms_pumas <- pumas(\"MS\", year = 2020)\n\nplot(ms_pumas$geometry)\nms_data_for_map <- ms_hh_recoded %>%\n  group_by(race_ethnicity, married, PUMA) %>%\n  summarize(\n    percent_above_40 = 100 * (sum(WGTP[GRPIP >= 40]) / sum(WGTP))\n  ) %>%\n  filter(race_ethnicity == \"Black\",\n         married == \"Not married\")\nlibrary(tmap)\n\njoined_pumas <- ms_pumas %>%\n  left_join(ms_data_for_map, by = c(\"PUMACE10\" = \"PUMA\"))\n\ntm_shape(joined_pumas) + \n  tm_polygons(col = \"percent_above_40\", \n              palette = \"Reds\",\n              title = \"% rent-burdened\\nunmarried Black households\") + \n  tm_layout(legend.outside = TRUE,\n            legend.outside.position = \"right\")"},{"path":"analyzing-census-microdata.html","id":"survey-design-and-the-acs-pums","chapter":"10 Analyzing Census microdata","heading":"10.3 Survey design and the ACS PUMS","text":"earlier chapters addressed, American Community Survey based sample US population turn subject sampling error. becomes particularly acute dealing small sub-populations like explored PUMA level previous section. Given PUMS data individual-level records aggregates, standard errors turn margins error must computed analyst. correctly requires accounting complex sample design ACS. Fortunately, tidycensus help survey srvyr packages includes tools assist tasks.","code":""},{"path":"analyzing-census-microdata.html","id":"getting-replicate-weights","chapter":"10 Analyzing Census microdata","heading":"10.3.1 Getting replicate weights","text":"Census Bureau recommends using Successive Difference Replication method compute standard errors around derived estimates PUMS data. calculate standard errors, Census Bureau publishes 80 “replicate weights” observation, representing either person (PWGTP1 PWGTP80) household (WGTP1 WGTP80) weights (American Community Survey Office 2021). formula computing standard error \\(SE\\) derived PUMS estimate \\(x\\) follows:\\[\nSE(x) = \\sqrt{\\frac{4}{80}\\sum\\limits_{r=1}^{80}(x_r-x)^2 }\n\\]\\(x\\) PUMS estimate \\(x_r\\) rth weighted estimate.respect SDR standard errors, PUMS documentation acknowledges (p. 12),Successive Difference Replication (SDR) standard errors margins error expected accurate generalized variance formulas (GVF) standard errors margins error, although may inconvenient users calculate.“inconvenience” generally due need download 80 additional weighting variables prepare equation written . rep_weights parameter get_pums() makes easier users retrieve replicate weights variables without request 80 directly. call get_pums(), analyst can use rep_weights = \"person\" person-weights, \"housing\" household weights, \"\" get sets.code re-downloads Mississippi rent burden dataset used , household replicate weights included.80 household replicate weights included dataset. key distinction code, however, housing tenure variable TEN included variables_filter argument, instead returning full sample households Mississippi. standard error estimation complex survey samples requires special methods subpopulations, covered .","code":"\nms_hh_replicate <- get_pums(\n  variables = c(\"TEN\", hh_variables), \n  state = \"MS\",\n  recode = TRUE,\n  year = 2020,\n  variables_filter = list(\n    SPORDER = 1\n  ),\n  rep_weights = \"housing\"\n)\nnames(ms_hh_replicate)##  [1] \"SERIALNO\"    \"SPORDER\"     \"GRPIP\"       \"PUMA\"        \"ST\"         \n##  [6] \"TEN\"         \"HHT\"         \"HISP\"        \"RAC1P\"       \"ST_label\"   \n## [11] \"TEN_label\"   \"HHT_label\"   \"HISP_label\"  \"RAC1P_label\" \"WGTP\"       \n## [16] \"PWGTP\"       \"WGTP1\"       \"WGTP2\"       \"WGTP3\"       \"WGTP4\"      \n## [21] \"WGTP5\"       \"WGTP6\"       \"WGTP7\"       \"WGTP8\"       \"WGTP9\"      \n## [26] \"WGTP10\"      \"WGTP11\"      \"WGTP12\"      \"WGTP13\"      \"WGTP14\"     \n## [31] \"WGTP15\"      \"WGTP16\"      \"WGTP17\"      \"WGTP18\"      \"WGTP19\"     \n## [36] \"WGTP20\"      \"WGTP21\"      \"WGTP22\"      \"WGTP23\"      \"WGTP24\"     \n## [41] \"WGTP25\"      \"WGTP26\"      \"WGTP27\"      \"WGTP28\"      \"WGTP29\"     \n## [46] \"WGTP30\"      \"WGTP31\"      \"WGTP32\"      \"WGTP33\"      \"WGTP34\"     \n## [51] \"WGTP35\"      \"WGTP36\"      \"WGTP37\"      \"WGTP38\"      \"WGTP39\"     \n## [56] \"WGTP40\"      \"WGTP41\"      \"WGTP42\"      \"WGTP43\"      \"WGTP44\"     \n## [61] \"WGTP45\"      \"WGTP46\"      \"WGTP47\"      \"WGTP48\"      \"WGTP49\"     \n## [66] \"WGTP50\"      \"WGTP51\"      \"WGTP52\"      \"WGTP53\"      \"WGTP54\"     \n## [71] \"WGTP55\"      \"WGTP56\"      \"WGTP57\"      \"WGTP58\"      \"WGTP59\"     \n## [76] \"WGTP60\"      \"WGTP61\"      \"WGTP62\"      \"WGTP63\"      \"WGTP64\"     \n## [81] \"WGTP65\"      \"WGTP66\"      \"WGTP67\"      \"WGTP68\"      \"WGTP69\"     \n## [86] \"WGTP70\"      \"WGTP71\"      \"WGTP72\"      \"WGTP73\"      \"WGTP74\"     \n## [91] \"WGTP75\"      \"WGTP76\"      \"WGTP77\"      \"WGTP78\"      \"WGTP79\"     \n## [96] \"WGTP80\""},{"path":"analyzing-census-microdata.html","id":"creating-a-survey-object","chapter":"10 Analyzing Census microdata","heading":"10.3.2 Creating a survey object","text":"replicate weights hand, analysts can turn suite tools R handling complex survey samples. survey package (Lumley 2010) standard handling types datasets R. recent srvyr package (Freedman Ellis Schneider 2021) wraps survey allow use tidyverse functions survey objects. packages return survey class object intelligently calculates standard errors data tabulated appropriate functions. tidycensus includes function, to_survey(), convert ACS microdata survey srvyr objects way incorporates recommended formula SDR standard error calculation replicate weights.to_survey() function returns original dataset object class tbl_svy svyrep.design minimal hassle.Note use filter() converting replicate weights dataset survey object subset data renter-occupied households paying cash rent. computing standard errors derived estimates using complex survey samples, necessary take entire structure sample account. turn, important first convert dataset survey object identify “subpopulation” model fit.analysis subpopulations, srvyr::filter() works like survey::subset() appropriate standard error estimation. data structure taken account calculating standard errors.","code":"\nlibrary(survey)\nlibrary(srvyr)\n\nms_hh_svy <- ms_hh_replicate %>%\n  to_survey(type = \"housing\", \n            design = \"rep_weights\") %>%\n  filter(TEN == 3)\n\nclass(ms_hh_svy)## [1] \"tbl_svy\"       \"svyrep.design\""},{"path":"analyzing-census-microdata.html","id":"calculating-estimates-and-errors-with-srvyr","chapter":"10 Analyzing Census microdata","heading":"10.3.3 Calculating estimates and errors with srvyr","text":"srvyr’s survey_*() family functions automatically calculates standard errors around tabulated estimates using tidyverse-equivalent functions. example, analogous use count() tabulate weighted data, survey_count() survey object also return appropriately-calculated standard errors.\nTable 10.4: Tabulated PUMS data household types Mississippi PUMA standard errors\nsurvey_count() function returns tabulations household type PUMA Mississippi along estimate’s standard error. srvyr package can also accommodate complex workflows. adaptation rent burden analysis computed , using srvyr function survey_mean().\nTable 10.5: Derived estimates PUMS data standard errors\nderived estimates , srvyr workflow also returns standard errors.","code":"\nms_hh_svy %>% \n  survey_count(PUMA, HHT_label)\nms_svy_summary <- ms_hh_svy %>%\n  mutate(\n    race_ethnicity = case_when(\n      HISP != \"01\" ~ \"Hispanic\",\n      HISP == \"01\" & RAC1P == \"1\" ~ \"White\",\n      HISP == \"01\" & RAC1P == \"2\" ~ \"Black\",\n      TRUE ~ \"Other\"\n    ),\n    married = case_when(\n      HHT == \"1\" ~ \"Married\",\n      TRUE ~ \"Not married\"\n    ),\n    above_40 = GRPIP >= 40\n  ) %>%\n  filter(race_ethnicity != \"Other\") %>%\n  group_by(race_ethnicity, married) %>%\n  summarize(\n    prop_above_40 = survey_mean(above_40)\n  )  "},{"path":"analyzing-census-microdata.html","id":"converting-standard-errors-to-margins-of-error","chapter":"10 Analyzing Census microdata","heading":"10.3.4 Converting standard errors to margins of error","text":"convert standard errors margins error around derived PUMS estimates, analysts multiply standard errors following coefficients:90 percent confidence level: 1.64590 percent confidence level: 1.64595 percent confidence level: 1.9695 percent confidence level: 1.9699 percent confidence level: 2.5699 percent confidence level: 2.56Computing margins error around derived ACS estimates PUMS data allows familiar visualization uncertainty ACS shown earlier book. example calculates margins error 90 percent confidence level rent burden estimates Mississippi, draws margin error plot illustrated Section 4.3.\nFigure 10.3: Margin error plot derived PUMS estimates\nplot effectively represents uncertainty associated estimates relatively small Hispanic population Mississippi.","code":"\nms_svy_summary_moe <- ms_svy_summary %>%\n  mutate(prop_above_40_moe = prop_above_40_se * 1.645,\n         label = paste(race_ethnicity, married, sep = \", \")) \n\nggplot(ms_svy_summary_moe, aes(x = prop_above_40, \n                               y = reorder(label, \n                                           prop_above_40))) +\n  geom_errorbar(aes(xmin = prop_above_40 - prop_above_40_moe, \n                     xmax = prop_above_40 + prop_above_40_moe)) +\n  geom_point(size = 3, color = \"navy\") +\n  labs(title = \"Rent burdened-households in Mississippi\",\n       x = \"2016-2020 ACS estimate (from PUMS data)\",\n       y = \"\",\n       caption = \"Rent-burdened defined when gross rent is 40 percent or more\\nof household income. Error bars represent a 90 percent confidence level.\") +\n  scale_x_continuous(labels = scales::percent) + \n  theme_grey(base_size = 12)"},{"path":"analyzing-census-microdata.html","id":"modeling-with-pums-data","chapter":"10 Analyzing Census microdata","heading":"10.4 Modeling with PUMS data","text":"rich complexity demographic data available PUMS samples allow estimation statistical models study wide range social processes. Like tabulation summary statistics PUMS data, however, statistical models use complex survey samples require special methods. Fortunately, methods incorporated srvyr survey packages.estimating model, data acquired get_pums() along appropriate replicate weights. example model whether individual labor force aged 25 49 changed residences past year function educational attainment, wages, age, class worker, family status Rhode Island.Even though model focus population labor force aged 25 49, variables_filter used full dataset needed appropriate model estimation. addressed next section.","code":"\nri_pums_to_model <- get_pums(\n  variables = c(\"PUMA\", \"SEX\", \"MIG\",\n                \"AGEP\", \"SCHL\", \"WAGP\", \n                \"COW\", \"ESR\", \"MAR\", \"NOC\"),\n  state = \"RI\",\n  survey = \"acs5\",\n  year = 2020,\n  rep_weights = \"person\"\n)"},{"path":"analyzing-census-microdata.html","id":"data-preparation","chapter":"10 Analyzing Census microdata","heading":"10.4.1 Data preparation","text":"Similar Section 8.2.3, perform feature engineering fitting model. largely involves recoding outcome variable predictors general categories assist ease interpretation. recoding workflows book, case_when() collapses categories.Given estimating logistic regression model binary outcome (whether individual migrant, migrated coded either 0 1. recoded variables used categorical predictors, parameter estimates refer probabilities migrated relative reference category (e.g. college graduates relative individuals graduated college).next step, subpopulation model estimated identified using filter(). focus individuals aged 25 49 employed earned wages past year.","code":"\nri_pums_recoded <- ri_pums_to_model %>%\n  mutate(\n    emp_type = case_when(\n      COW %in% c(\"1\", \"2\") ~ \"private\",\n      COW %in% c(\"3\", \"4\", \"5\") ~ \"public\",\n      TRUE ~ \"self\"\n      ), \n    child = case_when(\n      NOC > 0 ~ \"yes\",\n      TRUE ~ \"no\"\n    ),\n    married = case_when(\n      MAR == 1 ~ \"yes\",\n      TRUE ~ \"no\"\n    ),\n    college = case_when(\n      SCHL %in% as.character(21:24) ~ \"yes\",\n      TRUE ~ \"no\"\n    ),\n    sex = case_when(\n      SEX == 2 ~ \"female\",\n      TRUE ~ \"male\"\n    ),\n    migrated = case_when(\n      MIG == 1 ~ 0,\n      TRUE ~ 1\n    )\n  )\nri_model_svy <- ri_pums_recoded %>% \n  to_survey() %>%\n  filter(\n    ESR == 1,   # civilian employed\n    WAGP > 0,   # earned wages last year\n    AGEP >= 25,\n    AGEP <= 49\n  ) %>%\n  rename(age = AGEP, wages = WAGP)"},{"path":"analyzing-census-microdata.html","id":"fitting-and-evaluating-the-model","chapter":"10 Analyzing Census microdata","heading":"10.4.2 Fitting and evaluating the model","text":"family modeling functions survey package used modeling data survey design objects, take account replicate weights, survey design, subpopulation structure. example , use svyglm() function purpose. formula written using standard R formula notation, survey design object passed design parameter, family = quasibinomial() used fit logistic regression model.fit, can examine results:model identifies notable differences recent migrants relative non-migrants, controlling demographic factors. Males likely moved females, younger people relative older people subpopulation. Individuals children slightly stationary, whereas college-educated individuals sample likely moved. PUMAs included categorical predictor largely control geographic differences state; model identify substantive differences among Rhode Island PUMAs analysis.","code":"\nlibrary(survey)\n\nmigration_model <- svyglm(\n  formula = migrated ~ log(wages) + sex + age + emp_type + \n    child + married + college + PUMA,\n  design = ri_model_svy,\n  family = quasibinomial()\n) \nsummary(migration_model)## \n## Call:\n## svyglm(formula = migrated ~ log(wages) + sex + age + emp_type + \n##     child + married + college + PUMA, design = ri_model_svy, \n##     family = quasibinomial())\n## \n## Survey design:\n## Called via srvyr\n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)     1.489784   0.496118   3.003  0.00379 ** \n## log(wages)     -0.098089   0.047027  -2.086  0.04093 *  \n## sexmale         0.249331   0.056123   4.443 3.53e-05 ***\n## age            -0.068431   0.008195  -8.350 6.98e-12 ***\n## emp_typepublic -0.057571   0.099477  -0.579  0.56477    \n## emp_typeself   -0.243479   0.196835  -1.237  0.22055    \n## childyes       -0.192214   0.105508  -1.822  0.07309 .  \n## marriedyes     -0.141021   0.115814  -1.218  0.22776    \n## collegeyes      0.256121   0.094649   2.706  0.00869 ** \n## PUMA00102       0.098035   0.150894   0.650  0.51818    \n## PUMA00103       0.102302   0.162798   0.628  0.53195    \n## PUMA00104       0.187429   0.184095   1.018  0.31240    \n## PUMA00201       0.190723   0.135870   1.404  0.16516    \n## PUMA00300       0.288592   0.179487   1.608  0.11271    \n## PUMA00400      -0.329335   0.202088  -1.630  0.10801    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for quasibinomial family taken to be 10453.18)\n## \n## Number of Fisher Scoring iterations: 5"},{"path":"analyzing-census-microdata.html","id":"exercises-8","chapter":"10 Analyzing Census microdata","heading":"10.5 Exercises","text":"Using dataset acquired exercises Chapter 9 (example Wyoming dataset chapter), tabulate group-wise summary using PWGTP column dplyr functions ’ve learned section.Using dataset acquired exercises Chapter 9 (example Wyoming dataset chapter), tabulate group-wise summary using PWGTP column dplyr functions ’ve learned section.Advanced follow-: using get_acs(), attempt acquire aggregated data ACS. Compare tabulated estimate ACS estimate.Advanced follow-: using get_acs(), attempt acquire aggregated data ACS. Compare tabulated estimate ACS estimate.Second advanced follow-: request data , time replicate weights. Calculate margin error ’ve learned section - time, compare posted ACS margin error!Second advanced follow-: request data , time replicate weights. Calculate margin error ’ve learned section - time, compare posted ACS margin error!","code":""},{"path":"other-census-and-government-data-resources.html","id":"other-census-and-government-data-resources","chapter":"11 Other Census and government data resources","heading":"11 Other Census and government data resources","text":"examples covered book point use data recent US Census Bureau datasets Decennial Census since 2000 American Community Survey. datasets available US Census Bureau’s APIs turn accessible tidycensus related tools. However, analysts historians may interested accessing data much earlier - perhaps way back 1790, first US Census! Fortunately, historical datasets available analysts National Historical Geographic Information System (NHGIS) project Minnesota Population Center’s IPUMS project. data repositories typically attracted researchers using commercial software ArcGIS (NHGIS) Stata/SAS (IPUMS), Minnesota Population Center developed associated ipumsr R package help analysts integrate datasets R-based workflows.Additionally, US Census Bureau publishes many surveys datasets besides decennial US Census American Community Survey. tidycensus focuses core datasets, R packages provide support wide range datasets available Census Bureau government agencies.first part chapter provides overview access use historical US Census datasets R NHGIS, IPUMS, ipumsr package. Due size datasets involved, datasets provided sample data available book’s data repository. reproduce, readers follow steps provided sign IPUMS account download data . second part chapter covers R workflows Census data resources outside decennial US Census American Community Survey. highlights packages censusapi, allows programmatic access US Census Bureau APIs, lehdr, grants access LEHD LODES dataset analyzing commuting flows jobs distributions. government data resources also addressed end chapter.","code":""},{"path":"other-census-and-government-data-resources.html","id":"mapping-historical-geographies-of-new-york-city-with-nhgis","chapter":"11 Other Census and government data resources","heading":"11.1 Mapping historical geographies of New York City with NHGIS","text":"National Historical Geographic Information System (NHGIS) project (Manson et al. 2021) tremendous resource contemporary historical US Census data. datasets (e.g. 2000 2010 decennial US Censuses, ACS) can accessed tidycensus NHGIS, NHGIS excellent option users prefer browsing data menus request data /require historical information earlier 2000. example section illustrate applied workflow using NHGIS companion R package, ipumsr (Ellis Burk 2020) map geographies immigration New York City 1910 Census.","code":""},{"path":"other-census-and-government-data-resources.html","id":"getting-started-with-nhgis","chapter":"11 Other Census and government data resources","heading":"11.1.1 Getting started with NHGIS","text":"get started NHGIS, visit NHGIS website click “REGISTER” link top screen register account. NHGIS asks basic information work plan use data, ’ll agree NHGIS usage license agreement. registered, return NHGIS home page click “Get Data” button visit NHGIS data browser interface.\nFigure 11.1: NHGIS data browser interface\nseries options left-hand side screen. options include:Geographic levels: level aggregation data. NHGIS includes series filters help choose correct level aggregation; click plus sign select . Keep mind geographic levels available variables years. reproduce example section, click “CENSUS TRACT” “SUBMIT.”Geographic levels: level aggregation data. NHGIS includes series filters help choose correct level aggregation; click plus sign select . Keep mind geographic levels available variables years. reproduce example section, click “CENSUS TRACT” “SUBMIT.”Years: year(s) like request data. Decennial, non-decennial, 5-year ranges available Census tracts. Note many years greyed - means data available years Census tract level. earliest year Census tract-level data available 1910, year choose; check box next “1910” click SUBMIT.Years: year(s) like request data. Decennial, non-decennial, 5-year ranges available Census tracts. Note many years greyed - means data available years Census tract level. earliest year Census tract-level data available 1910, year choose; check box next “1910” click SUBMIT.Topics: menu helps filter specific areas interest searching data select. Data organized categories (e.g. race, ethnicity, origins) sub-topics (e.g. age, sex). Topics available chosen geography/year combination greyed . Choose “Nativity Place Birth” topic click SUBMIT.Topics: menu helps filter specific areas interest searching data select. Data organized categories (e.g. race, ethnicity, origins) sub-topics (e.g. age, sex). Topics available chosen geography/year combination greyed . Choose “Nativity Place Birth” topic click SUBMIT.Datasets: specific datasets like request data, particularly useful multiple datasets available given year. applied example, one dataset aligns choices: Population Data Census tracts New York City 1910. , need select anything .Datasets: specific datasets like request data, particularly useful multiple datasets available given year. applied example, one dataset aligns choices: Population Data Census tracts New York City 1910. , need select anything .Select Data menu shows Census tables available given filter selections. Usefully, menu includes embedded links give additional information available data choices, along “popularity” bar graph showing -downloaded tables particular dataset.\nFigure 11.2: NHGIS Select Data screen\nexample, choose tables “NT26: Ancestry” “NT45: Race/Ethnicity” clicking green plus signs next two select . , click GIS FILES tab. tab allows select companion shapefiles can merged demographic extracts mapping desktop GIS software like R. Choose either Census Tract options click “CONTINUE” review selection. “REVIEW SUBMIT” screen, keep “Comma delimited” file structure selected give extract description like.finished, click SUBMIT. ’ll taken “EXTRACTS HISTORY” screen can download data ready; ’ll receive notification email data can downloaded. receive notification, return NHGIS download table data GIS data directory computer.","code":""},{"path":"other-census-and-government-data-resources.html","id":"working-with-nhgis-data-in-r","chapter":"11 Other Census and government data resources","heading":"11.1.2 Working with NHGIS data in R","text":"acquired, NHGIS spatial attribute data can integrated seamlessly R-based data analysis workflows thanks ipumsr package. ipumsr includes series NHGIS-specific functions: read_nhgis(), reads tabular aggregate data; read_nhgis_sf(), reads spatial data simple features object; read_nhgis_sp(), reads spatial data legacy sp format.read_nhgis_sf() built-features make working spatial demographic data simpler R users. data_file argument pointed CSV file demographic data, shape_file argument pointed shapefile, read_nhgis_sf() read files simultaneously join correctly based common GISJOIN column found files. additional perk read_nhgis_sf() can handle zipped folders, removing additional step analyst’s data preparation workflow.example uses read_nhgis_sf() read spatial demographic data immigrants New York City 1910. 1910 shapefile folder includes NYC Census tracts separate dataset US counties, top-level folder unzipped, shape_file pointed second-level zipped folder, shape_layer argument used exclusively read tracts. filter() call drop Census tracts corresponding data (, outside NYC).read_nhgis_sf() read tracts shapefile simple features object joined corresponding CSV file imported data labels data codebook. Note reproducing example data downloaded , unique zipped folder & file name based unique download ID. “99” example reflects 99th extract NHGIS given user, unique dataset name.best way review variables labels View() command RStudio, efficient sf objects geometry first dropped sf::st_drop_geometry().\nFigure 11.3: NHGIS data RStudio Viewer\nshown graphic , variable labels particularly useful using View() understand different variables mean without reference codebook.","code":"\nlibrary(ipumsr)\nlibrary(tidyverse)\n\n# Note: the NHGIS file name depends on a download number\n# that is unique to each user, so your file names will be \n# different from the code below\nnyc_1910 <- read_nhgis_sf(\n  data_file = \"data/NHGIS/nhgis0099_csv.zip\",\n  shape_file = \"data/NHGIS/nhgis0099_shape/nhgis0099_shapefile_tl2000_us_tract_1910.zip\",\n  shape_layer = starts_with(\"US_tract_1910\")\n) %>%\n  filter(!is.na(TRACTA))## Use of data from NHGIS is subject to conditions including that users should\n## cite the data appropriately. Use command `ipums_conditions()` for more details.\n## \n## \n## Reading data file...\n## Reading geography...\n## options:        ENCODING=latin1 \n## Reading layer `US_tract_1910' from data source \n##   `/tmp/RtmpUTRqLN/file18cc5431260e/US_tract_1910.shp' using driver `ESRI Shapefile'\n## Simple feature collection with 1989 features and 6 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 489737.4 ymin: 130629.6 xmax: 2029575 ymax: 816129.7\n## Projected CRS: USA_Contiguous_Albers_Equal_Area_Conic\nView(sf::st_drop_geometry(nyc_1910))"},{"path":"other-census-and-government-data-resources.html","id":"mapping-nhgis-data-in-r","chapter":"11 Other Census and government data resources","heading":"11.1.3 Mapping NHGIS data in R","text":"message displayed reading NHGIS shapefile indicates Census tract data projected coordinate reference system, USA_Contiguous_Albers_Equal_Area_Conic. spatial data can displayed plot():\nFigure 11.4: Plot NYC Census tracts 1910 using Albers Equal Area CRS\ndata reflect Census tracts New York City, appear rotated counter-clockwise. coordinate reference system used, ESRI:100023, appropriate entire United States (fact, base CRS used tigris::shift_geometry()), appropriate specific small area. covered Chapter 5, crsuggest package helps identify appropriate projected coordinate reference system options.\nTable 11.1: Suggested CRS options New York City\nBased suggestions, ’ll select CRS “NAD83(2011) / New York Long Island” EPSG code 6538.\nFigure 11.5: NYC Census tracts area-appropriate CRS\nGiven information two tables downloaded NHGIS, multiple ways visualize demographics New York City 1910. first example choropleth map percentage total population born outside United States. “total population” column dataset, code uses dplyr’s rowwise() c_across() functions perform row-wise calculations sum across columns A60001 A60007. transmute() function works like combination mutate() select(): calculates two new columns, selects columns output dataset nyc_pctfb.result can visualized mapping packages covered Chapter 6, ggplot2 geom_sf().\nFigure 11.6: Percent foreign-born Census tract NYC 1910, mapped ggplot2\nManhattan’s Lower East Side stands part city largest proportion foreign-born residents 1910, percentages exceeding 60%.alternative view focus one specific groups represented columns dataset. example, number Italy-born residents Census tract represented column A6G014; type information represented either graduated symbol map dot-density map. Using techniques learned Section 6.3.4.3, can use as_dot_density() function tidycensus package generate one dot approximately every 100 Italian immigrants. Next, Census tracts dissolved st_union() function generate base layer top dots plotted.Section 6.3.4.3, used tmap::tm_dots() create dot-density map. ggplot2 geom_sf() also work well dot-density mapping; cartographers can either use geom_sf() small size argument, set shape = \".\" data point represented single pixel screen.\nFigure 11.7: Dot-density map Italy-born population NYC 1910, mapped ggplot2\nresulting map highlights areas large concentrations Italian immigrants New York City 1910.","code":"\nplot(nyc_1910$geometry)\nlibrary(crsuggest)\nlibrary(sf)\n\nsuggest_crs(nyc_1910)\nnyc_1910_proj <- st_transform(nyc_1910, 6538)\n\nplot(nyc_1910_proj$geometry)\nnyc_pctfb <- nyc_1910_proj %>%\n  rowwise() %>%\n  mutate(total = sum(c_across(A60001:A60007))) %>%\n  ungroup() %>%\n  transmute(\n    tract_id = GISJOIN,\n    pct_fb = A60005 / total\n  ) \nggplot(nyc_pctfb, aes(fill = pct_fb)) + \n  geom_sf(color = NA) + \n  scale_fill_viridis_c(option = \"magma\", labels = scales::percent) + \n  theme_void(base_family = \"Verdana\") + \n  labs(title = \"Percent foreign-born by Census tract, 1910\",\n       subtitle = \"New York City\",\n       caption =  \"Data source: NHGIS\",\n       fill = \"Percentage\")\nlibrary(tidycensus)\n\nitaly_dots <- nyc_1910_proj %>%\n  as_dot_density(\n    value = \"A6G014\",\n    values_per_dot = 100\n  )\n\nnyc_base <- nyc_1910_proj %>%\n  st_union()\nggplot() + \n  geom_sf(data = nyc_base, size = 0.1) + \n  geom_sf(data = italy_dots, shape = \".\", color = \"darkgreen\") + \n  theme_void(base_family = \"Verdana\") + \n  labs(title = \"Italy-born population in New York City, 1910\",\n       subtitle = \"1 dot = 100 people\",\n       caption = \"Data source: NHGIS\")"},{"path":"other-census-and-government-data-resources.html","id":"analyzing-complete-count-historical-microdata-with-ipums-and-r","chapter":"11 Other Census and government data resources","heading":"11.2 Analyzing complete-count historical microdata with IPUMS and R","text":"Chapters 9 10 covered process acquiring analyzing microdata American Community Survey tidycensus. noted, workflows available recent demographics, reflecting recent availability ACS. Historical researchers need data goes back, likely turn IPUMS-USA datasets. IPUMS-USA makes available microdata samples way back 1790, enabling historical demographic research possible elsewhere.core focus Chapter 10 use sampling weights appropriately analyze model microdata. Historical Census datasets, however, subject “72-year rule”, states:U.S. government release personally identifiable information individual individual agency 72 years collected decennial census. “72-Year Rule” (92 Stat. 915; Public Law 95-416; October 5, 1978) restricts access decennial census records individual named record legal heir.means decennial Census records reflect periods 72 years ago older can made available researchers IPUMS team. fact, complete-count Census microdata can downloaded IPUMS person-level Census years 1850-1940, household level years earlier 1850.availability complete-count Census records offers tremendous analytic opportunity researchers, also comes challenges. largest ACS microdata sample - 2016-2020 5-year ACS - around 16 million records, can read memory standard desktop computer 16GB RAM. Complete-count Census data can records exceeding 100 million, possible read memory R standard computer. chapter covers R-based workflows handling massive Census microdata without needing upgrade one’s computer set cloud computing instance. solution presented involves setting local database PostgreSQL DBeaver platform, interacting microdata database using R’s tidyverse database interface tooling.","code":""},{"path":"other-census-and-government-data-resources.html","id":"getting-microdata-from-ipums","chapter":"11 Other Census and government data resources","heading":"11.2.1 Getting microdata from IPUMS","text":"get started, visit IPUMS-USA website https://usa.ipums.org/usa/. already signed IPUMS account Section 11.1.1, log user name password; otherwise follow instructions register account, can use IPUMS resources including NHGIS. logged , click “Get Data” button visit data selection menu.\nFigure 11.8: IPUMS data browser\n’ll use various options displayed image define extract. options include:Select samples: choose one data samples include microdata extract. can choose wide range American Community Survey Decennial US Census samples, can download full count Census data “USA FULL COUNT” tab. reproduce example, choose 1910 100% dataset first un-checking “Default sample year” box, clicking “USA FULL COUNT” tab, choosing 1910 dataset clicking SUBMIT SAMPLE SELECTIONS.Select samples: choose one data samples include microdata extract. can choose wide range American Community Survey Decennial US Census samples, can download full count Census data “USA FULL COUNT” tab. reproduce example, choose 1910 100% dataset first un-checking “Default sample year” box, clicking “USA FULL COUNT” tab, choosing 1910 dataset clicking SUBMIT SAMPLE SELECTIONS.Select harmonized variables: One major benefits using IPUMS microdata analysis IPUMS team produced harmonized variables aim reconcile variable IDs variable definitions time allowing easier longitudinal analysis. default, users browse select harmonized variables. Choose household-level variables person-level variables browsing drop-menus selecting appropriate variable IDs; added output extract. users want work variables originally source dataset, click SOURCE VARIABLES radio button switch source variables. replicate example, choose STATEFIP (household > geographic), SEX, AGE, MARST (person > demographic), LIT (person > education) variables.Select harmonized variables: One major benefits using IPUMS microdata analysis IPUMS team produced harmonized variables aim reconcile variable IDs variable definitions time allowing easier longitudinal analysis. default, users browse select harmonized variables. Choose household-level variables person-level variables browsing drop-menus selecting appropriate variable IDs; added output extract. users want work variables originally source dataset, click SOURCE VARIABLES radio button switch source variables. replicate example, choose STATEFIP (household > geographic), SEX, AGE, MARST (person > demographic), LIT (person > education) variables.Display options: menu gives number options modify display variables browsing; try different options ’d like.Display options: menu gives number options modify display variables browsing; try different options ’d like.finished, click “VIEW CART” link go data cart. ’ll see variables returned extract.\nFigure 11.9: IPUMS data cart\nNotice variables output extract selected; number technical variables pre-selected, similar approach taken get_pums() tidycensus. ready create extract, click CREATE DATA EXTRACT button get summary extract submitting.\nFigure 11.10: IPUMS extract request screen\nNote example shown reflects changing output data format CSV, used load IPUMS data database. using R directly smaller extract ipumsr R package, default fixed-width text file extension .dat can selected data can read R ipumsr::read_ipums_micro(). replicate workflow , however, output data format changed CSV.Click SUBMIT EXTRACT submit extract request IPUMS system. ’ll need agree special usage terms 100% data, wait patiently extract process. ’ll get email notification extract ready; , return IPUMS download data extract computer. output format gzipped CSV prefix unique download ID, e.g. usa_00032.csv.gz. Use appropriate utility unzip CSV file.","code":""},{"path":"other-census-and-government-data-resources.html","id":"loading-microdata-into-a-database","chapter":"11 Other Census and government data resources","heading":"11.2.2 Loading microdata into a database","text":"Census data analysts may feel comfortable working .csv files, reading R readr::read_csv() one many options loading data comma-separated text files. data extract created previous section presents additional challenges Census analyst, however. contains approximately 92.4 million rows - one person 1910 US Census! fill memory standard laptop computer read R quickly using standard tools.alternative solution can performed standard laptop computer setting database. solution proposed uses PostgreSQL, popular open-source database, DBeaver, free cross-platform tool working databases.PostgreSQL already installed computer, visit https://www.postgresql.org/download/ follow instructions operating system install . full tutorial PostgreSQL operating system beyond scope book, example use standard defaults. finished installing PostgreSQL, prompted set default database, called postgres associated user, also named postgres. ’ll asked set password database; examples chapter also use postgres password, can choose whatever password like.default database set , visit https://dbeaver.io/download/ download/install appropriate DBeaver Community Edition operating system. Launch DBeaver, look “New Database Connection” icon. Click launch connection PostgreSQL database. Choose “PostgreSQL” menu database options fill connection settings appropriately.\nFigure 11.11: DBeaver connection screen\ndefaults, host localhost running port 5432 database name postgres username postgres well. Enter password selected setting PostgreSQL click Finish. ’ll notice database connection appear “Database Navigator” pane DBeaver.Within database, can create schemas organize sets related tables. create new schema, right-click postgres database Database Navigator pane choose “Create > Schema”. example uses schema named “ipums”. Within new schema ’ll able import CSV file table. Expand “ipums” schema right-click “Tables” choose Import Data. Select unzipped IPUMS CSV file progress menus. example changes default “Target” name census1910.final menu, click Start button import data. Given size dataset, likely take time; took around 2 hours prepare example machine. data imported successfully new database table DBeaver, ’ll able inspect dataset using DBeaver interface.\nFigure 11.12: DBeaver data view\n","code":""},{"path":"other-census-and-government-data-resources.html","id":"accessing-your-microdata-database-with-r","chapter":"11 Other Census and government data resources","heading":"11.2.3 Accessing your microdata database with R","text":"typical R workflow flat files, analyst read dataset (CSV file) entirely -memory R use preferred toolset interact data. alternative discussed section involves connecting 1910 Census database R using R’s database interface toolkit tidyverse analyze data. major benefit workflow allows analyst perform tidyverse operations datasets database. means tidyverse functions translated Structured Query Language (SQL) queries passed database, outputs displayed R, allowing analyst interact data without read memory.first step requires making database connection R. DBI infrastructure R allows users make connections wide range databases using consistent interface (R Special Interest Group Databases (R-SIG-DB), Wickham, Müller 2021). PostgreSQL interface handled RPostgres package (Wickham, Ooms, Müller 2021).dbConnect() function used make database connection, assigned object conn. Arguments include database driver Postgres() username, password, database name, host, port, familiar database setup process.connected database, database extension dplyr, dbplyr, facilitates interaction database tables (Wickham, Girlich, Ruiz 2021). tbl() links connection object conn retrieve data database; in_schema() function points tbl() census1910 table ipums schema.Printing new object census1910 shows general structure 1910 Census microdata:data 13 columns unknown number rows; database table large dbplyr won’t calculate automatically. However, connection database allows interaction 1910 Census microdata using familiar tidyverse workflows, addressed next section.","code":"\nlibrary(tidyverse)\nlibrary(RPostgres)\nlibrary(dbplyr)\n\nconn <- dbConnect(\n  drv = Postgres(),\n  user = \"postgres\",\n  password = \"postgres\",\n  host = \"localhost\",\n  port = \"5432\",\n  dbname = \"postgres\"\n)\ncensus1910 <- tbl(conn, in_schema(\"ipums\", \"census1910\"))\n\ncensus1910## # Source:   table<\"ipums\".\"census1910\"> [?? x 13]\n## # Database: postgres [postgres@localhost:5432/postgres]\n##     YEAR sample serial  hhwt statefip    gq pernum perwt   sex   age marst   lit\n##    <int>  <int>  <int> <dbl>    <int> <int>  <int> <dbl> <int> <int> <int> <int>\n##  1  1910 191004      1     1        2     1      1     1     1    43     6     4\n##  2  1910 191004      2     1        2     1      1     1     1    34     6     4\n##  3  1910 191004      3     1        2     1      1     1     1    41     1     4\n##  4  1910 191004      3     1        2     1      2     1     2    39     1     4\n##  5  1910 191004      3     1        2     1      3     1     1    37     6     4\n##  6  1910 191004      4     1        2     1      1     1     1    24     6     4\n##  7  1910 191004      5     1        2     1      1     1     1    24     6     4\n##  8  1910 191004      5     1        2     1      2     1     1    35     6     4\n##  9  1910 191004      5     1        2     1      3     1     1    45     2     4\n## 10  1910 191004      5     1        2     1      4     1     1    55     6     4\n## # … with more rows, and 1 more variable: histid <chr>"},{"path":"other-census-and-government-data-resources.html","id":"analyzing-big-census-microdata-in-r","chapter":"11 Other Census and government data resources","heading":"11.2.4 Analyzing big Census microdata in R","text":"default printing microdata shown reveal number rows dataset, tidyverse tools can used request information database. example, summarize() function generate summary tabulations shown earlier book; without companion group_by(), whole dataset.92.4 million rows dataset, reflecting US population size 1910. Given working complete-count data, workflow using microdata differs sample data covered Chapters 9 10. IPUMS default returns person-weight column, value rows dataset column 1, reflecting 1:1 relationship records actual people United States time.dplyr’s database interface can accommodate complex examples well. Let’s say want tabulate literacy statistics 1910 population age 18 Texas sex. straightforward tidyverse pipeline can accommodate request database. reference, male coded 1 female 2, literacy codes follows:1: , illiterate (read write)1: , illiterate (read write)2: read, can write2: read, can write3: write, can read3: write, can read4: Yes, literate (reads writes)4: Yes, literate (reads writes)dbplyr also includes helper functions better understand working work derived results. example, show_query() function can attached end tidyverse pipeline show SQL query R code translated order perform operations -database:analyst wants result database operation brought R R object rather database view, collect() function can used end pipeline load data directly. companion function ipumsr, ipums_collect(), add variable value labels collected data based IPUMS codebook.aforementioned toolsets can now used robust analyses historical microdata based complete-count Census data. example illustrates extend literacy sex example visualize literacy gaps sex state 1910.\nFigure 11.13: Literacy gaps sex state 1910\n","code":"\ncensus1910 %>% summarize(n())## # Source:   lazy query [?? x 1]\n## # Database: postgres [postgres@localhost:5432/postgres]\n##      `n()`\n##    <int64>\n## 1 92404011\ncensus1910 %>%\n  filter(age > 17, statefip == \"48\") %>%\n  group_by(sex, lit) %>%\n  summarize(num = n())## # Source:   lazy query [?? x 3]\n## # Database: postgres [postgres@localhost:5432/postgres]\n## # Groups:   sex\n##     sex   lit     num\n##   <int> <int> <int64>\n## 1     1     1  115952\n## 2     1     2      28\n## 3     1     3   14183\n## 4     1     4  997844\n## 5     2     1  111150\n## 6     2     2      16\n## 7     2     3   14531\n## 8     2     4  883636\ncensus1910 %>%\n  filter(age > 17, statefip == \"48\") %>%\n  group_by(sex, lit) %>%\n  summarize(num = n()) %>%\n  show_query()## <SQL>\n## SELECT \"sex\", \"lit\", COUNT(*) AS \"num\"\n## FROM \"ipums\".\"census1910\"\n## WHERE ((\"age\" > 17.0) AND (\"statefip\" = '48'))\n## GROUP BY \"sex\", \"lit\"\nliteracy_props <- census1910 %>%\n  filter(age > 18) %>%\n  group_by(statefip, sex, lit) %>%\n  summarize(num = n()) %>%\n  group_by(statefip, sex) %>%\n  mutate(total = sum(num, na.rm = TRUE)) %>%\n  ungroup() %>%\n  mutate(prop = num / total) %>%\n  filter(lit == 4) %>%\n  collect()\n\nstate_names <- tigris::fips_codes %>%\n  select(state_code, state_name) %>%\n  distinct()\n\nliteracy_props_with_name <- literacy_props %>%\n  mutate(statefip = str_pad(statefip, 2, \"left\", \"0\")) %>%\n  left_join(state_names, by = c(\"statefip\" = \"state_code\")) %>%\n  mutate(sex = ifelse(sex == 1, \"Male\", \"Female\")) \n\nggplot(literacy_props_with_name, aes(x = prop, y = reorder(state_name, prop), \n                           color = sex)) + \n  geom_line(aes(group = state_name), color = \"grey10\") + \n  geom_point(size = 2.5) +\n  theme_minimal() + \n  scale_color_manual(values = c(Male = \"navy\", Female = \"darkred\")) + \n  scale_x_continuous(labels = scales::percent) + \n  labs(x = \"Percent fully literate, 1910\",\n       color = \"\",\n       y = \"\")"},{"path":"other-census-and-government-data-resources.html","id":"other-us-government-datasets","chapter":"11 Other Census and government data resources","heading":"11.3 Other US government datasets","text":"point, book focused smaller number US Census Bureau datasets, primary focus decennial US Census American Community Survey. However, many US government datasets available researchers, US Census Bureau others different US government agencies. section covers series R packages help analysts access resources, illustrates applied workflows using datasets.","code":""},{"path":"other-census-and-government-data-resources.html","id":"accessing-census-data-resources-with-censusapi","chapter":"11 Other Census and government data resources","heading":"11.3.1 Accessing Census data resources with censusapi","text":"censusapi (Recht 2021) R package designed give R users access US Census Bureau’s API endpoints. Unlike tidycensus, focuses select number datasets, censusapi’s getCensus() function can widely applied hundreds possible datasets Census Bureau makes available. censusapi requires knowledge structure Census Bureau API requests work; however makes package flexible tidycensus may preferable users want submit highly customized queries decennial Census ACS APIs.censusapi uses Census API key tidycensus, though references R environment variable CENSUS_KEY. environment variable set user’s .Renviron file, functions censusapi pick key without supply directly.usethis package (Wickham Bryan 2021) user-friendly way work .Renviron file R function edit_r_environ(). Calling function bring .Renviron file text editor, allowing users set environment variables made available R R starts .Add line CENSUS_KEY='KEY ' .Renviron file, replacing KEY text API key, restart R get access key.censusapi’s core function getCensus(), translates R code Census API queries. name argument references API name; censusapi documentation function listCensusApis() helps understand format .example makes request Economic Census API, getting data employment payroll NAICS code 72 (accommodation food services businesses) counties Texas 2017.\nTable 11.2: Data 2017 Economic Census acquired censusapi package\ncensusapi can also used combination packages covered book tigris mapping. example uses Small Area Health Insurance Estimates API, delivers modeled estimates health insurance coverage county level various demographic breakdowns. Using censusapi tigris, can retrieve data percent population age 19 without health insurance counties US. information joined counties dataset tigris shifted geometry, mapped ggplot2.\nFigure 11.14: Map youth without health insurance using censusapi tigris\nmodeled estimates, state-level influences county-level estimates apparent map.","code":"\nlibrary(usethis)\n\nedit_r_environ()\nlibrary(censusapi)\n\ntx_econ17 <- getCensus(\n  name = \"ecnbasic\",\n  vintage = 2017,\n  vars = c(\"EMP\", \"PAYANN\", \"GEO_ID\"),\n  region = \"county:*\",\n  regionin = \"state:48\",\n  NAICS2017 = 72\n)\nlibrary(tigris)\nlibrary(tidyverse)\n\nus_youth_sahie <- getCensus(\n  name = \"timeseries/healthins/sahie\",\n  vars = c(\"GEOID\", \"PCTUI_PT\"),\n  region = \"county:*\",\n  regionin = \"state:*\",\n  time = 2019,\n  AGECAT = 4\n)\n\nus_counties <- counties(cb = TRUE, resolution = \"20m\", year = 2019) %>%\n  shift_geometry(position = \"outside\") %>%\n  inner_join(us_youth_sahie, by = \"GEOID\") \n\nggplot(us_counties, aes(fill = PCTUI_PT)) + \n  geom_sf(color = NA) + \n  theme_void() + \n  scale_fill_viridis_c() + \n  labs(fill = \"% uninsured \",\n       caption = \"Data source: US Census Bureau Small Area\\nHealth Insurance Estimates via the censusapi R package\",\n       title = \"  Percent uninsured under age 19 by county, 2019\")"},{"path":"other-census-and-government-data-resources.html","id":"analyzing-labor-markets-with-lehdr","chapter":"11 Other Census and government data resources","heading":"11.3.2 Analyzing labor markets with lehdr","text":"Another useful package working Census Bureau data lehdr R package (Green, Wang, Mahmoudi 2021), access Longitudinal Employer-Household Dynamics (LEHD) Origin-Destination Employment Statistics (LODES) data. LODES available Census API, meriting alternative package approach. LODES includes synthetic estimates residential, workplace, residential-workplace links Census block level, allowing highly detailed geographic analysis jobs commuter patterns time.core function implemented lehdr grab_lodes(), downloads LODES file specified lodes_type (either \"rac\" residential, \"wac\" workplace, \"od\" origin-destination) given state year. raw LODES data available Census block level, agg_geo parameter offers convenient way roll estimates higher levels aggregation. origin-destination data, state_part = \"main\" argument captures within-state commuters; use state_part = \"aux\" get commuters --state. optional argument use_cache = TRUE stores downloaded LODES data cache directory user’s computer; recommended avoid re-download data future analyses.result dataset shows tract--tract commute flows (S000) broken variety characteristics, referenced LODES documentation.\nTable 11.3: Origin-destination data acquired lehdr package\nlehdr can used variety purposes including transportation economic development planning. example, workflow illustrates use LODES data understand origins commuters Microsoft campus (represented Census tract) Redmond, Washington. Commuters LODES normalized total population age 18 , acquired tidycensus 2018 Census tracts Seattle-area counties. dataset ms_commuters include Census tract geometries (obtained geometry = TRUE tidycensus) estimate number Microsoft-area commuters per 1000 adults Census tract.result can visualized map ggplot2, alternatively mapping tools introduced Chapter 6. Note use erase_water() function tigris package introduced Section 7.5 high area threshold remove large bodies water like Lake Washington Census tract shapes.\nFigure 11.15: Map commuter origins Microsoft Campus area, Seattle-area counties\n","code":"\nlibrary(lehdr)\nlibrary(tidycensus)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tigris)\n\nwa_lodes_od <- grab_lodes(\n  state = \"wa\",\n  year = 2018,\n  lodes_type = \"od\",\n  agg_geo = \"tract\",\n  state_part = \"main\",\n  use_cache = TRUE\n)\nseattle_adults <- get_acs(\n  geography = \"tract\",\n  variables = \"S0101_C01_026\",\n  state = \"WA\",\n  county = c(\"King\", \"Kitsap\", \"Pierce\",\n             \"Snohomish\"),\n  year = 2018,\n  geometry = TRUE\n)\n\nmicrosoft <- filter(wa_lodes_od, w_tract == \"53033022803\")\n\nms_commuters <- seattle_adults %>%\n  left_join(microsoft, by = c(\"GEOID\" = \"h_tract\")) %>%\n  mutate(ms_per_1000 = 1000 * (S000 / estimate)) %>%\n  st_transform(6596) %>%\n  erase_water(area_threshold = 0.99)\nggplot(ms_commuters, aes(fill = ms_per_1000)) + \n  geom_sf(color = NA) + \n  theme_void() + \n  scale_fill_viridis_c(option = \"cividis\") + \n  labs(title = \"Microsoft commuters per 1000 adults\",\n       subtitle = \"2018 LODES data, Seattle-area counties\",\n       fill = \"Rate per 1000\")"},{"path":"other-census-and-government-data-resources.html","id":"bureau-of-labor-statistics-data-with-blscraper","chapter":"11 Other Census and government data resources","heading":"11.3.3 Bureau of Labor Statistics data with blscrapeR","text":"Another essential resource data US labor market characteristics Bureau Labor Statistics, makes data available users via API. BLS data can accessed R blscrapeR package (Eberwein 2021). Like government APIs, BLS API requires API key, obtained https://data.bls.gov/registrationEngine/. Sign email address validate key using link sent email, return R get started blscrapeR.blscrapeR includes number helper functions assist users commonly requested statistics. example, get_bls_county() function fetches employment data counties US given month.\nTable 11.4: May 2021 employment data BLS\ncomplex queries also possible bls_api() function; however, requires knowing BLS series ID. Series IDs composite strings made several alphanumeric codes can represent different datasets, geographies, industries, among others. blscrapeR includes internal dataset, series_ids, help users view commonly requested IDs.\nTable 11.5: Series IDs use blscrapeR\nseries_ids contains indicators Current Population Survey Local Area Unemployment Statistics databases. data series can constructed following instructions <https://www.bls.gov/help/hlpforma.htm>. example illustrates data Current Employment Statistics, series ID formatted follows:Positions 1-2: prefix (example, SA)Positions 1-2: prefix (example, SA)Position 3: seasonal adjustment code (either S, seasonally adjusted, U, unadjusted)Position 3: seasonal adjustment code (either S, seasonally adjusted, U, unadjusted)Positions 4-5: two-digit state FIPS codePositions 4-5: two-digit state FIPS codePositions 6-10: five-digit area code, set 00000 entire state requested.Positions 6-10: five-digit area code, set 00000 entire state requested.Positions 11-18: super sector/industry codePositions 11-18: super sector/industry codePositions 19-20: data type codePositions 19-20: data type codeWe’ll use information get unadjusted data accommodation workers Maui, Hawaii since beginning 2018. Note specific codes detailed vary indicator indicator, industries available geographies.\nTable 11.6: Accommodations employment Maui, Hawaii\ntime series can visualized ggplot2:\nFigure 11.16: Time-series accommodations employment Maui, Hawaii\ndata suggest stable accommodations sector prior onset COVID-19 pandemic, recovery industry starting toward end 2020.","code":"\n# remotes::install_github(\"keberwein/blscrapeR\")\nlibrary(blscrapeR)\n\nset_bls_key(\"YOUR KEY HERE\")\nlatest_employment <- get_bls_county(\"May 2021\")\nhead(series_ids)\nmaui_accom <- bls_api(seriesid = \"SMU15279807072100001\", \n                      startyear = 2018)\nmaui_accom %>%\n  mutate(period_order = rev(1:nrow(.))) %>%\n  ggplot(aes(x = period_order, y = value, group = 1)) + \n  geom_line(color = \"darkgreen\") + \n  theme_minimal() + \n  scale_y_continuous(limits = c(0, max(maui_accom$value) + 1)) + \n  scale_x_continuous(breaks = seq(1, 42, 12),\n                     labels = paste(\"Jan\", 2018:2021)) + \n  labs(x = \"\",\n       y = \"Number of jobs (in 1000s)\",\n       title = \"Accommodation employment in Maui, Hawaii\",\n       subtitle = \"Data source: BLS Current Employment Statistics\")"},{"path":"other-census-and-government-data-resources.html","id":"working-with-agricultural-data-with-tidyusda","chapter":"11 Other Census and government data resources","heading":"11.3.4 Working with agricultural data with tidyUSDA","text":"Agriculture can difficult sector collect market statistics, available many data sources LODES BLS Current Employment Statistics. Dedicated statistics US agriculture can acquired tidyUSDA R package (Lindblad 2021), interacts USDA data resources using tidyverse-centric workflows. Get API key https://quickstats.nass.usda.gov/api use request data USDA QuickStats API. example, API key stored \"USDA_KEY\" environment variable.Now, let’s see US counties produce acres devoted cucumbers. use getQuickstat() function effectively, helpful construct query first https://quickstats.nass.usda.gov/ see options available, bring options arguments R.information allows us look top cucumber-growing counties US:\nTable 11.7: Top cucumber-growing counties United States\n","code":"\nlibrary(tidyUSDA)\n\nusda_key <- Sys.getenv(\"USDA_KEY\")\ncucumbers <- getQuickstat(\n  key = usda_key,\n  program = \"CENSUS\",\n  data_item = \"CUCUMBERS, FRESH MARKET - ACRES HARVESTED\",\n  sector = \"CROPS\",\n  group = \"VEGETABLES\",\n  commodity = \"CUCUMBERS\",\n  category = \"AREA HARVESTED\",\n  domain = \"TOTAL\",\n  geographic_level = \"COUNTY\",\n  year = \"2017\"\n)\ncucumbers %>%\n  select(state_name, county_name, Value) %>%\n  arrange(desc(Value))"},{"path":"other-census-and-government-data-resources.html","id":"getting-government-data-without-r-packages","chapter":"11 Other Census and government data resources","heading":"11.4 Getting government data without R packages","text":"breath R’s developer ecosystem means many cases need US government data, enterprising developer written code can use. However, won’t true every data resource, especially US government agencies continue release new API endpoints open access data. case, may interested writing data access functions purpose. section gives pointers make HTTP requests data APIs using httr package process data appropriately use R.","code":""},{"path":"other-census-and-government-data-resources.html","id":"making-requests-to-apis-with-httr","chapter":"11 Other Census and government data resources","heading":"11.4.1 Making requests to APIs with httr","text":"data API packages R rely httr R package (Wickham 2020), provides R functions common HTTP requests GET, POST, PUT, among others. example, ’ll use httr::GET() function make request new Department Housing Urban Development Comprehensive Affordable Housing Strategy (CHAS) API. require getting HUD API token storing environment variable described earlier chapter.Every API structured differently respect accepts queries authenticates API tokens. example, documentation instructs users pass API token HTTP request Authentication: Bearer header. example URL documentation, https://www.huduser.gov/hudapi/public/chas?type=3&year=2012-2016&stateId=51&entityId=59, passed GET(), add_headers() function httr used assemble appropriate string send API.got back HTTP status code 200, know request API succeeded. view returned data, use content() function translate result text = \"text\" argument.data returned JavaScript Object Notation, JSON, common data format returning data APIs. data shown moderately interpretable, require additional translation use R. jsonlite package (Ooms 2014) includes poewrful set tools working JSON objects R; , ’ll use fromJSON() function convert JSON data frame.\nTable 11.8: CHAS API request data frame\ndata now formatted R data frame can embedded R-based workflows.","code":"\nlibrary(glue)\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\n\nmy_token <- Sys.getenv(\"HUD_TOKEN\")\n\nhud_chas_request <- GET(\"https://www.huduser.gov/hudapi/public/chas?type=3&year=2012-2016&stateId=51&entityId=59\",\n                        add_headers(Authorization = glue(\"Bearer {my_token}\")))\n\nhud_chas_request$status_code## [1] 200\ncontent(hud_chas_request, as = \"text\")## [1] \"[{\\\"geoname\\\":\\\"Fairfax County, Virginia\\\",\\\"sumlevel\\\":\\\"County\\\",\\\"year\\\":\\\"2012-2016\\\",\\\"A1\\\":\\\"12895.0\\\",\\\"A2\\\":\\\"21805.0\\\",\\\"A3\\\":\\\"34700.0\\\",\\\"A4\\\":\\\"14165.0\\\",\\\"A5\\\":\\\"17535.0\\\",\\\"A6\\\":\\\"31700.0\\\",\\\"A7\\\":\\\"10690.0\\\",\\\"A8\\\":\\\"10350.0\\\",\\\"A9\\\":\\\"21040.0\\\",\\\"A10\\\":\\\"16890.0\\\",\\\"A11\\\":\\\"14325.0\\\",\\\"A12\\\":\\\"31215.0\\\",\\\"A13\\\":\\\"210710.0\\\",\\\"A14\\\":\\\"63990.0\\\",\\\"A15\\\":\\\"274700.0\\\",\\\"A16\\\":\\\"265350.0\\\",\\\"A17\\\":\\\"128005.0\\\",\\\"A18\\\":\\\"393360.0\\\",\\\"B1\\\":\\\"58925.0\\\",\\\"B2\\\":\\\"57735.0\\\",\\\"B3\\\":\\\"116660.0\\\",\\\"B4\\\":\\\"205390.0\\\",\\\"B5\\\":\\\"68400.0\\\",\\\"B6\\\":\\\"273790.0\\\",\\\"B7\\\":\\\"1035.0\\\",\\\"B8\\\":\\\"1875.0\\\",\\\"B9\\\":\\\"2910.0\\\",\\\"C1\\\":\\\"23945.0\\\",\\\"C2\\\":\\\"31650.0\\\",\\\"C3\\\":\\\"55595.0\\\",\\\"C4\\\":\\\"240370.0\\\",\\\"C5\\\":\\\"94480.0\\\",\\\"C6\\\":\\\"334850.0\\\",\\\"D1\\\":\\\"207915.0\\\",\\\"D2\\\":\\\"73630.0\\\",\\\"D3\\\":\\\"281545.0\\\",\\\"D4\\\":\\\"35735.0\\\",\\\"D5\\\":\\\"28910.0\\\",\\\"D6\\\":\\\"64645.0\\\",\\\"D7\\\":\\\"20670.0\\\",\\\"D8\\\":\\\"23405.0\\\",\\\"D9\\\":\\\"44075.0\\\",\\\"D10\\\":\\\"1035.0\\\",\\\"D11\\\":\\\"2060.0\\\",\\\"D12\\\":\\\"3095.0\\\",\\\"E1\\\":\\\"28030.0\\\",\\\"E2\\\":\\\"3760.0\\\",\\\"E3\\\":\\\"2910.0\\\",\\\"E5\\\":\\\"25945.0\\\",\\\"E6\\\":\\\"5755.0\\\",\\\"E7\\\":\\\"0.0\\\",\\\"E9\\\":\\\"14305.0\\\",\\\"E10\\\":\\\"6735.0\\\",\\\"E11\\\":\\\"0.0\\\",\\\"E13\\\":\\\"16085.0\\\",\\\"E14\\\":\\\"15135.0\\\",\\\"E15\\\":\\\"0.0\\\",\\\"E17\\\":\\\"32295.0\\\",\\\"E18\\\":\\\"242405.0\\\",\\\"E19\\\":\\\"0.0\\\",\\\"E21\\\":\\\"116660.0\\\",\\\"E22\\\":\\\"273790.0\\\",\\\"E23\\\":\\\"2910.0\\\",\\\"F1\\\":\\\"17265.0\\\",\\\"F2\\\":\\\"2665.0\\\",\\\"F3\\\":\\\"1875.0\\\",\\\"F5\\\":\\\"15720.0\\\",\\\"F6\\\":\\\"1815.0\\\",\\\"F7\\\":\\\"0.0\\\",\\\"F9\\\":\\\"8165.0\\\",\\\"F10\\\":\\\"2185.0\\\",\\\"F11\\\":\\\"0.0\\\",\\\"F13\\\":\\\"7900.0\\\",\\\"F14\\\":\\\"6430.0\\\",\\\"F15\\\":\\\"0.0\\\",\\\"F17\\\":\\\"8685.0\\\",\\\"F18\\\":\\\"55305.0\\\",\\\"F19\\\":\\\"0.0\\\",\\\"F21\\\":\\\"57735.0\\\",\\\"F22\\\":\\\"68400.0\\\",\\\"F23\\\":\\\"1875.0\\\",\\\"G1\\\":\\\"10765.0\\\",\\\"G2\\\":\\\"1095.0\\\",\\\"G3\\\":\\\"1035.0\\\",\\\"G5\\\":\\\"10225.0\\\",\\\"G6\\\":\\\"3940.0\\\",\\\"G7\\\":\\\"0.0\\\",\\\"G9\\\":\\\"6140.0\\\",\\\"G10\\\":\\\"4550.0\\\",\\\"G11\\\":\\\"0.0\\\",\\\"G13\\\":\\\"8185.0\\\",\\\"G14\\\":\\\"8705.0\\\",\\\"G15\\\":\\\"0.0\\\",\\\"G17\\\":\\\"23610.0\\\",\\\"G18\\\":\\\"187100.0\\\",\\\"G19\\\":\\\"0.0\\\",\\\"H1\\\":\\\"27605.0\\\",\\\"H2\\\":\\\"23420.0\\\",\\\"H4\\\":\\\"24970.0\\\",\\\"H5\\\":\\\"12660.0\\\",\\\"H7\\\":\\\"13465.0\\\",\\\"H8\\\":\\\"3470.0\\\",\\\"H10\\\":\\\"14930.0\\\",\\\"H11\\\":\\\"2060.0\\\",\\\"H13\\\":\\\"27750.0\\\",\\\"H14\\\":\\\"2460.0\\\",\\\"H16\\\":\\\"108720.0\\\",\\\"I1\\\":\\\"16895.0\\\",\\\"I2\\\":\\\"14490.0\\\",\\\"I4\\\":\\\"14970.0\\\",\\\"I5\\\":\\\"7115.0\\\",\\\"I7\\\":\\\"7550.0\\\",\\\"I8\\\":\\\"1180.0\\\",\\\"I10\\\":\\\"7000.0\\\",\\\"I11\\\":\\\"425.0\\\",\\\"I13\\\":\\\"5900.0\\\",\\\"I14\\\":\\\"195.0\\\",\\\"I16\\\":\\\"52315.0\\\",\\\"J1\\\":\\\"10705.0\\\",\\\"J2\\\":\\\"8930.0\\\",\\\"J4\\\":\\\"10005.0\\\",\\\"J5\\\":\\\"5550.0\\\",\\\"J7\\\":\\\"5915.0\\\",\\\"J8\\\":\\\"2290.0\\\",\\\"J10\\\":\\\"7930.0\\\",\\\"J11\\\":\\\"1635.0\\\",\\\"J13\\\":\\\"21850.0\\\",\\\"J14\\\":\\\"2265.0\\\",\\\"J16\\\":\\\"56405.0\\\"}]\"\nhud_chas_request %>%\n  content(as = \"text\") %>%\n  fromJSON()"},{"path":"other-census-and-government-data-resources.html","id":"writing-your-own-data-access-functions","chapter":"11 Other Census and government data resources","heading":"11.4.2 Writing your own data access functions","text":"plan use particular API endpoint frequently, different areas times, good idea write data access function. API functions ideally identify components API request can vary, allow users make modifications ways correspond components.URL used example can separated two components: API endpoint https://www.huduser.gov/hudapi/public/chas, common API requests, query ?type=3&year=2012-2016&stateId=51&entityId=59, shows components request can vary. query generally follow question mark sign composed key-value pairs, e.g. type=3, example means county according HUD API documentation. queries can passed list query parameter GET() formatted appropriately HTTP request httr.function example can use get started writing API data request functions. general components function follows:function defined name, get_hud_chas(), number parameters user supply arguments. Default arguments can specified (e.g. year = \"2013-2017\" request data without user needing specify directly. default argument NULL, ignored GET() request.function defined name, get_hud_chas(), number parameters user supply arguments. Default arguments can specified (e.g. year = \"2013-2017\" request data without user needing specify directly. default argument NULL, ignored GET() request.function first checks see existing HUD token exists user’s environment referenced name HUD_TOKEN user hasn’t supplied directly.function first checks see existing HUD token exists user’s environment referenced name HUD_TOKEN user hasn’t supplied directly.Next, function constructs GET() query, translating function arguments HTTP query.Next, function constructs GET() query, translating function arguments HTTP query.request failed, function returns error message API help inform user went wrong.request failed, function returns error message API help inform user went wrong.request succeeds, function converts result text, JSON data frame pivoted long form using tidyr’s pivot_longer() function. Finally, result returned. use return() function , isn’t strictly necessary use helpful see function exits code.request succeeds, function converts result text, JSON data frame pivoted long form using tidyr’s pivot_longer() function. Finally, result returned. use return() function , isn’t strictly necessary use helpful see function exits code.may notice couple new notational differences code used elsewhere book. Functions referenced packagename::function() notation, new base R pipe |> used instead magrittr pipe %>%. allows function run without loading external libraries library(), generally good idea writing functions avoid namespace conflicts.may notice couple new notational differences code used elsewhere book. Functions referenced packagename::function() notation, new base R pipe |> used instead magrittr pipe %>%. allows function run without loading external libraries library(), generally good idea writing functions avoid namespace conflicts.Now, let’s try function different locality Virginia:\nTable 11.9: Affordable housing data Alexandria, Virginia using custom-built API function\nfunction returns affordable data Alexandria, Virginia format suitable use tidyverse tools.","code":"\nget_hud_chas <- function(\n  type, \n  year = \"2013-2017\", \n  state_id = NULL, \n  entity_id = NULL, \n  token = NULL\n) {\n  \n  # Check to see if a token exists\n  if (is.null(token)) {\n    if (Sys.getenv(\"HUD_TOKEN\") != \"\") {\n      token <- Sys.getenv(\"HUD_TOKEN\")\n    }\n  }\n  \n  # Specify the base URL\n  base_url <- \"https://www.huduser.gov/hudapi/public/chas\"\n  \n  # Make the query\n  hud_query <- httr::GET(\n    base_url,\n    httr::add_headers(Authorization = glue::glue(\"Bearer {token}\")),\n    query = list(\n      type = type,\n      year = year,\n      stateId = state_id,\n      entityId = entity_id)\n  )\n  \n  # Return the HTTP error message if query failed\n  if (hud_query$status_code != \"200\") {\n    msg <- httr::content(hud_query, as = \"text\")\n    return(msg)\n  }\n  \n  # Return the content as text\n  hud_content <- httr::content(hud_query, as = \"text\")\n  \n  # Convert the data to a long-form tibble\n  hud_tibble <- hud_content |>\n    jsonlite::fromJSON() |>\n    dplyr::as_tibble() |>\n    tidyr::pivot_longer(cols = !dplyr::all_of(c(\"geoname\", \"sumlevel\", \"year\")),\n                        names_to = \"indicator\",\n                        values_to = \"value\")\n  \n  return(hud_tibble)\n  \n}\nget_hud_chas(type = 3, state_id = 51, entity_id = 510)"},{"path":"other-census-and-government-data-resources.html","id":"exercises-9","chapter":"11 Other Census and government data resources","heading":"11.5 Exercises","text":"Visit https://ipums.org spend time exploring NHGIS /IPUMS. Attempt replicate NHGIS workflow different variable time period.Visit https://ipums.org spend time exploring NHGIS /IPUMS. Attempt replicate NHGIS workflow different variable time period.Explore censusapi package create request Census API endpoint using dataset weren’t previously familiar . Join data shapes ’ve obtained tigris make map data.Explore censusapi package create request Census API endpoint using dataset weren’t previously familiar . Join data shapes ’ve obtained tigris make map data.","code":""},{"path":"working-with-census-data-outside-the-united-states.html","id":"working-with-census-data-outside-the-united-states","chapter":"12 Working with Census data outside the United States","heading":"12 Working with Census data outside the United States","text":"Although methods presented book generalizable, examples given book point focused United States. Readers book certainly limited United States projects, interested apply methods presented examples around world. International Census & demographic data focus chapter. first section focuses global demographic indicators -country comparisons gives overview idbr R package accessing indicators US Census Bureau’s International Data Base. sections follow cover country-specific packages around world deliver Census data R users similar ways US-focused packages covered earlier book. Examples Canada, Mexico, Brazil, Kenya illustrate apply methods readers learned book Census data analyses parts world.","code":""},{"path":"working-with-census-data-outside-the-united-states.html","id":"the-international-data-base-and-the-idbr-r-package","chapter":"12 Working with Census data outside the United States","heading":"12.1 The International Data Base and the idbr R package","text":"US Census Bureau’s International Database (IDB) repository dozens demographic indicators 200 countries around world. IDB includes historical information year countries well population projections 2100. -place demographic characteristics derived wide range international Censuses surveys, future projections estimated cohort-component method.Census Bureau makes datasets available researchers interactive data tool also API, allowing programmatic data access. idbr R package (K. Walker 2021b), first released 2016 updated version 1.0 2021, uses simple R interface help users gain access analyze data IDB.get started idbr, users can install package CRAN set Census API keys idb_api_key() function. user’s Census API key already installed using tidycensus, idbr pick making step unnecessary.core function implemented idbr get_idb(), can access datasets available IDB. two main datasets available: 1-year--age population dataset, allows population data country downloaded broken age sex; 5-year--age dataset, gives access population 5-year age bands also many fertility, mortality, migration indicators organized age bands.basic IDB query uses country, specified either ISO-2 code country name English, vector one variables, vector one years. example, can fetch historical projected population Nigeria 1990 2100:\nTable 12.1: Nigeria population data acquired idbr\nresult can visualized time series using ggplot2.\nFigure 12.1: Historical projected population Nigeria\nidbr includes functionality help users look variable codes use function calls. function idb_variables() returns data frame variables available IDB, along informative label; idb_concepts() function prints list concepts can supplied concept parameter return group variables belong concept (e.g. mortality rates, components population growth).worldwide analyses, users can specify argument country = \"\" return available countries IDB. concert tidyverse tools, makes global comparative analyses straightforward. example , get data life expectancy birth countries 2021, view top 10 bottom 10 countries.\nTable 12.2: Countries longest life expectancies birth, 2021\n\nTable 12.3: Countries shortest life expectancies birth, 2021\n","code":"\nlibrary(idbr)\n# Unnecessary if API key is installed with tidycensus\nidb_api_key(\"YOUR KEY GOES HERE\")\nnigeria_pop <- get_idb(\n  country = \"Nigeria\",\n  variables = \"pop\",\n  year = 1990:2100\n)\nlibrary(tidyverse)\n\nggplot(nigeria_pop, aes(x = year, y = pop)) + \n  geom_line(color = \"darkgreen\") + \n  theme_minimal() + \n  scale_y_continuous(labels = scales::label_number_si()) + \n  labs(title = \"Population of Nigeria\",\n       subtitle = \"1990 to 2100 (projected)\",\n       x = \"Year\",\n       y = \"Population at midyear\")\nworld_lex <- get_idb(\n  country = \"all\",\n  variables = \"e0\",\n  year = 2021\n)\n\nworld_lex %>% \n  slice_max(e0, n = 10)\nworld_lex %>% \n  slice_min(e0, n = 10)"},{"path":"working-with-census-data-outside-the-united-states.html","id":"visualizing-idb-data","chapter":"12 Working with Census data outside the United States","heading":"12.1.1 Visualizing IDB data","text":"idbr aims return data IDB structure suitable creative data visualizations. includes population pyramids (like introduced Chapter 4) world maps.mentioned earlier chapter, get_idb() can retrieve population data age sex one-year age bands age sex arguments supplied variables left blank. illustration found example , get_idb() used get 1-year--age population data 2021 Japan, broken sex.\nTable 12.4: 1-year age band population data Japan 2021\nFollowing example illustrated Section 4.5.2, population pyramid can created changing data values one sex negative, plotting data back--back horizontal bar charts.\nFigure 12.2: Population pyramid Japan\naging structure Japan’s population notable, especially number female centenarians (women aged 100 ) stand chart, number likely grow years ahead.get_idb() also includes geometry parameter help users make world maps demographic indicators. setting geometry = TRUE, get_idb() returns simple features object geometry column attached demographic data. , fetch data total fertility rate country 2021.couple points note respect data returned. dataset leaves number small countries appear zoomed-world map. ensure retained, use argument resolution = \"high\" higher-resolution dataset (may also want regional mapping). also rows returned NA dataset; fill areas appear map (like Antarctica) data IDB.data returned can mapped geom_sf() ggplot2 mapping tool introduced book. Specifying suitable map projection coord_sf() whole world - like Robinson projection used - recommended.\nFigure 12.3: Map total fertility rates country 2021\n","code":"\njapan_data <- get_idb(\n  country = \"Japan\",\n  year = 2021,\n  age = 0:100,\n  sex = c(\"male\", \"female\")\n) \njapan_data %>%\n  mutate(pop = ifelse(sex == \"Male\", pop * -1, pop)) %>%\n  ggplot(aes(x = pop, y = as.factor(age), fill = sex)) + \n  geom_col(width = 1) + \n  theme_minimal(base_size = 12) + \n  scale_x_continuous(labels = function(x) paste0(abs(x / 1000000), \"m\")) + \n  scale_y_discrete(breaks = scales::pretty_breaks(n = 10)) + \n  scale_fill_manual(values = c(\"darkred\", \"navy\")) + \n  labs(title = \"Population structure of Japan in 2021\",\n       x = \"Population\",\n       y = \"Age\",\n       fill = \"\")\nlibrary(idbr)\nlibrary(tidyverse)\n\nfertility_data <- get_idb(\n  country = \"all\",\n  year = 2021,\n  variables = \"tfr\",\n  geometry = TRUE,\n)\nggplot(fertility_data, aes(fill = tfr)) + \n  theme_bw() + \n  geom_sf() + \n  coord_sf(crs = 'ESRI:54030') + \n  scale_fill_viridis_c() + \n  labs(fill = \"Total fertility\\nrate (2021)\")"},{"path":"working-with-census-data-outside-the-united-states.html","id":"interactive-and-animated-visualization-of-global-demographic-data","chapter":"12 Working with Census data outside the United States","heading":"12.1.2 Interactive and animated visualization of global demographic data","text":"Chapter 6, readers learned create interactive maps US Census data tmap leaflet packages. methods can applied global data returned get_idb(), Web Mercator projection used Leaflet library ideal world maps inflates area countries near poles relative countries near Equator. problem Web Mercator covered Section 6.5.2 particularly important global maps. turn, preferable turn interactive charting libraries ggiraph.get started, let’s return global fertility data create new column store information want include tooltip appears viewer hovers country.now set visualization. Note instead using geom_sf(), use ggiraph’s geom_sf_interactive(), includes new aesthetics. map new tooltip column tooltip aesthetic, generate hover tooltip, map code column data_id aesthetic, change country’s color hovered . girafe() function renders interactive graphic; can specify want map zooming girafe_options().\nFigure 12.4: Interactive world map ggiraph\ntime-series data availability IDB also works well creating animated time-series graphics help gganimate package (Pedersen Robinson 2020). gganimate extends ggplot2 animate sequences plots using intuitive syntax ggplot2 users. Earlier chapter, learned create population pyramid IDB data single year (2021). data requested instead multiple years, gganimate can used transition year year animated population pyramid, showing population structure country evolved (projected evolve) time.\nFigure 12.5: Animated time-series population pyramid Mexico\nanimation illustrates aging Mexico’s population, observed projected, specified time period.","code":"\nfertility_data$tooltip <- paste0(fertility_data$name,\n                                 \": \",\n                                 round(fertility_data$tfr, 2))\nlibrary(ggiraph)\n\nfertility_map <- ggplot(fertility_data, aes(fill = tfr)) + \n  theme_bw() + \n  geom_sf_interactive(aes(tooltip = tooltip, data_id = code), size = 0.1) + \n  coord_sf(crs = 'ESRI:54030') + \n  scale_fill_viridis_c() + \n  labs(fill = \"Total fertility\\nrate (2021)\")\n\ngirafe(ggobj = fertility_map) %>%\n  girafe_options(opts_zoom(max = 10))\nlibrary(idbr)\nlibrary(tidyverse)\nlibrary(gganimate)\n\nmx_pyramid_data <- get_idb(\n  country = \"Mexico\",\n  sex = c(\"male\", \"female\"),\n  age = 0:100,\n  year = 1990:2050\n) %>%\n  mutate(pop = ifelse(sex == \"Male\", pop * -1, pop))\n\nmx_animation <- ggplot(mx_pyramid_data, \n                       aes(x = pop, \n                           y = as.factor(age), \n                           fill = sex)) + \n  geom_col(width = 1) + \n  theme_minimal(base_size = 16) + \n  scale_x_continuous(labels = function(x) paste0(abs(x / 1000000), \n                                                 \"m\")) + \n  scale_y_discrete(breaks = scales::pretty_breaks(n = 10)) + \n  scale_fill_manual(values = c(\"darkred\", \"darkgreen\")) + \n  transition_states(year) + \n  labs(title = \"Population structure of Mexico in {closest_state}\",\n       x = \"Population\",\n       y = \"Age\",\n       fill = \"\")\n\nanimate(mx_animation, height = 500, width = 800)"},{"path":"working-with-census-data-outside-the-united-states.html","id":"country-specific-census-data-packages","chapter":"12 Working with Census data outside the United States","heading":"12.2 Country-specific Census data packages","text":"Analysts around world need access high-quality tabular spatial demographic data make decisions much like data available US Census Bureau. Censuses conducted countries around world, countries expose data API way US Census Bureau . Nonetheless, number R packages help analysts acquire work country-specific Census data. section book covers sampling packages, examples given Canada, Mexico, Kenya, Brazil. examples illustrate apply many techniques introduced book variety non-US examples.","code":""},{"path":"working-with-census-data-outside-the-united-states.html","id":"canada-cancensus","chapter":"12 Working with Census data outside the United States","heading":"12.2.1 Canada: cancensus","text":"cancensus R package (von Bergmann, Shkolnik, Jacobs 2021) grants comprehensive access Canadian Census data CensusMapper APIs. Statistics Canada, Canadian agency distributes Canadian demographic data, maintain APIs, package author Jens von Bergmann’s CensusMapper product makes Canadian data accessible via web interface data API. Working cancensus feel familiar tidycensus users, cancensus integrates well within tidyverse includes option return feature geometry along demographic data wide range Canadian geographies.get started cancensus, obtain API key CensusMapper website store .Renviron file discussed Section 11. Supplying key cancensus.api_key option allow key picked automatically cancensus functions.list_census_vectors() function operates similar way load_variables() tidycensus. Specify dataset generate browsable data frame variable IDs can used query data CensusMapper API.\nTable 12.5: List Census vectors cancensus\ndata can browsed View() function RStudio much like recommended tidycensus Section 2.3. Two companion functions, list_census_regions() list_census_datasets(), also used identify region codes dataset codes ’d like request data.determined, appropriate codes can passed arguments various parameters get_census() function, main data access function used cancensus. example fetches data English speakers Census tract Montreal area way.example constructs query CensusMapper API using following arguments:dataset = \"CA16\" requests data 2016 Canadian Census;dataset = \"CA16\" requests data 2016 Canadian Census;regions = list(CMA = \"24462\") gets data Montreal metropolitan area using named list, matching region type CMA specific ID Montreal area;regions = list(CMA = \"24462\") gets data Montreal metropolitan area using named list, matching region type CMA specific ID Montreal area;vectors = \"v_CA16_1364\" fetches data specific Census vector represents English language speaking, identified using list_census_vectors();vectors = \"v_CA16_1364\" fetches data specific Census vector represents English language speaking, identified using list_census_vectors();level = \"CT\" gets data Census tract level;level = \"CT\" gets data Census tract level;geo_format = \"sf\" instructs get_census() return simple feature geometry Census tracts along Census data;geo_format = \"sf\" instructs get_census() return simple feature geometry Census tracts along Census data;labels = \"short\" returns simpler labels output dataset.labels = \"short\" returns simpler labels output dataset.queried, data appear follows:\nTable 12.6: Data English speakers Montreal Census tract\ndefault, cancensus returns number contextual variables along requested Census vector; total population, total households, total dwellings, area can used normalization automatically. information can used normalize data English speakers calculating new column mutate() named pct_english; familiar mapping tools Chapter 6 used visualize data.\nFigure 12.6: Interactive map percentage English language speakers home Montreal area\nExploring data interactively reveals distinctive patterns English language speaking Montreal. English common language spoken home central Census tracts southwestern portion Island Montreal, percentages much lower suburban areas outer portions metropolitan region.","code":"\nlibrary(cancensus)\nlibrary(tidyverse)\noptions(cancensus.api_key = Sys.getenv(\"CANCENSUS_API_KEY\"))\n\nvar_list <- list_census_vectors(\"CA16\")\nmontreal_english <- get_census(\n  dataset = \"CA16\",\n  regions = list(CMA = \"24462\"),\n  vectors = \"v_CA16_1364\",\n  level = \"CT\",\n  geo_format = \"sf\",\n  labels = \"short\"\n) \nlibrary(tmap)\ntmap_mode(\"view\")\n\nmontreal_pct <- montreal_english %>%\n  mutate(pct_english = 100 * (v_CA16_1364 / Population))\n\ntm_shape(montreal_pct) + \n  tm_polygons(\n    col = \"pct_english\", \n    alpha = 0.5, \n    palette = \"viridis\",\n    style = \"jenks\", \n    n = 7, \n    title = \"Percent speaking<br/>English at home\"\n  )"},{"path":"working-with-census-data-outside-the-united-states.html","id":"kenya-rkenyacensus","chapter":"12 Working with Census data outside the United States","heading":"12.2.2 Kenya: rKenyaCensus","text":"rKenyaCensus package (Kariuki 2020) makes indicators 2019 Kenya Population Housing Census available R users. package result painstaking effort package author Shel Kariuki scrape data PDFs containing Census results - way original data distributed - convert R data frames. Kenyan Census tables install directly package can accessed name. Let’s take look table V4_T2.30, contains information religion county Kenya. assign table variable religion ease reference.\nTable 12.7: Religion data 2019 Kenyan Census\ndata organized county Kenya religious affiliations arranged column. first row represents data entirety Kenya. additional data wrangling steps allow data readily plotted using familiar tooling. example, inclusion Total column makes calculation proportions straightforward. example , new column prop_islam computed passed ggplot2 bar chart.\nFigure 12.7: Bar chart Islam prevalence county Kenya\nvisualization illustrates Kenyan counties small Muslim populations; however, smaller number counties much larger Muslim populations, percentages cases close 100%. raises related questions geography patterns, can mapped using additional functionality rKenyaCensus.rKenyaCensus package includes built-county boundaries dataset facilitate mapping various indicators Census, KenyaCounties_SHP. object class SpatialPolygonsDataFrame, need converted simple features object sf::st_as_sf()\nFigure 12.8: ggplot2 map Kenya county boundaries\nlittle additional cleaning, religion data can joined county boundaries dataset, facilitating choropleth mapping ggplot2 geom_sf().\nFigure 12.9: Choropleth map Islam Kenya\nmap shows distinct religious divides Kenya play geographically country, proportionally Muslim residents western part Kenya much higher percentages eastern part country.","code":"\n# remotes::install_github(\"Shelmith-Kariuki/rKenyaCensus\")\nlibrary(rKenyaCensus)\nlibrary(tidyverse)\n\nreligion <- V4_T2.30\nreligion_prop <- religion %>%\n  filter(County != \"KENYA\") %>%\n  mutate(county_title = str_to_title(County), \n         prop_islam = Islam / Total)\n\nggplot(religion_prop, aes(x = prop_islam, \n                          y = reorder(county_title, prop_islam))) + \n  geom_col(fill = \"navy\", alpha = 0.6) + \n  theme_minimal(base_size = 12.5) + \n  scale_x_continuous(labels = scales::percent) + \n  labs(title = \"Percent Muslim by County in Kenya\",\n       subtitle = \"2019 Kenyan Census\", \n       x = \"\",\n       y = \"\",\n       caption = \"Data source: rKenyaCensus R package\")\nlibrary(sf)\n\nkenya_counties_sf <- st_as_sf(KenyaCounties_SHP)\n\nggplot(kenya_counties_sf) + \n  geom_sf() + \n  theme_void()\nkenya_islam_map <- kenya_counties_sf %>%\n  mutate(County = str_remove(County, \" CITY\")) %>%\n  left_join(religion_prop, by = \"County\") \n\nggplot(kenya_islam_map, aes(fill = prop_islam)) + \n  geom_sf() + \n  scale_fill_viridis_c(labels = scales::percent) + \n  theme_void() + \n  labs(fill = \"% Muslim\",\n       title = \"Percent Muslim by County in Kenya\",\n       subtitle = \"2019 Kenyan Census\",\n       caption = \"Data acquired with the rKenyaCensus R package\")"},{"path":"working-with-census-data-outside-the-united-states.html","id":"mexico-combining-mxmaps-and-inegir","chapter":"12 Working with Census data outside the United States","heading":"12.2.3 Mexico: combining mxmaps and inegiR","text":"Mexico’s national statistics office, Instituto Nacional de Estadística y Geografía (INEGI), offers API access many datasets. Used together, mxmaps (Valle-Jones 2021) inegiR (Flores 2019) R packages allow geographic analysis visualization Mexican Census data obtained INEGI.Like APIs introduced book, INEGI API requires API token. token can requested INEGI website. acquired, can used mxmaps inegiR request data; ’m saving key environment variable INEGI_API_KEY.mxmaps package based choroplethr R package (Lamstein 2020), offers convenient interface visualizing data US ACS. mxmaps wraps inegiR’s data access functions choropleth_inegi() function, can request data indicators INEGI’s data bank visualize . map shows percentage residents state born Mexico outside current state residence using indicator code 304003001. full functionality, inegiR mxmaps packages must installed GitHub remotes::install_github().\nFigure 12.10: Choropleth map internal migration Mexico state\nmap illustrates states internal migrants Mexico include Baja California Sur, Quintana Roo, Nuevo Leon. information can also accessed directly using inegiR package. inegi_series() function returns one state time, can combined country-wide dataset map_dfr().\nTable 12.8: Illustration data acquired inegiR\n","code":"\n# remotes::install_github(\"Eflores89/inegiR\")\n# remotes::install_github(\"diegovalle/mxmaps\")\nlibrary(mxmaps)\ntoken_inegi <- Sys.getenv(\"INEGI_API_KEY\")\n\nstate_regions <- df_mxstate_2020$region\nchoropleth_inegi(token_inegi, state_regions, \n                 indicator = \"3104003001\",\n                 legend = \"%\",\n                 title = \"Percentage born outside\\nstate of residence\") + \n  theme_void(base_size = 14) + \n  labs(caption = \"Data sources: INEGI, mxmaps R package\")## \nDownloading data from the INEGI API\nlibrary(inegiR)\ntoken_inegi <- Sys.getenv(\"INEGI_API_KEY\")\nstate_regions <- mxmaps::df_mxstate_2020$region\n\npct_migrants <- map_dfr(state_regions, ~{\n  inegi_series(series_id = \"3104003001\", \n               token = token_inegi, \n               geography = .x,\n               database = \"BISE\") %>%\n    mutate(state_code = .x)\n})"},{"path":"working-with-census-data-outside-the-united-states.html","id":"brazil-aligning-the-geobr-r-package-with-raw-census-data-files-for-spatial-analysis","chapter":"12 Working with Census data outside the United States","heading":"12.2.4 Brazil: aligning the geobr R package with raw Census data files for spatial analysis","text":"three country-specific examples shown - Canada, Kenya, Mexico - illustrate workflows obtaining data directly R package mapping results. many cases, however, data come directly R package, requiring analysts research raw data work raw files. example illustrates accomplish Brazil, spatial data found well-documented widely used R package geobr, demographic data downloaded processed directly Instituto Brasileiro de Geografia e Estatística website using R tooling.","code":""},{"path":"working-with-census-data-outside-the-united-states.html","id":"spatial-data-for-brazil-with-geobr","chapter":"12 Working with Census data outside the United States","heading":"12.2.4.1 Spatial data for Brazil with geobr","text":"geobr (Pereira Goncalves 2021) package IPEA helps load spatial datasets many Brazilian geographies, including Census boundaries. datasets generously hosted IPEA downloaded user’s R session corresponding function called. code_tract parameter can accept 7-digit municipality code; , code Rio de Janeiro used. Municipality codes can looked corresponding read_municipality() function.default, geobr retrieves Census tract dataset simplified geometries appropriate small-scale demographic visualization inappropriate spatial analysis large-scale visualization. quick visualization shows Census tract geometries Rio de Janeiro:\nFigure 12.11: Basic ggplot2 map Census tracts Rio de Janeiro\n","code":"\n# Install first from GitHub:\n# remotes::install_github(\"ipeaGIT/geobr\", subdir = \"r-package\")\nlibrary(geobr)\n\nrj_tracts <- read_census_tract(code_tract = 3304557)\nggplot(rj_tracts) + \n  geom_sf(lwd = 0.1) + \n  theme_void()"},{"path":"working-with-census-data-outside-the-united-states.html","id":"identifying-demographic-data-resources-for-brazil","chapter":"12 Working with Census data outside the United States","heading":"12.2.4.2 Identifying demographic data resources for Brazil","text":"many demographic applications, users want identify Brazilian data can joined Census tract shapes. makes useful exercise illustrate applied workflow working raw tables. example get data 2010 Brazilian Census merge geometries obtained geobr analysis.Census data tables can downloaded Instituto Brasileiro de Geografia e Estatística (IBGE) website’s FTP page, provides aggregated data Census tract level. datasets organized state, ’ll need identify appropriate zipped folder download bring data need R. R’s web access tools, discussed Section 11, can used download data. analysis focus Brasilia, capital city Brazil; turn, ’ll need data Distrito Federal (Federal District) Brasilia located.workflow operates follows:loading httr, create new subdirectory named brazil working directory. store data download IBGE.loading httr, create new subdirectory named brazil working directory. store data download IBGE.identify URL ’ll requesting data, extract basename URL basename(); name file disk.identify URL ’ll requesting data, extract basename URL basename(); name file disk.file.path() constructs output location downloaded file, GET() httr makes HTTP request, using write_disk() write downloaded file output location.file.path() constructs output location downloaded file, GET() httr makes HTTP request, using write_disk() write downloaded file output location.Finally, unzip() unzips folder. can take look got back:Finally, unzip() unzips folder. can take look got back:actual path, given subdirectory includes accented character, may differ based operating system /locale. Finally, ’ll copy files nested subdirectory brasilia_data directory.","code":"\nlibrary(httr)\n\ndir.create(\"brazil\")\n\ndata_url <- \"https://ftp.ibge.gov.br/Censos/Censo_Demografico_2010/Resultados_do_Universo/Agregados_por_Setores_Censitarios/DF_20171016.zip\"\n\nzip_name <- basename(data_url)\n\nout_file <- file.path(\"brazil\", zip_name)\n\nGET(data_url, write_disk(out_file, overwrite = TRUE))\n\nunzip(out_file, exdir = \"brazil/brasilia_data\")\ndata_path <- \"brazil/brasilia_data/DF/Base informa\\x87oes setores2010 universo DF/CSV\"\n\nlist.files(data_path)##  [1] \"Basico_DF.csv\"           \"Domicilio01_DF.csv\"     \n##  [3] \"Domicilio02_DF.csv\"      \"DomicilioRenda_DF.csv\"  \n##  [5] \"Entorno01_DF.csv\"        \"Entorno02_DF.csv\"       \n##  [7] \"Entorno03_DF.csv\"        \"Entorno04_DF.csv\"       \n##  [9] \"Entorno05_DF.csv\"        \"Pessoa01_DF.csv\"        \n## [11] \"Pessoa02_DF.csv\"         \"Pessoa03_DF.csv\"        \n## [13] \"Pessoa04_DF.csv\"         \"Pessoa05_DF.csv\"        \n## [15] \"Pessoa06_DF.csv\"         \"Pessoa07_DF.csv\"        \n## [17] \"Pessoa08_DF.csv\"         \"Pessoa09_DF.csv\"        \n## [19] \"Pessoa10_DF.csv\"         \"Pessoa11_DF.csv\"        \n## [21] \"Pessoa12_DF.csv\"         \"Pessoa13_DF.csv\"        \n## [23] \"PessoaRenda_DF.csv\"      \"Responsavel01_DF.csv\"   \n## [25] \"Responsavel02_DF.csv\"    \"ResponsavelRenda_DF.csv\"\nfile.copy(data_path, \"brazil/brasilia_data\", recursive = TRUE)"},{"path":"working-with-census-data-outside-the-united-states.html","id":"working-with-brazilian-demographic-data","chapter":"12 Working with Census data outside the United States","heading":"12.2.4.3 Working with Brazilian demographic data","text":"files include series CSV files represent characteristics Census tracts Federal District. Let’s read first file, Basico_DF.csv, see get back. important step use read_csv2() rather read_csv(); file semicolon-delimited uses commas decimal places, meaning read_csv() interpret incorrectly. use accented characters raw data file also requires setting encoding Latin-1.\nTable 12.9: Data 2010 Brazilian Census\nfile includes series ID codes series variables associated Census tract. meaning variables dataset can looked PDF data dictionary available download IBGE website. ’ll focus variable V007, represents average monthly income Census tract income-earning households.\nFigure 12.12: ggplot2 histogram median monthly income Census tract Brasilia\nTypical neighborhood income distributions, histogram heavily right-skewed large cluster Census tracts low earnings long tail wealthier areas.","code":"\nbasico <- read_csv2(\"brazil/brasilia_data/CSV/Basico_DF.csv\", \n                     locale = locale(encoding = \"latin1\"))\nbrasilia_income <- basico %>%\n  mutate(code_tract = as.character(Cod_setor)) %>%\n  select(code_tract, monthly_income = V007)\n\nggplot(brasilia_income, \n       aes(x = monthly_income)) + \n  geom_histogram(bins = 100, \n                 alpha = 0.5, \n                 color = \"navy\", \n                 fill = \"navy\") + \n  theme_minimal()"},{"path":"working-with-census-data-outside-the-united-states.html","id":"spatial-analysis-of-brazilian-census-data","chapter":"12 Working with Census data outside the United States","heading":"12.2.4.4 Spatial analysis of Brazilian Census data","text":"information can used concert geobr package analyze geographic distribution earnings using techniques covered elsewhere book. first step analysis involves returning geobr get Census tracts Brasilia/Federal District using Brasilia’s municipality code. argument simplified = FALSE used given need higher-quality tract boundaries spatial analysis. Next, income data merged tract shapes, transformed appropriate coordinate reference system Brasilia.joining data, income information can visualized map using familiar mapping tools like geom_sf().\nFigure 12.13: Choropleth map median monthly income Brasilia\nmap highlights income disparities central Brasilia rural portions Federal District, identifies Lago Sul area home highest-earning households, average.spatial data prepared appropriately, methods Section 7.7 can used explore patterns spatial autocorrelation monthly income Brasilia area. example uses local form Moran’s \\(\\) statistic, described detail Section 7.7.3. steps used described brief :queens-case contiguity-based spatial weights matrix created functions spdep package, using argument zero.policy = TRUE small number tracts dataset neighbors omitting NA values;queens-case contiguity-based spatial weights matrix created functions spdep package, using argument zero.policy = TRUE small number tracts dataset neighbors omitting NA values;monthly_income column scaled z-score;monthly_income column scaled z-score;LISA statistic computed localmoran_perm() function, using permutation-based method compute statistical significance;LISA statistic computed localmoran_perm() function, using permutation-based method compute statistical significance;LISA results attached spatial dataset recoded cluster groups visualization.LISA results attached spatial dataset recoded cluster groups visualization.computed, results can visualized map.\nFigure 12.14: LISA cluster map median monthly income Census tract Brasilia\nmap illustrates strong clustering high monthly incomes central Brasilia low-high spatial outliers scattered among higher-earning areas. Low-low cluster census tracts tend found edges urbanized portions Federal District, representing sparsely populated rural areas.","code":"\nlibrary(sf)\nlibrary(geobr)\n\nbrasilia_tracts <- read_census_tract(\n  code_tract = 5300108,\n  simplified = FALSE\n) %>%\n  select(code_tract)\n\nbrasilia_income_geo <- brasilia_tracts %>%\n  left_join(brasilia_income, by = \"code_tract\") %>%\n  st_transform(22523)\nggplot(brasilia_income_geo, aes(fill = monthly_income)) + \n  geom_sf(color = NA) + \n  theme_void() + \n  scale_fill_viridis_c()\nlibrary(spdep)\nset.seed(1983)\n\n# Omit NAs and scale the income variable\nbrasilia_input <- brasilia_income_geo %>%\n  mutate(scaled_income = as.numeric(scale(monthly_income))) %>%\n  na.omit() \n\n# Generate contiguity-based weights with zero.policy = TRUE\nbrasilia_weights <- brasilia_input %>%\n  poly2nb() %>%\n  nb2listw(zero.policy = TRUE)\n\n# Compute the LISA and convert to a tibble\nbrasilia_lisa <- localmoran_perm(brasilia_input$scaled_income, \n                                 brasilia_weights,\n                                 nsim = 999L, \n                                 alternative = \"two.sided\",\n                                 na.action = na.pass) %>%\n  as_tibble() %>%\n  set_names(c(\"local_i\", \"exp_i\", \"var_i\", \"z_i\", \"p_i\",\n              \"p_i_sim\", \"pi_sim_folded\", \"skewness\", \"kurtosis\"))\n\n# Add the LISA columns to the spatial dataset \n# and recode them into cluster values\nbrasilia_lisa_clusters <- brasilia_input %>%\n  select(code_tract, scaled_income) %>%\n  bind_cols(brasilia_lisa) %>%\n  mutate(lisa_cluster = case_when(\n    p_i >= 0.05 ~ \"Not significant\",\n    scaled_income > 0 & local_i > 0 ~ \"High-high\",\n    scaled_income > 0 & local_i < 0 ~ \"High-low\",\n    scaled_income < 0 & local_i > 0 ~ \"Low-low\",\n    scaled_income < 0 & local_i < 0 ~ \"Low-high\"\n  ))\ncolor_values <- c(`High-high` = \"red\", \n                  `High-low` = \"pink\", \n                  `Low-low` = \"blue\", \n                  `Low-high` = \"lightblue\", \n                  `Not significant` = \"white\")\n\nggplot(brasilia_lisa_clusters, aes(fill = lisa_cluster)) + \n  geom_sf(size = 0.1) + \n  theme_void(base_family = \"Verdana\") + \n  scale_fill_manual(values = color_values) + \n  labs(title = \"LISA clusters of median monthly income\",\n       subtitle = \"Census tracts, Brasilia/Federal District\",\n       fill = \"Cluster type\",\n       caption = \"Data sources: 2010 Brazilian Census via IBGE; geobr R package\")"},{"path":"working-with-census-data-outside-the-united-states.html","id":"other-international-data-resources","chapter":"12 Working with Census data outside the United States","heading":"12.3 Other international data resources","text":"Many international data resources R exist, robust R ecosystem emerging standardize data source. Europe, nomisr R package (Odell 2018) provides interface official statistics United Kingdom, insee package (Leclerc 2021) France. Europe-wide data can acquired using eurostat package (Lahti et al. 2017), interacts European Union data sources. Africa, afrimapr project effort disseminate Africa-wide datasets R users. includes wide variety spatial datasets countries across Africa.Microdata users interested IPUMS International project, makes historical microdata 100 countries available researchers. Like US IPUMS data, ipumsr R package can help users work international microdata R. companion project IPUMS International Historical Geographic Information System (IHGIS) project, aims disseminate international spatial demographic data much way NHGIS project (covered Section 11.1.1 historical Census data US.","code":""},{"path":"working-with-census-data-outside-the-united-states.html","id":"exercises-10","chapter":"12 Working with Census data outside the United States","heading":"12.4 Exercises","text":"Use idbr create map global demographic indicator shown . Use ggiraph ggplot2 make static interactive versions map.Choose one four countries highlighted chapter (Canada, Kenya, Mexico, Brazil) explore corresponding R packages . Explore available demographic indicators countries, try making map chart indicator.","code":""},{"path":"conclusion.html","id":"conclusion","chapter":"Conclusion","heading":"Conclusion","text":"title suggests, book overview methods, maps, models can used complete applied social research projects R Census data. breadth approaches packages covered book suggests, R’s ecosystems working topics large constantly changing. chosen focus packages developed (tidycensus, tigris, mapboxapi, crsuggest, idbr) frameworks like tidyverse sf integrate packages, many directions go learn .Census data analysts may interested delving deeper packages introduced Chapter 11, censusapi, ipumsr, lehdr. Geospatial analysts want read Geocomputation R (Lovelace, Nowosad, Muenchow 2019) gain strong command R’s capabilities spatial data. interested machine learning prediction, major focus book, read Ken Steif’s Public Policy Analytics (Steif 2021), uses skills learned book applied machine learning workflows public policy. Readers want learn modeling, especially within tidyverse framework, keep eye Tidy Modeling R (Kuhn Silge 2021), yet complete time writing finished offer comprehensive overview model data tidyverse-friendly way.packages outlined book also updated new data made available. top development priority tidycensus making access 2020 Census data seamless released data.census.gov Census API, allowing users work 2020 Census data exact way ’ve working datasets package. feature suggestions welcome tidycensus GitHub issues page, user contributions!future development work packages certainly done recent threats Census data quality mind. fortunate R community breadth data resources fingertips, important aware ephemeral can . Census Bureau release typical 2020 1-year ACS estimates due data collection problems COVID-19 pandemic, breaking time series workflows use 1-year ACS data. 5-year 2016-2020 ACS dataset still released, larger margins error due COVID-19 data collection problems present ACS samples next several years.Census data community also grappling tension preserving respondent privacy maintaining data quality. 2020 decennial Census data released using differential privacy, approach introduces errors datasets attempt preserve overall population characteristics limiting possibility re-identifying individuals data (Abowd 2018). Advocates differential privacy argue disclosure avoidance techniques necessary prevent re-identification attacks important Census Bureau given modern database reconstruction technologies (Hawes 2020). Critics approach, however, argued differential privacy disastrous impact Census data quality (Ruggles et al. 2019), potentially making microdata block-level aggregate data unusable. loss data quality also threatens disproportionately impact rural populations racial & ethnic minorities. make difficult evaluate racial health disparities (Santos-Lozada, Howard, Verdery 2020), accurately estimate COVID-19 mortality rates (Hauer Santos-Lozada 2021), complete countless analyses require access high-quality small-area Census data. broadly, Ruggles Van Riper (2021) argue re-identification attacks database reconstruction little different produced random number generator, actual threats population Census data re-identification already publicly available internet.Census data also faced threats higher levels authority. major initiative Trump Administration introduce citizenship question decennial Census, currently asked American Community Survey. move, ultimately rejected United States Supreme Court, widely interpreted effort suppress responses nonwhite immigrant communities (Frey 2019). also may precursor effort make Congressional apportionment contingent citizen population rather entire population mandated US Constitution. Although effort succeed, Trump Administration engaged efforts underfund undermine success Census, exacerbated data collection difficulties COVID-19 pandemic (Bahrampour, Rabinowitz, Mellnik 2021).discussion intended conclude book cynical note. Rather, reinforce one book’s central take-aways: critical importance high-quality, open data free open source software analyze data. Census data democratizing force many ways. allow communities analyze characteristics use information solve problems may otherwise overlooked higher levels government. allow analysts independently evaluate re-districting scenarios call disenfranchising local residents. help us “see” latent inequalities manifest within societies propose solutions rectify .initiatives facilitated open data, open tools analyze . discussed earlier book, even open government datasets traditionally hard work . may stored bulky datasets require navigation complex file systems, required expensive commercial software platforms process analyze . contrast, resources like R Census API put Census data queries fingertips user, integrate data analysis tools ways help analysts get insights faster. dismiss learning curve R methods skills necessary generate understand insights. Rather, stress open data open source software reduce financial logistical barriers access, creating larger diverse user community can generate unique insights help solve problems. Contributing effort one main reasons writing book publishing open website, much appreciate taken time read .","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
